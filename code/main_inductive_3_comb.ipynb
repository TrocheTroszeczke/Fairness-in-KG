{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f98128",
   "metadata": {},
   "source": [
    "### Train the inductive link prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea7a7dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'WN18RR_v4'\n",
    "model_id = 'SiaLP_6_new'\n",
    "lower_bound = 1\n",
    "upper_bound_path = 10\n",
    "upper_bound_subg = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54797cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#difine the names for saving\n",
    "model_name = 'Model_' + model_id + '_' + data_name\n",
    "one_hop_model_name = 'One_hop_model_' + model_id + '_' + data_name\n",
    "ids_name = 'IDs_' + model_id + '_' + data_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fba371e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import opensmile\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from sklearn.utils import shuffle\n",
    "from sys import getsizeof\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c098e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadKG:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.x = 'Hello'\n",
    "        \n",
    "    def load_train_data(self, data_path, one_hop, data, s_t_r, entity2id, id2entity,\n",
    "                     relation2id, id2relation):\n",
    "        \n",
    "        data_ = set()\n",
    "    \n",
    "        ####load the train, valid and test set##########\n",
    "        with open (data_path, 'r') as f:\n",
    "            \n",
    "            data_ini = f.readlines()\n",
    "                        \n",
    "            for i in range(len(data_ini)):\n",
    "            \n",
    "                x = data_ini[i].split()\n",
    "                \n",
    "                x_ = tuple(x)\n",
    "                \n",
    "                data_.add(x_)\n",
    "        \n",
    "        ####relation dict#################\n",
    "        index = len(relation2id)\n",
    "     \n",
    "        for key in data_:\n",
    "            \n",
    "            if key[1] not in relation2id:\n",
    "                \n",
    "                relation = key[1]\n",
    "                \n",
    "                relation2id[relation] = index\n",
    "                \n",
    "                id2relation[index] = relation\n",
    "                \n",
    "                index += 1\n",
    "                \n",
    "                #the inverse relation\n",
    "                iv_r = '_inverse_' + relation\n",
    "                \n",
    "                relation2id[iv_r] = index\n",
    "                \n",
    "                id2relation[index] = iv_r\n",
    "                \n",
    "                index += 1\n",
    "        \n",
    "        #get the id of the inverse relation, by above definition, initial relation has \n",
    "        #always even id, while inverse relation has always odd id.\n",
    "        def inverse_r(r):\n",
    "            \n",
    "            if r % 2 == 0: #initial relation\n",
    "                \n",
    "                iv_r = r + 1\n",
    "            \n",
    "            else: #inverse relation\n",
    "                \n",
    "                iv_r = r - 1\n",
    "            \n",
    "            return(iv_r)\n",
    "        \n",
    "        ####entity dict###################\n",
    "        index = len(entity2id)\n",
    "        \n",
    "        for key in data_:\n",
    "            \n",
    "            source, target = key[0], key[2]\n",
    "            \n",
    "            if source not in entity2id:\n",
    "                                \n",
    "                entity2id[source] = index\n",
    "                \n",
    "                id2entity[index] = source\n",
    "                \n",
    "                index += 1\n",
    "            \n",
    "            if target not in entity2id:\n",
    "                \n",
    "                entity2id[target] = index\n",
    "                \n",
    "                id2entity[index] = target\n",
    "                \n",
    "                index += 1\n",
    "                \n",
    "        #create the set of triples using id instead of string        \n",
    "        for ele in data_:\n",
    "            \n",
    "            s = entity2id[ele[0]]\n",
    "            \n",
    "            r = relation2id[ele[1]]\n",
    "            \n",
    "            t = entity2id[ele[2]]\n",
    "            \n",
    "            if (s,r,t) not in data:\n",
    "                \n",
    "                data.add((s,r,t))\n",
    "            \n",
    "            s_t_r[(s,t)].add(r)\n",
    "            \n",
    "            if s not in one_hop:\n",
    "                \n",
    "                one_hop[s] = set()\n",
    "            \n",
    "            one_hop[s].add((r,t))\n",
    "            \n",
    "            if t not in one_hop:\n",
    "                \n",
    "                one_hop[t] = set()\n",
    "            \n",
    "            r_inv = inverse_r(r)\n",
    "            \n",
    "            s_t_r[(t,s)].add(r_inv)\n",
    "            \n",
    "            one_hop[t].add((r_inv,s))\n",
    "            \n",
    "        #change each set in one_hop to list\n",
    "        for e in one_hop:\n",
    "            \n",
    "            one_hop[e] = list(one_hop[e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6cd0f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObtainPathsByDynamicProgramming:\n",
    "\n",
    "    def __init__(self, amount_bd=50, size_bd=50, threshold=20000):\n",
    "        \n",
    "        self.amount_bd = amount_bd #how many Tuples we choose in one_hop[node] for next recursion\n",
    "                        \n",
    "        self.size_bd = size_bd #size bound limit the number of paths to a target entity t\n",
    "        \n",
    "        #number of times paths with specific length been performed for recursion\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    '''\n",
    "    Given an entity s, the function will find the paths from s to other entities, using recursion.\n",
    "    \n",
    "    One may refer to LeetCode Problem 797 for details:\n",
    "        https://leetcode.com/problems/all-paths-from-source-to-target/\n",
    "    '''\n",
    "    def obtain_paths(self, mode, s, t_input, lower_bd, upper_bd, one_hop):\n",
    "\n",
    "        if type(lower_bd) != type(1) or lower_bd < 1:\n",
    "            \n",
    "            raise TypeError(\"!!! invalid lower bound setting, must >= 1 !!!\")\n",
    "            \n",
    "        if type(upper_bd) != type(1) or upper_bd < 1:\n",
    "            \n",
    "            raise TypeError(\"!!! invalid upper bound setting, must >= 1 !!!\")\n",
    "            \n",
    "        if lower_bd > upper_bd:\n",
    "            \n",
    "            raise TypeError(\"!!! lower bound must not exced upper bound !!!\")\n",
    "            \n",
    "        if s not in one_hop:\n",
    "            \n",
    "            raise ValueError('!!! entity not in one_hop. Please work on existing entities')\n",
    "\n",
    "        #here is the result dict. Its key is each entity t sharing paths from s\n",
    "        #The value of each t is a set containing the paths from s to t\n",
    "        #These paths can be either the direct connection r, or a multi-hop path\n",
    "        res = defaultdict(set)\n",
    "        \n",
    "        #qualified_t contains the types of t we want to consider,\n",
    "        #that is, what t will be added to the result set.\n",
    "        qualified_t = set()\n",
    "\n",
    "        #under this mode, we will only consider the direct neighbour of s\n",
    "        if mode == 'direct_neighbour':\n",
    "        \n",
    "            for Tuple in one_hop[s]:\n",
    "            \n",
    "                t = Tuple[1]\n",
    "                \n",
    "                qualified_t.add(t)\n",
    "        \n",
    "        #under this mode, we will only consider one specified entity t\n",
    "        elif mode == 'target_specified':\n",
    "            \n",
    "            qualified_t.add(t_input)\n",
    "        \n",
    "        #under this mode, we will consider any entity\n",
    "        elif mode == 'any_target':\n",
    "            \n",
    "            for s_any in one_hop:\n",
    "                \n",
    "                qualified_t.add(s_any)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            raise ValueError('not a valid mode')\n",
    "        \n",
    "        '''\n",
    "        We use recursion to find the paths\n",
    "        On current node with the path [r1, ..., rk] and on-path entities {s, e1, ..., ek-1, node}\n",
    "        from s to this node, we will further find the direct neighbor t' of this node. \n",
    "        If t' is not an on-path entity (not among s, e1,...ek-1, node), we recursively proceed to t' \n",
    "        '''\n",
    "        def helper(node, path, on_path_en, res, qualified_t, lower_bd, upper_bd, one_hop, count_dict):\n",
    "\n",
    "            #when the current path is within lower_bd and upper_bd, \n",
    "            #and the node is among the qualified t, and it has not been fill of paths w.r.t size_limit,\n",
    "            #we will add this path to the node\n",
    "            if (len(path) >= lower_bd) and (len(path) <= upper_bd) and (\n",
    "                node in qualified_t) and (len(res[node]) < self.size_bd):\n",
    "                \n",
    "                res[node].add(tuple(path))\n",
    "                    \n",
    "            #won't start new recursions if the current path length already reaches upper limit\n",
    "            #or the number of recursions performed on this length has reached the limit\n",
    "            if (len(path) < upper_bd) and (count_dict[len(path)] <= self.threshold):\n",
    "                                \n",
    "                #temp list is the id list for us to go-over one_hop[node]\n",
    "                temp_list = [i for i in range(len(one_hop[node]))]\n",
    "                random.shuffle(temp_list) #so we random-shuffle the list\n",
    "                \n",
    "                #only take 20 recursions if there are too many (r,t)\n",
    "                for i in temp_list[:self.amount_bd]:\n",
    "                    \n",
    "                    #obtain tuple of (r,t)\n",
    "                    Tuple = one_hop[node][i]\n",
    "                    r, t = Tuple[0], Tuple[1]\n",
    "                    \n",
    "                    #add to count_dict even if eventually this step not proceed\n",
    "                    count_dict[len(path)] += 1\n",
    "                    \n",
    "                    #if t not on the path and we not exceed the computation threshold, \n",
    "                    #then finally proceed to next recursion\n",
    "                    if (t not in on_path_en) and (count_dict[len(path)] <= self.threshold):\n",
    "\n",
    "                        helper(t, path + [r], on_path_en.union({t}), res, qualified_t, \n",
    "                               lower_bd, upper_bd, one_hop, count_dict)\n",
    "\n",
    "        length_dict = defaultdict(int)\n",
    "        count_dict = defaultdict(int)\n",
    "        \n",
    "        helper(s, [], {s}, res, qualified_t, lower_bd, upper_bd, one_hop, count_dict)\n",
    "        \n",
    "        return(res, count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecaf24ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/' + data_name + '/train.txt'\n",
    "valid_path = '../data/' + data_name + '/valid.txt'\n",
    "test_path = '../data/' + data_name + '/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5718867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the classes\n",
    "Class_1 = LoadKG()\n",
    "Class_2 = ObtainPathsByDynamicProgramming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57c472f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the dictionaries and sets for load KG\n",
    "one_hop = dict() \n",
    "data = set()\n",
    "s_t_r = defaultdict(set)\n",
    "\n",
    "#define the dictionaries, which is shared by initail and inductive train/valid/test\n",
    "entity2id = dict()\n",
    "id2entity = dict()\n",
    "relation2id = dict()\n",
    "id2relation = dict()\n",
    "\n",
    "#fill in the sets and dicts\n",
    "Class_1.load_train_data(train_path, one_hop, data, s_t_r,\n",
    "                        entity2id, id2entity, relation2id, id2relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54c4c0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the dictionaries and sets for load KG\n",
    "one_hop_valid = dict() \n",
    "data_valid = set()\n",
    "s_t_r_valid = defaultdict(set)\n",
    "\n",
    "#fill in the sets and dicts\n",
    "Class_1.load_train_data(valid_path, one_hop_valid, data_valid, s_t_r_valid,\n",
    "                        entity2id, id2entity, relation2id, id2relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "178bd0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the dictionaries and sets for load KG\n",
    "one_hop_test = dict() \n",
    "data_test = set()\n",
    "s_t_r_test = defaultdict(set)\n",
    "\n",
    "#fill in the sets and dicts\n",
    "Class_1.load_train_data(test_path, one_hop_test, data_test, s_t_r_test,\n",
    "                        entity2id, id2entity, relation2id, id2relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee8babf",
   "metadata": {},
   "source": [
    "#### Build the path-based siamese neural network structure\n",
    "\n",
    "We use biLSTM to train on the input path embedding sequence to predict the output embedding or the relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68239c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 13:37:33.944270: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Input layer, using integer to represent each relation type\n",
    "#note that inputs_path is the path inputs, while inputs_out_re is the output relation inputs\n",
    "fst_path = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "scd_path = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "thd_path = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "#the relation input layer (for output embedding)\n",
    "id_rela = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "# Embed each integer in a 300-dimensional vector as input,\n",
    "# note that we add another \"space holder\" embedding, \n",
    "# which hold the spaces if the initial length of paths are not the same\n",
    "in_embd_var = layers.Embedding(len(relation2id)+1, 300)\n",
    "\n",
    "# Obtain the embedding\n",
    "fst_p_embd = in_embd_var(fst_path)\n",
    "scd_p_embd = in_embd_var(scd_path)\n",
    "thd_p_embd = in_embd_var(thd_path)\n",
    "\n",
    "# Embed each integer in a 300-dimensional vector as output\n",
    "rela_embd = layers.Embedding(len(relation2id)+1, 300)(id_rela)\n",
    "\n",
    "#add 2 layer bi-directional LSTM\n",
    "lstm_layer_1 = layers.Bidirectional(layers.LSTM(150, return_sequences=True))\n",
    "lstm_layer_2 = layers.Bidirectional(layers.LSTM(150, return_sequences=True))\n",
    "\n",
    "#first LSTM layer\n",
    "fst_lstm_mid = lstm_layer_1(fst_p_embd)\n",
    "scd_lstm_mid = lstm_layer_1(scd_p_embd)\n",
    "thd_lstm_mid = lstm_layer_1(thd_p_embd)\n",
    "\n",
    "#second LSTM layer\n",
    "fst_lstm_out = lstm_layer_2(fst_lstm_mid)\n",
    "scd_lstm_out = lstm_layer_2(scd_lstm_mid)\n",
    "thd_lstm_out = lstm_layer_2(thd_lstm_mid)\n",
    "\n",
    "#reduce max\n",
    "fst_reduce_max = tf.reduce_max(fst_lstm_out, axis=1)\n",
    "scd_reduce_max = tf.reduce_max(scd_lstm_out, axis=1)\n",
    "thd_reduce_max = tf.reduce_max(thd_lstm_out, axis=1)\n",
    "\n",
    "#concatenate the output vector from both siamese tunnel: (Batch, 900)\n",
    "path_concat = layers.concatenate([fst_reduce_max, scd_reduce_max, thd_reduce_max], axis=-1)\n",
    "\n",
    "#add dropout on top of the concatenation from all channels\n",
    "dropout = layers.Dropout(0.25)(path_concat)\n",
    "\n",
    "#multiply into output embd size by dense layer: (Batch, 300)\n",
    "path_out_vect = layers.Dense(300, activation='tanh')(dropout)\n",
    "\n",
    "#remove the time dimension from the output embd since there is only one step\n",
    "rela_out_embd = tf.reduce_sum(rela_embd, axis=1)\n",
    "\n",
    "# Normalize the vectors to have unit length\n",
    "path_out_vect_norm = tf.math.l2_normalize(path_out_vect, axis=-1)\n",
    "rela_out_embd_norm = tf.math.l2_normalize(rela_out_embd, axis=-1)\n",
    "\n",
    "# Calculate the dot product\n",
    "dot_product = layers.Dot(axes=-1)([path_out_vect_norm, rela_out_embd_norm])\n",
    "\n",
    "#put together the model\n",
    "model = keras.Model([fst_path, scd_path, thd_path, id_rela], dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5cd4009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config the Adam optimizer \n",
    "opt = keras.optimizers.Adam(learning_rate=0.0005, decay=1e-6)\n",
    "\n",
    "#compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40033a4",
   "metadata": {},
   "source": [
    "#### Build the subgraph-based siamese neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c407303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#each input is an vector with number of relations to be dim:\n",
    "#each dim represent the existence (1) or not (0) of an out-going relation from the entity\n",
    "source_path_1 = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "source_path_2 = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "source_path_3 = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "source_path_4 = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "source_path_5 = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "source_path_6 = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "target_path_1 = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "target_path_2 = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "target_path_3 = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "target_path_4 = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "target_path_5 = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "target_path_6 = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "#the relation input layer (for output embedding)\n",
    "id_rela_ = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "# Embed each integer in a 300-dimensional vector as input,\n",
    "# note that we add another \"space holder\" embedding, \n",
    "# which hold the spaces if the initial length of paths are not the same\n",
    "#the source and target embedding and separate\n",
    "in_embd_var_ = layers.Embedding(len(relation2id)+1, 300)\n",
    "\n",
    "# Obtain the source embeddings\n",
    "source_embd_1 = in_embd_var_(source_path_1)\n",
    "source_embd_2 = in_embd_var_(source_path_2)\n",
    "source_embd_3 = in_embd_var_(source_path_3)\n",
    "source_embd_4 = in_embd_var_(source_path_4)\n",
    "source_embd_5 = in_embd_var_(source_path_5)\n",
    "source_embd_6 = in_embd_var_(source_path_6)\n",
    "\n",
    "#Obtain the target embeddings\n",
    "target_embd_1 = in_embd_var_(target_path_1)\n",
    "target_embd_2 = in_embd_var_(target_path_2)\n",
    "target_embd_3 = in_embd_var_(target_path_3)\n",
    "target_embd_4 = in_embd_var_(target_path_4)\n",
    "target_embd_5 = in_embd_var_(target_path_5)\n",
    "target_embd_6 = in_embd_var_(target_path_6)\n",
    "\n",
    "# Embed each integer in a 300-dimensional vector as output\n",
    "rela_embd_ = layers.Embedding(len(relation2id)+1, 300)(id_rela_)\n",
    "\n",
    "#add 2 layer bi-directional LSTM network\n",
    "lstm_1 = layers.Bidirectional(layers.LSTM(150, return_sequences=True))\n",
    "lstm_2 = layers.Bidirectional(layers.LSTM(150, return_sequences=True))\n",
    "\n",
    "###source lstm implimentation########\n",
    "#first LSTM layer\n",
    "source_mid_1 = lstm_1(source_embd_1)\n",
    "source_mid_2 = lstm_1(source_embd_2)\n",
    "source_mid_3 = lstm_1(source_embd_3)\n",
    "source_mid_4 = lstm_1(source_embd_4)\n",
    "source_mid_5 = lstm_1(source_embd_5)\n",
    "source_mid_6 = lstm_1(source_embd_6)\n",
    "\n",
    "#second LSTM layer\n",
    "source_out_1 = lstm_2(source_mid_1)\n",
    "source_out_2 = lstm_2(source_mid_2)\n",
    "source_out_3 = lstm_2(source_mid_3)\n",
    "source_out_4 = lstm_2(source_mid_4)\n",
    "source_out_5 = lstm_2(source_mid_5)\n",
    "source_out_6 = lstm_2(source_mid_6)\n",
    "\n",
    "#reduce max\n",
    "source_max_1 = tf.reduce_max(source_out_1, axis=1)\n",
    "source_max_2 = tf.reduce_max(source_out_2, axis=1)\n",
    "source_max_3 = tf.reduce_max(source_out_3, axis=1)\n",
    "source_max_4 = tf.reduce_max(source_out_4, axis=1)\n",
    "source_max_5 = tf.reduce_max(source_out_5, axis=1)\n",
    "source_max_6 = tf.reduce_max(source_out_6, axis=1)\n",
    "\n",
    "#concatenate the output vector from both siamese tunnel: (Batch, 900)\n",
    "source_concat = layers.concatenate([source_max_1, \n",
    "                                    source_max_2, \n",
    "                                    source_max_3,\n",
    "                                    source_max_4,\n",
    "                                    source_max_5,\n",
    "                                    source_max_6], axis=-1)\n",
    "\n",
    "#add dropout on top of the concatenation from all channels\n",
    "source_dropout = layers.Dropout(0.25)(source_concat)\n",
    "\n",
    "###target lstm implimentation########\n",
    "#first LSTM layer\n",
    "target_mid_1 = lstm_1(target_embd_1)\n",
    "target_mid_2 = lstm_1(target_embd_2)\n",
    "target_mid_3 = lstm_1(target_embd_3)\n",
    "target_mid_4 = lstm_1(target_embd_4)\n",
    "target_mid_5 = lstm_1(target_embd_5)\n",
    "target_mid_6 = lstm_1(target_embd_6)\n",
    "\n",
    "#second LSTM layer\n",
    "target_out_1 = lstm_2(target_mid_1)\n",
    "target_out_2 = lstm_2(target_mid_2)\n",
    "target_out_3 = lstm_2(target_mid_3)\n",
    "target_out_4 = lstm_2(target_mid_4)\n",
    "target_out_5 = lstm_2(target_mid_5)\n",
    "target_out_6 = lstm_2(target_mid_6)\n",
    "\n",
    "#reduce max\n",
    "target_max_1 = tf.reduce_max(target_out_1, axis=1)\n",
    "target_max_2 = tf.reduce_max(target_out_2, axis=1)\n",
    "target_max_3 = tf.reduce_max(target_out_3, axis=1)\n",
    "target_max_4 = tf.reduce_max(target_out_4, axis=1)\n",
    "target_max_5 = tf.reduce_max(target_out_5, axis=1)\n",
    "target_max_6 = tf.reduce_max(target_out_6, axis=1)\n",
    "\n",
    "#concatenate the output vector from both siamese tunnel: (Batch, 900)\n",
    "target_concat = layers.concatenate([target_max_1, \n",
    "                                    target_max_2, \n",
    "                                    target_max_3,\n",
    "                                    target_max_4,\n",
    "                                    target_max_5,\n",
    "                                    target_max_6], axis=-1)\n",
    "\n",
    "#add dropout on top of the concatenation from all channels\n",
    "target_dropout = layers.Dropout(0.25)(target_concat)\n",
    "\n",
    "#further concatenate source and target output embeddings: (Batch, 1800)\n",
    "final_concat = layers.concatenate([source_dropout, target_dropout], axis=-1)\n",
    "\n",
    "#multiply into output embd size by dense layer: (Batch, 300)\n",
    "out_vect = layers.Dense(300, activation='tanh')(final_concat)\n",
    "\n",
    "#remove the time dimension from the output embd since there is only one step\n",
    "rela_out_embd_ = tf.reduce_sum(rela_embd_, axis=1)\n",
    "\n",
    "# Normalize the vectors to have unit length\n",
    "out_vect_norm = tf.math.l2_normalize(out_vect, axis=-1)\n",
    "rela_out_embd_norm_ = tf.math.l2_normalize(rela_out_embd_, axis=-1)\n",
    "\n",
    "# Calculate the dot product\n",
    "dot_product_ = layers.Dot(axes=-1)([out_vect_norm, rela_out_embd_norm_])\n",
    "\n",
    "#put together the model\n",
    "model_2 = keras.Model([source_path_1, source_path_2, source_path_3, source_path_4,\n",
    "                       source_path_5, source_path_6,\n",
    "                       target_path_1, target_path_2, target_path_3, target_path_4, \n",
    "                       target_path_5, target_path_6,\n",
    "                       id_rela_], dot_product_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01a1f5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config the Adam optimizer \n",
    "opt_ = keras.optimizers.Adam(learning_rate=0.0005, decay=1e-6)\n",
    "\n",
    "#compile the model\n",
    "model_2.compile(loss='binary_crossentropy', optimizer=opt_, metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fd204f",
   "metadata": {},
   "source": [
    "### Build the big-batch for path-based model\n",
    "We will build the big-batch for the path-based model training. That is, we will build three list to store three paths, respectively.\n",
    "\n",
    "In order to reduce computational complexity, we will run the path-finding algorithm for each entity e in the dataset before the training. That is, for each entity e, we will have two dictionaries. Dict 1 stores the paths between e and any other entities in the dataset. Will Dict 2 stores the paths between e and its direct neighbors. The two dicts will be used and invariant throughout the training.\n",
    "\n",
    "* At each step, three different paths between two entities s and t are selected. Each path is append to one of the list. \n",
    "* If this step is for positive samples, the existing relation r will be selected between s and t. If there are more than one relation from s to t, we randomly choose one. Also, the label list will be appended 1.\n",
    "* If this step is for negative samples, one relation that does not exist between s and t will be selected randomly and append to the relation list. Also, the label list will be appended 0.\n",
    "* In practice, the positive step is always fallowed by a negative step. The same paths in the positive step will be used in the next negative step, while the relation is a negative one chosen in the above way.\n",
    "* We do this until the length limit is reached.\n",
    "\n",
    "**For relation prediciton, we will only need to train using (s,r,t) triple. (t,r-1,s) is not necessary and hence not included in training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bb3ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to build the big batche for path-based training\n",
    "def build_big_batches_path(lower_bd, upper_bd, data, one_hop, s_t_r,\n",
    "                      x_p_list, x_r_list, y_list,\n",
    "                      relation2id, entity2id, id2relation, id2entity):\n",
    "    \n",
    "    #the set of all relation IDs\n",
    "    relation_id_set = set()\n",
    "    \n",
    "    #the set of all initial relations\n",
    "    ini_r_id_set = set()\n",
    "    \n",
    "    for i in range(len(id2relation)):\n",
    "        \n",
    "        if i not in id2relation:\n",
    "            raise ValueError('error when generaing id2relation')\n",
    "        \n",
    "        relation_id_set.add(i)\n",
    "        \n",
    "        if i % 2 == 0: #initial relation id is always an even number\n",
    "            ini_r_id_set.add(i)\n",
    "    \n",
    "    num_r = len(id2relation)\n",
    "    num_ini_r = len(ini_r_id_set)\n",
    "    \n",
    "    if num_ini_r != int(num_r/2):\n",
    "        raise ValueError('error when generating id2relation')\n",
    "    \n",
    "    #in case not all entities in entity2id are in one_hop, \n",
    "    #so we need to find out who are indeed in\n",
    "    existing_ids = set()\n",
    "    \n",
    "    for s_1 in one_hop:\n",
    "        existing_ids.add(s_1)\n",
    "        \n",
    "    existing_ids = list(existing_ids)\n",
    "    random.shuffle(existing_ids)\n",
    "    \n",
    "    count = 0\n",
    "    for s in existing_ids:\n",
    "        \n",
    "        #impliment the path finding algorithm to find paths between s and t\n",
    "        result, length_dict = Class_2.obtain_paths('direct_neighbour', s, 'nb', lower_bd, upper_bd, one_hop)\n",
    "        \n",
    "        for iteration in range(10):\n",
    "\n",
    "            #proceed only if at least three paths are between s and t\n",
    "            for t in result:\n",
    "\n",
    "                if len(s_t_r[(s,t)]) == 0:\n",
    "\n",
    "                    raise ValueError(s,t,id2entity[s], id2entity[t])\n",
    "\n",
    "                #we are only interested in forward link in relation prediciton\n",
    "                ini_r_list = list()\n",
    "\n",
    "                #obtain initial relations between s and t\n",
    "                for r in s_t_r[(s,t)]:\n",
    "                    if r % 2 == 0:#initial relation id is always an even number\n",
    "                        ini_r_list.append(r)\n",
    "\n",
    "                #if there exist more than three paths between s and t, \n",
    "                #and inital connection between s and t exists,\n",
    "                #and not every r in the relation dictionary exists between s and t (although this is rare)\n",
    "                #we then proceed\n",
    "                if len(result[t]) >= 3 and len(ini_r_list) > 0 and len(ini_r_list) < int(num_ini_r):\n",
    "\n",
    "                    #obtain the list form of all the paths from s to t\n",
    "                    temp_path_list = list(result[t])\n",
    "\n",
    "                    temp_pair = random.sample(temp_path_list, 3)\n",
    "\n",
    "                    path_1, path_2, path_3 = temp_pair[0], temp_pair[1], temp_pair[2]\n",
    "\n",
    "                    #####positive#####################\n",
    "                    #append the paths: note that we add the space holder id at the end of the shorter path\n",
    "                    x_p_list['1'].append(list(path_1) + [num_r]*abs(len(path_1)-upper_bd))\n",
    "                    x_p_list['2'].append(list(path_2) + [num_r]*abs(len(path_2)-upper_bd))\n",
    "                    x_p_list['3'].append(list(path_3) + [num_r]*abs(len(path_3)-upper_bd))\n",
    "\n",
    "                    #append relation\n",
    "                    r = random.choice(ini_r_list)\n",
    "                    x_r_list.append([r])\n",
    "                    y_list.append(1.)\n",
    "\n",
    "                    #####negative#####################\n",
    "                    #append the paths: note that we add the space holder id at the end\n",
    "                    #of the shorter path\n",
    "                    x_p_list['1'].append(list(path_1) + [num_r]*abs(len(path_1)-upper_bd))\n",
    "                    x_p_list['2'].append(list(path_2) + [num_r]*abs(len(path_2)-upper_bd))\n",
    "                    x_p_list['3'].append(list(path_3) + [num_r]*abs(len(path_3)-upper_bd))\n",
    "\n",
    "                    #append relation\n",
    "                    neg_r_list = list(ini_r_id_set.difference(set(ini_r_list)))\n",
    "                    r_ran = random.choice(neg_r_list)\n",
    "                    x_r_list.append([r_ran])\n",
    "                    y_list.append(0.)\n",
    "        \n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print('generating big-batches for path-based model', count, len(existing_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f215dcc4",
   "metadata": {},
   "source": [
    "### Build the big-batch for the subgraph-based network training\n",
    "\n",
    "Again, to reduce computational complexity, we store the subgraph of each entity e at the biginning.\n",
    "\n",
    "* At each step, we will select one triple (s,r,t) from the dataset. Then, reaching out paths of s and t is generated respectively according to their out-going relations.\n",
    "* We will select three paths for each of source and target entity. Add them to the corresponding list.\n",
    "* If this is a positive sample step, the id of relation r is appended to the relation list.\n",
    "* If this is a negative sample step, the id of a random relation is appended to the relation lsit.\n",
    "* Similarly, one negative sample step always follows one positive step. The one-hop vectors from the previous positve step is used again for the negative step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6c3264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again, it is too slow to run the path-finding algorithm again and again on the complete FB15K-237\n",
    "#Instead, we will find the subgraph for each entity once.\n",
    "#then in the subgraph based training, the subgraphs are stored and used for multiple times\n",
    "def store_subgraph_dicts(lower_bd, upper_bd, data, one_hop, s_t_r,\n",
    "                         relation2id, entity2id, id2relation, id2entity):\n",
    "    \n",
    "    #the set of all relation IDs\n",
    "    relation_id_set = set()\n",
    "    \n",
    "    for i in range(len(id2relation)):\n",
    "        \n",
    "        if i not in id2relation:\n",
    "            raise ValueError('error when generaing id2relation')\n",
    "        \n",
    "        relation_id_set.add(i)\n",
    "    \n",
    "    num_r = len(id2relation)\n",
    "    \n",
    "    #in case not all entities in entity2id are in one_hop, \n",
    "    #so we need to find out who are indeed in\n",
    "    existing_ids = set()\n",
    "    \n",
    "    for s_1 in one_hop:\n",
    "        existing_ids.add(s_1)\n",
    "    \n",
    "    #the ids to start path finding\n",
    "    existing_ids = list(existing_ids)\n",
    "    random.shuffle(existing_ids)\n",
    "    \n",
    "    #Dict stores the subgraph for each entity\n",
    "    Dict_1 = dict()\n",
    "    \n",
    "    count = 0\n",
    "    for s in existing_ids:\n",
    "        \n",
    "        path_set = set()\n",
    "            \n",
    "        result, length_dict = Class_2.obtain_paths('any_target', s, 'any', lower_bd, upper_bd, one_hop)\n",
    "\n",
    "        for t_ in result:\n",
    "            for path in result[t_]:\n",
    "                path_set.add(path)\n",
    "\n",
    "        del(result, length_dict)\n",
    "        \n",
    "        path_list = list(path_set)\n",
    "        \n",
    "        path_select = random.sample(path_list, min(len(path_list), 100))\n",
    "            \n",
    "        Dict_1[s] = deepcopy(path_select)\n",
    "        \n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print('generating and storing paths for the path-based model', count, len(existing_ids))\n",
    "        \n",
    "    return(Dict_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1551116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to build the big-batch for one-hope neighbor training\n",
    "def build_big_batches_subgraph(lower_bd, upper_bd, data, one_hop, s_t_r,\n",
    "                      x_s_list, x_t_list, x_r_list, y_list, Dict,\n",
    "                      relation2id, entity2id, id2relation, id2entity):\n",
    "    \n",
    "    #the set of all relation IDs\n",
    "    relation_id_set = set()\n",
    "    \n",
    "    #the set of all initial relations\n",
    "    ini_r_id_set = set()\n",
    "    \n",
    "    for i in range(len(id2relation)):\n",
    "        \n",
    "        if i not in id2relation:\n",
    "            raise ValueError('error when generaing id2relation')\n",
    "        \n",
    "        relation_id_set.add(i)\n",
    "        \n",
    "        if i % 2 == 0: #initial relation id is always an even number\n",
    "            ini_r_id_set.add(i)\n",
    "    \n",
    "    num_r = len(id2relation)\n",
    "    num_ini_r = len(ini_r_id_set)\n",
    "    \n",
    "    if num_ini_r != int(num_r/2):\n",
    "        raise ValueError('error when generating id2relation')\n",
    "        \n",
    "    #if an entity has at least three out-stretching paths, it is a qualified one\n",
    "    qualified = set()\n",
    "    for e in Dict:\n",
    "        if len(Dict[e]) >= 6:\n",
    "            qualified.add(e)\n",
    "    qualified = list(qualified)\n",
    "    \n",
    "    data = list(data)\n",
    "    \n",
    "    for iteration in range(10):\n",
    "\n",
    "        data = shuffle(data)\n",
    "\n",
    "        for i_0 in range(len(data)):\n",
    "\n",
    "            triple = data[i_0]\n",
    "\n",
    "            s, r, t = triple[0], triple[1], triple[2] #obtain entities and relation IDs\n",
    "\n",
    "            if s in qualified and t in qualified:\n",
    "\n",
    "                #obtain the path list for true entities\n",
    "                path_s, path_t = list(Dict[s]), list(Dict[t])\n",
    "\n",
    "                #####positive step###########\n",
    "                #randomly obtain three paths for true entities\n",
    "                temp_s = random.sample(path_s, 6)\n",
    "                temp_t = random.sample(path_t, 6)\n",
    "                s_p_1, s_p_2, s_p_3, s_p_4, s_p_5, s_p_6 = temp_s[0], temp_s[1], temp_s[2], temp_s[3], temp_s[4], temp_s[5]\n",
    "                t_p_1, t_p_2, t_p_3, t_p_4, t_p_5, t_p_6 = temp_t[0], temp_t[1], temp_t[2], temp_t[3], temp_t[4], temp_t[5]\n",
    "\n",
    "                #append the paths: note that we add the space holder id at the end of the shorter path\n",
    "                x_s_list['1'].append(list(s_p_1) + [num_r]*abs(len(s_p_1)-upper_bd))\n",
    "                x_s_list['2'].append(list(s_p_2) + [num_r]*abs(len(s_p_2)-upper_bd))\n",
    "                x_s_list['3'].append(list(s_p_3) + [num_r]*abs(len(s_p_3)-upper_bd))\n",
    "                x_s_list['4'].append(list(s_p_4) + [num_r]*abs(len(s_p_4)-upper_bd))\n",
    "                x_s_list['5'].append(list(s_p_5) + [num_r]*abs(len(s_p_5)-upper_bd))\n",
    "                x_s_list['6'].append(list(s_p_6) + [num_r]*abs(len(s_p_6)-upper_bd))\n",
    "\n",
    "                x_t_list['1'].append(list(t_p_1) + [num_r]*abs(len(t_p_1)-upper_bd))\n",
    "                x_t_list['2'].append(list(t_p_2) + [num_r]*abs(len(t_p_2)-upper_bd))\n",
    "                x_t_list['3'].append(list(t_p_3) + [num_r]*abs(len(t_p_3)-upper_bd))\n",
    "                x_t_list['4'].append(list(t_p_4) + [num_r]*abs(len(t_p_4)-upper_bd))\n",
    "                x_t_list['5'].append(list(t_p_5) + [num_r]*abs(len(t_p_5)-upper_bd))\n",
    "                x_t_list['6'].append(list(t_p_6) + [num_r]*abs(len(t_p_6)-upper_bd))\n",
    "\n",
    "                #append relation\n",
    "                x_r_list.append([r])\n",
    "                y_list.append(1.)\n",
    "\n",
    "                #####negative step for relation###########\n",
    "                #append the paths: note that we add the space holder id at the end of the shorter path\n",
    "                x_s_list['1'].append(list(s_p_1) + [num_r]*abs(len(s_p_1)-upper_bd))\n",
    "                x_s_list['2'].append(list(s_p_2) + [num_r]*abs(len(s_p_2)-upper_bd))\n",
    "                x_s_list['3'].append(list(s_p_3) + [num_r]*abs(len(s_p_3)-upper_bd))\n",
    "                x_s_list['4'].append(list(s_p_4) + [num_r]*abs(len(s_p_4)-upper_bd))\n",
    "                x_s_list['5'].append(list(s_p_5) + [num_r]*abs(len(s_p_5)-upper_bd))\n",
    "                x_s_list['6'].append(list(s_p_6) + [num_r]*abs(len(s_p_6)-upper_bd))\n",
    "\n",
    "                x_t_list['1'].append(list(t_p_1) + [num_r]*abs(len(t_p_1)-upper_bd))\n",
    "                x_t_list['2'].append(list(t_p_2) + [num_r]*abs(len(t_p_2)-upper_bd))\n",
    "                x_t_list['3'].append(list(t_p_3) + [num_r]*abs(len(t_p_3)-upper_bd))\n",
    "                x_t_list['4'].append(list(t_p_4) + [num_r]*abs(len(t_p_4)-upper_bd))\n",
    "                x_t_list['5'].append(list(t_p_5) + [num_r]*abs(len(t_p_5)-upper_bd))\n",
    "                x_t_list['6'].append(list(t_p_6) + [num_r]*abs(len(t_p_6)-upper_bd))\n",
    "\n",
    "                #append relation\n",
    "                neg_r_list = list(ini_r_id_set.difference({r}))\n",
    "                r_ran = random.choice(neg_r_list)\n",
    "                x_r_list.append([r_ran])\n",
    "                y_list.append(0.)\n",
    "                \n",
    "                ##############################################\n",
    "                ##############################################\n",
    "                #randomly choose two negative sampled entities\n",
    "                s_ran = random.choice(qualified)\n",
    "                t_ran = random.choice(qualified)\n",
    "\n",
    "                #obtain the path list for random entities\n",
    "                path_s_ran, path_t_ran = list(Dict[s_ran]), list(Dict[t_ran])\n",
    "                \n",
    "                #####positive step#################\n",
    "                #Again: randomly obtain three paths\n",
    "                temp_s = random.sample(path_s, 6)\n",
    "                temp_t = random.sample(path_t, 6)\n",
    "                s_p_1, s_p_2, s_p_3, s_p_4, s_p_5, s_p_6 = temp_s[0], temp_s[1], temp_s[2], temp_s[3], temp_s[4], temp_s[5]\n",
    "                t_p_1, t_p_2, t_p_3, t_p_4, t_p_5, t_p_6 = temp_t[0], temp_t[1], temp_t[2], temp_t[3], temp_t[4], temp_t[5]\n",
    "\n",
    "                #append the paths: note that we add the space holder id at the end of the shorter path\n",
    "                x_s_list['1'].append(list(s_p_1) + [num_r]*abs(len(s_p_1)-upper_bd))\n",
    "                x_s_list['2'].append(list(s_p_2) + [num_r]*abs(len(s_p_2)-upper_bd))\n",
    "                x_s_list['3'].append(list(s_p_3) + [num_r]*abs(len(s_p_3)-upper_bd))\n",
    "                x_s_list['4'].append(list(s_p_4) + [num_r]*abs(len(s_p_4)-upper_bd))\n",
    "                x_s_list['5'].append(list(s_p_5) + [num_r]*abs(len(s_p_5)-upper_bd))\n",
    "                x_s_list['6'].append(list(s_p_6) + [num_r]*abs(len(s_p_6)-upper_bd))\n",
    "\n",
    "                x_t_list['1'].append(list(t_p_1) + [num_r]*abs(len(t_p_1)-upper_bd))\n",
    "                x_t_list['2'].append(list(t_p_2) + [num_r]*abs(len(t_p_2)-upper_bd))\n",
    "                x_t_list['3'].append(list(t_p_3) + [num_r]*abs(len(t_p_3)-upper_bd))\n",
    "                x_t_list['4'].append(list(t_p_4) + [num_r]*abs(len(t_p_4)-upper_bd))\n",
    "                x_t_list['5'].append(list(t_p_5) + [num_r]*abs(len(t_p_5)-upper_bd))\n",
    "                x_t_list['6'].append(list(t_p_6) + [num_r]*abs(len(t_p_6)-upper_bd))\n",
    "\n",
    "                #append relation\n",
    "                x_r_list.append([r])\n",
    "                y_list.append(1.)\n",
    "\n",
    "                #####negative for source entity###########\n",
    "                #randomly obtain three paths\n",
    "                temp_s = random.sample(path_s_ran, 6)\n",
    "                s_p_1, s_p_2, s_p_3, s_p_4, s_p_5, s_p_6 = temp_s[0], temp_s[1], temp_s[2], temp_s[3], temp_s[4], temp_s[5]\n",
    "\n",
    "                #append the paths: note that we add the space holder id at the end of the shorter path\n",
    "                x_s_list['1'].append(list(s_p_1) + [num_r]*abs(len(s_p_1)-upper_bd))\n",
    "                x_s_list['2'].append(list(s_p_2) + [num_r]*abs(len(s_p_2)-upper_bd))\n",
    "                x_s_list['3'].append(list(s_p_3) + [num_r]*abs(len(s_p_3)-upper_bd))\n",
    "                x_s_list['4'].append(list(s_p_4) + [num_r]*abs(len(s_p_4)-upper_bd))\n",
    "                x_s_list['5'].append(list(s_p_5) + [num_r]*abs(len(s_p_5)-upper_bd))\n",
    "                x_s_list['6'].append(list(s_p_6) + [num_r]*abs(len(s_p_6)-upper_bd))\n",
    "\n",
    "                x_t_list['1'].append(list(t_p_1) + [num_r]*abs(len(t_p_1)-upper_bd))\n",
    "                x_t_list['2'].append(list(t_p_2) + [num_r]*abs(len(t_p_2)-upper_bd))\n",
    "                x_t_list['3'].append(list(t_p_3) + [num_r]*abs(len(t_p_3)-upper_bd))\n",
    "                x_t_list['4'].append(list(t_p_4) + [num_r]*abs(len(t_p_4)-upper_bd))\n",
    "                x_t_list['5'].append(list(t_p_5) + [num_r]*abs(len(t_p_5)-upper_bd))\n",
    "                x_t_list['6'].append(list(t_p_6) + [num_r]*abs(len(t_p_6)-upper_bd))\n",
    "\n",
    "                #append relation\n",
    "                x_r_list.append([r])\n",
    "                y_list.append(0.)\n",
    "\n",
    "                #####positive step###########\n",
    "                #Again: randomly obtain three paths\n",
    "                temp_s = random.sample(path_s, 6)\n",
    "                temp_t = random.sample(path_t, 6)\n",
    "                s_p_1, s_p_2, s_p_3, s_p_4, s_p_5, s_p_6 = temp_s[0], temp_s[1], temp_s[2], temp_s[3], temp_s[4], temp_s[5]\n",
    "                t_p_1, t_p_2, t_p_3, t_p_4, t_p_5, t_p_6 = temp_t[0], temp_t[1], temp_t[2], temp_t[3], temp_t[4], temp_t[5]\n",
    "\n",
    "                #append the paths: note that we add the space holder id at the end of the shorter path\n",
    "                x_s_list['1'].append(list(s_p_1) + [num_r]*abs(len(s_p_1)-upper_bd))\n",
    "                x_s_list['2'].append(list(s_p_2) + [num_r]*abs(len(s_p_2)-upper_bd))\n",
    "                x_s_list['3'].append(list(s_p_3) + [num_r]*abs(len(s_p_3)-upper_bd))\n",
    "                x_s_list['4'].append(list(s_p_4) + [num_r]*abs(len(s_p_4)-upper_bd))\n",
    "                x_s_list['5'].append(list(s_p_5) + [num_r]*abs(len(s_p_5)-upper_bd))\n",
    "                x_s_list['6'].append(list(s_p_6) + [num_r]*abs(len(s_p_6)-upper_bd))\n",
    "\n",
    "                x_t_list['1'].append(list(t_p_1) + [num_r]*abs(len(t_p_1)-upper_bd))\n",
    "                x_t_list['2'].append(list(t_p_2) + [num_r]*abs(len(t_p_2)-upper_bd))\n",
    "                x_t_list['3'].append(list(t_p_3) + [num_r]*abs(len(t_p_3)-upper_bd))\n",
    "                x_t_list['4'].append(list(t_p_4) + [num_r]*abs(len(t_p_4)-upper_bd))\n",
    "                x_t_list['5'].append(list(t_p_5) + [num_r]*abs(len(t_p_5)-upper_bd))\n",
    "                x_t_list['6'].append(list(t_p_6) + [num_r]*abs(len(t_p_6)-upper_bd))\n",
    "\n",
    "                #append relation\n",
    "                x_r_list.append([r])\n",
    "                y_list.append(1.)\n",
    "\n",
    "                #####negative for target entity###########\n",
    "                #randomly obtain three paths\n",
    "                temp_t = random.sample(path_t_ran, 6)\n",
    "                t_p_1, t_p_2, t_p_3, t_p_4, t_p_5, t_p_6 = temp_t[0], temp_t[1], temp_t[2], temp_t[3], temp_t[4], temp_t[5]\n",
    "\n",
    "                #append the paths: note that we add the space holder id at the end of the shorter path\n",
    "                x_s_list['1'].append(list(s_p_1) + [num_r]*abs(len(s_p_1)-upper_bd))\n",
    "                x_s_list['2'].append(list(s_p_2) + [num_r]*abs(len(s_p_2)-upper_bd))\n",
    "                x_s_list['3'].append(list(s_p_3) + [num_r]*abs(len(s_p_3)-upper_bd))\n",
    "                x_s_list['4'].append(list(s_p_4) + [num_r]*abs(len(s_p_4)-upper_bd))\n",
    "                x_s_list['5'].append(list(s_p_5) + [num_r]*abs(len(s_p_5)-upper_bd))\n",
    "                x_s_list['6'].append(list(s_p_6) + [num_r]*abs(len(s_p_6)-upper_bd))\n",
    "\n",
    "                x_t_list['1'].append(list(t_p_1) + [num_r]*abs(len(t_p_1)-upper_bd))\n",
    "                x_t_list['2'].append(list(t_p_2) + [num_r]*abs(len(t_p_2)-upper_bd))\n",
    "                x_t_list['3'].append(list(t_p_3) + [num_r]*abs(len(t_p_3)-upper_bd))\n",
    "                x_t_list['4'].append(list(t_p_4) + [num_r]*abs(len(t_p_4)-upper_bd))\n",
    "                x_t_list['5'].append(list(t_p_5) + [num_r]*abs(len(t_p_5)-upper_bd))\n",
    "                x_t_list['6'].append(list(t_p_6) + [num_r]*abs(len(t_p_6)-upper_bd))\n",
    "\n",
    "                #append relation\n",
    "                x_r_list.append([r])\n",
    "                y_list.append(0.)\n",
    "\n",
    "            if i_0 % 2000 == 0:\n",
    "                print('generating big-batches for subgraph-based model', i_0, len(data), iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851e0ac2",
   "metadata": {},
   "source": [
    "### Start Training: load the KG and call classes\n",
    "\n",
    "Here, we use the validation set to see the training efficiency. That is, we use the validation to check whether the true relation between entities can be predicted by paths.\n",
    "\n",
    "The trick is: in validation, we have to use the same relation ID and entity ID as in the training. But we don't want to use the links in training anymore. That is, in validation, we want to use (and update if necessary) entity2id, id2entity, relation2id and id2relation. But we want to use new one_hop, data, data_ and s_t_r for validation set. Then, path-finding will also be based on new one_hop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28be7a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Model_SiaLP_6_new_WN18RR_v4'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8103ea50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One_hop_model_SiaLP_6_new_WN18RR_v4'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hop_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17c02a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IDs_SiaLP_6_new_WN18RR_v4'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7f57c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, we save the relation and ids\n",
    "Dict = dict()\n",
    "\n",
    "#save training data\n",
    "Dict['one_hop'] = one_hop\n",
    "Dict['data'] = data\n",
    "Dict['s_t_r'] = s_t_r\n",
    "\n",
    "#save valid data\n",
    "Dict['one_hop_valid'] = one_hop_valid\n",
    "Dict['data_valid'] = data_valid\n",
    "Dict['s_t_r_valid'] = s_t_r_valid\n",
    "\n",
    "#save test data\n",
    "Dict['one_hop_test'] = one_hop_test\n",
    "Dict['data_test'] = data_test\n",
    "Dict['s_t_r_test'] = s_t_r_test\n",
    "\n",
    "#save shared dictionaries\n",
    "Dict['entity2id'] = entity2id\n",
    "Dict['id2entity'] = id2entity\n",
    "Dict['relation2id'] = relation2id\n",
    "Dict['id2relation'] = id2relation\n",
    "\n",
    "with open('../weight_bin/' + ids_name + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(Dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25a5deac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating big-batches for path-based model 100 3861\n",
      "generating big-batches for path-based model 200 3861\n",
      "generating big-batches for path-based model 300 3861\n",
      "generating big-batches for path-based model 400 3861\n",
      "generating big-batches for path-based model 500 3861\n",
      "generating big-batches for path-based model 600 3861\n",
      "generating big-batches for path-based model 700 3861\n",
      "generating big-batches for path-based model 800 3861\n",
      "generating big-batches for path-based model 900 3861\n",
      "generating big-batches for path-based model 1000 3861\n",
      "generating big-batches for path-based model 1100 3861\n",
      "generating big-batches for path-based model 1200 3861\n",
      "generating big-batches for path-based model 1300 3861\n",
      "generating big-batches for path-based model 1400 3861\n",
      "generating big-batches for path-based model 1500 3861\n",
      "generating big-batches for path-based model 1600 3861\n",
      "generating big-batches for path-based model 1700 3861\n",
      "generating big-batches for path-based model 1800 3861\n",
      "generating big-batches for path-based model 1900 3861\n",
      "generating big-batches for path-based model 2000 3861\n",
      "generating big-batches for path-based model 2100 3861\n",
      "generating big-batches for path-based model 2200 3861\n",
      "generating big-batches for path-based model 2300 3861\n",
      "generating big-batches for path-based model 2400 3861\n",
      "generating big-batches for path-based model 2500 3861\n",
      "generating big-batches for path-based model 2600 3861\n",
      "generating big-batches for path-based model 2700 3861\n",
      "generating big-batches for path-based model 2800 3861\n",
      "generating big-batches for path-based model 2900 3861\n",
      "generating big-batches for path-based model 3000 3861\n",
      "generating big-batches for path-based model 3100 3861\n",
      "generating big-batches for path-based model 3200 3861\n",
      "generating big-batches for path-based model 3300 3861\n",
      "generating big-batches for path-based model 3400 3861\n",
      "generating big-batches for path-based model 3500 3861\n",
      "generating big-batches for path-based model 3600 3861\n",
      "generating big-batches for path-based model 3700 3861\n",
      "generating big-batches for path-based model 3800 3861\n",
      "generating big-batches for path-based model 100 1410\n",
      "generating big-batches for path-based model 200 1410\n",
      "generating big-batches for path-based model 300 1410\n",
      "generating big-batches for path-based model 400 1410\n",
      "generating big-batches for path-based model 500 1410\n",
      "generating big-batches for path-based model 600 1410\n",
      "generating big-batches for path-based model 700 1410\n",
      "generating big-batches for path-based model 800 1410\n",
      "generating big-batches for path-based model 900 1410\n",
      "generating big-batches for path-based model 1000 1410\n",
      "generating big-batches for path-based model 1100 1410\n",
      "generating big-batches for path-based model 1200 1410\n",
      "generating big-batches for path-based model 1300 1410\n",
      "generating big-batches for path-based model 1400 1410\n",
      "Epoch 1/10\n",
      "2853/2853 [==============================] - 249s 84ms/step - loss: 0.3199 - binary_accuracy: 0.8916 - val_loss: 0.2748 - val_binary_accuracy: 0.9044\n",
      "Epoch 2/10\n",
      "2853/2853 [==============================] - 241s 84ms/step - loss: 0.2389 - binary_accuracy: 0.9272 - val_loss: 0.2593 - val_binary_accuracy: 0.9310\n",
      "Epoch 3/10\n",
      "2853/2853 [==============================] - 240s 84ms/step - loss: 0.2166 - binary_accuracy: 0.9375 - val_loss: 0.2148 - val_binary_accuracy: 0.9373\n",
      "Epoch 4/10\n",
      "2853/2853 [==============================] - 244s 85ms/step - loss: 0.1956 - binary_accuracy: 0.9446 - val_loss: 0.2165 - val_binary_accuracy: 0.9366\n",
      "Epoch 5/10\n",
      "2853/2853 [==============================] - 242s 85ms/step - loss: 0.1713 - binary_accuracy: 0.9509 - val_loss: 0.1966 - val_binary_accuracy: 0.9353\n",
      "Epoch 6/10\n",
      "2853/2853 [==============================] - 228s 80ms/step - loss: 0.1536 - binary_accuracy: 0.9569 - val_loss: 0.1877 - val_binary_accuracy: 0.9506\n",
      "Epoch 7/10\n",
      "2853/2853 [==============================] - 214s 75ms/step - loss: 0.1304 - binary_accuracy: 0.9642 - val_loss: 0.1940 - val_binary_accuracy: 0.9474\n",
      "Epoch 8/10\n",
      "2853/2853 [==============================] - 215s 75ms/step - loss: 0.1150 - binary_accuracy: 0.9691 - val_loss: 0.2095 - val_binary_accuracy: 0.9479\n",
      "Epoch 9/10\n",
      "2853/2853 [==============================] - 2146s 752ms/step - loss: 0.1024 - binary_accuracy: 0.9733 - val_loss: 0.2187 - val_binary_accuracy: 0.9469\n",
      "Epoch 10/10\n",
      "2853/2853 [==============================] - 217s 76ms/step - loss: 0.0930 - binary_accuracy: 0.9766 - val_loss: 0.2212 - val_binary_accuracy: 0.9484\n",
      "Save model\n"
     ]
    }
   ],
   "source": [
    "###train the path-based model\n",
    "lower_bd = lower_bound\n",
    "upper_bd = upper_bound_path\n",
    "num_epoch = 10\n",
    "batch_size = 32\n",
    "        \n",
    "#define the training lists\n",
    "train_p_list, train_r_list, train_y_list = {'1': [], '2': [], '3': []}, list(), list()\n",
    "\n",
    "#define the validation lists\n",
    "valid_p_list, valid_r_list, valid_y_list = {'1': [], '2': [], '3': []}, list(), list()\n",
    "\n",
    "#######################################\n",
    "###build the big-batches###############      \n",
    "\n",
    "#fill in the training array list\n",
    "build_big_batches_path(lower_bd, upper_bd, data, one_hop, s_t_r,\n",
    "                      train_p_list, train_r_list, train_y_list,\n",
    "                      relation2id, entity2id, id2relation, id2entity)\n",
    "\n",
    "#fill in the validation array list\n",
    "build_big_batches_path(lower_bd, upper_bd, data_valid, one_hop_valid, s_t_r_valid,\n",
    "                      valid_p_list, valid_r_list, valid_y_list,\n",
    "                      relation2id, entity2id, id2relation, id2entity)    \n",
    "\n",
    "#######################################\n",
    "###do the training#####################\n",
    "#sometimes the validation dataset is so small so sparse, \n",
    "#which cannot find three paths between any pair of s and t.\n",
    "#in such a case, we will divide the training big-batch into train and valid\n",
    "if len(valid_y_list) >= 100:\n",
    "    #generate the input arrays\n",
    "    x_train_1 = np.asarray(train_p_list['1'], dtype='int')\n",
    "    x_train_2 = np.asarray(train_p_list['2'], dtype='int')\n",
    "    x_train_3 = np.asarray(train_p_list['3'], dtype='int')\n",
    "    x_train_r = np.asarray(train_r_list, dtype='int')\n",
    "    y_train = np.asarray(train_y_list, dtype='int')\n",
    "\n",
    "    #generate the validation arrays\n",
    "    x_valid_1 = np.asarray(valid_p_list['1'], dtype='int')\n",
    "    x_valid_2 = np.asarray(valid_p_list['2'], dtype='int')\n",
    "    x_valid_3 = np.asarray(valid_p_list['3'], dtype='int')\n",
    "    x_valid_r = np.asarray(valid_r_list, dtype='int')\n",
    "    y_valid = np.asarray(valid_y_list, dtype='int')\n",
    "\n",
    "else:\n",
    "    split = int(len(train_y_list)*0.8)\n",
    "    #generate the input arrays\n",
    "    x_train_1 = np.asarray(train_p_list['1'][:split], dtype='int')\n",
    "    x_train_2 = np.asarray(train_p_list['2'][:split], dtype='int')\n",
    "    x_train_3 = np.asarray(train_p_list['3'][:split], dtype='int')\n",
    "    x_train_r = np.asarray(train_r_list[:split], dtype='int')\n",
    "    y_train = np.asarray(train_y_list[:split], dtype='int')\n",
    "\n",
    "    #generate the validation arrays\n",
    "    x_valid_1 = np.asarray(train_p_list['1'][split:], dtype='int')\n",
    "    x_valid_2 = np.asarray(train_p_list['2'][split:], dtype='int')\n",
    "    x_valid_3 = np.asarray(train_p_list['3'][split:], dtype='int')\n",
    "    x_valid_r = np.asarray(train_r_list[split:], dtype='int')\n",
    "    y_valid = np.asarray(train_y_list[split:], dtype='int')\n",
    "\n",
    "#do the training\n",
    "model.fit([x_train_1, x_train_2, x_train_3, x_train_r], y_train, \n",
    "          validation_data=([x_valid_1, x_valid_2, x_valid_3, x_valid_r], y_valid),\n",
    "          batch_size=batch_size, epochs=num_epoch)   \n",
    "\n",
    "# Save model and weights\n",
    "add_h5 = model_name + '.h5'\n",
    "save_dir = os.path.join(os.getcwd(), '../weight_bin')\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, add_h5)\n",
    "model.save(model_path)\n",
    "print('Save model')\n",
    "del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b89853c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating and storing paths for the path-based model 100 3861\n",
      "generating and storing paths for the path-based model 200 3861\n",
      "generating and storing paths for the path-based model 300 3861\n",
      "generating and storing paths for the path-based model 400 3861\n",
      "generating and storing paths for the path-based model 500 3861\n",
      "generating and storing paths for the path-based model 600 3861\n",
      "generating and storing paths for the path-based model 700 3861\n",
      "generating and storing paths for the path-based model 800 3861\n",
      "generating and storing paths for the path-based model 900 3861\n",
      "generating and storing paths for the path-based model 1000 3861\n",
      "generating and storing paths for the path-based model 1100 3861\n",
      "generating and storing paths for the path-based model 1200 3861\n",
      "generating and storing paths for the path-based model 1300 3861\n",
      "generating and storing paths for the path-based model 1400 3861\n",
      "generating and storing paths for the path-based model 1500 3861\n",
      "generating and storing paths for the path-based model 1600 3861\n",
      "generating and storing paths for the path-based model 1700 3861\n",
      "generating and storing paths for the path-based model 1800 3861\n",
      "generating and storing paths for the path-based model 1900 3861\n",
      "generating and storing paths for the path-based model 2000 3861\n",
      "generating and storing paths for the path-based model 2100 3861\n",
      "generating and storing paths for the path-based model 2200 3861\n",
      "generating and storing paths for the path-based model 2300 3861\n",
      "generating and storing paths for the path-based model 2400 3861\n",
      "generating and storing paths for the path-based model 2500 3861\n",
      "generating and storing paths for the path-based model 2600 3861\n",
      "generating and storing paths for the path-based model 2700 3861\n",
      "generating and storing paths for the path-based model 2800 3861\n",
      "generating and storing paths for the path-based model 2900 3861\n",
      "generating and storing paths for the path-based model 3000 3861\n",
      "generating and storing paths for the path-based model 3100 3861\n",
      "generating and storing paths for the path-based model 3200 3861\n",
      "generating and storing paths for the path-based model 3300 3861\n",
      "generating and storing paths for the path-based model 3400 3861\n",
      "generating and storing paths for the path-based model 3500 3861\n",
      "generating and storing paths for the path-based model 3600 3861\n",
      "generating and storing paths for the path-based model 3700 3861\n",
      "generating and storing paths for the path-based model 3800 3861\n",
      "generating and storing paths for the path-based model 100 1410\n",
      "generating and storing paths for the path-based model 200 1410\n",
      "generating and storing paths for the path-based model 300 1410\n",
      "generating and storing paths for the path-based model 400 1410\n",
      "generating and storing paths for the path-based model 500 1410\n",
      "generating and storing paths for the path-based model 600 1410\n",
      "generating and storing paths for the path-based model 700 1410\n",
      "generating and storing paths for the path-based model 800 1410\n",
      "generating and storing paths for the path-based model 900 1410\n",
      "generating and storing paths for the path-based model 1000 1410\n",
      "generating and storing paths for the path-based model 1100 1410\n",
      "generating and storing paths for the path-based model 1200 1410\n",
      "generating and storing paths for the path-based model 1300 1410\n",
      "generating and storing paths for the path-based model 1400 1410\n",
      "generating big-batches for subgraph-based model 0 7940 0\n",
      "generating big-batches for subgraph-based model 2000 7940 0\n",
      "generating big-batches for subgraph-based model 4000 7940 0\n",
      "generating big-batches for subgraph-based model 6000 7940 0\n",
      "generating big-batches for subgraph-based model 0 7940 1\n",
      "generating big-batches for subgraph-based model 2000 7940 1\n",
      "generating big-batches for subgraph-based model 4000 7940 1\n",
      "generating big-batches for subgraph-based model 6000 7940 1\n",
      "generating big-batches for subgraph-based model 0 7940 2\n",
      "generating big-batches for subgraph-based model 2000 7940 2\n",
      "generating big-batches for subgraph-based model 4000 7940 2\n",
      "generating big-batches for subgraph-based model 6000 7940 2\n",
      "generating big-batches for subgraph-based model 0 7940 3\n",
      "generating big-batches for subgraph-based model 2000 7940 3\n",
      "generating big-batches for subgraph-based model 4000 7940 3\n",
      "generating big-batches for subgraph-based model 6000 7940 3\n",
      "generating big-batches for subgraph-based model 0 7940 4\n",
      "generating big-batches for subgraph-based model 2000 7940 4\n",
      "generating big-batches for subgraph-based model 4000 7940 4\n",
      "generating big-batches for subgraph-based model 6000 7940 4\n",
      "generating big-batches for subgraph-based model 0 7940 5\n",
      "generating big-batches for subgraph-based model 2000 7940 5\n",
      "generating big-batches for subgraph-based model 4000 7940 5\n",
      "generating big-batches for subgraph-based model 6000 7940 5\n",
      "generating big-batches for subgraph-based model 0 7940 6\n",
      "generating big-batches for subgraph-based model 2000 7940 6\n",
      "generating big-batches for subgraph-based model 4000 7940 6\n",
      "generating big-batches for subgraph-based model 6000 7940 6\n",
      "generating big-batches for subgraph-based model 0 7940 7\n",
      "generating big-batches for subgraph-based model 2000 7940 7\n",
      "generating big-batches for subgraph-based model 4000 7940 7\n",
      "generating big-batches for subgraph-based model 6000 7940 7\n",
      "generating big-batches for subgraph-based model 0 7940 8\n",
      "generating big-batches for subgraph-based model 2000 7940 8\n",
      "generating big-batches for subgraph-based model 4000 7940 8\n",
      "generating big-batches for subgraph-based model 6000 7940 8\n",
      "generating big-batches for subgraph-based model 0 7940 9\n",
      "generating big-batches for subgraph-based model 2000 7940 9\n",
      "generating big-batches for subgraph-based model 4000 7940 9\n",
      "generating big-batches for subgraph-based model 6000 7940 9\n",
      "generating big-batches for subgraph-based model 0 934 0\n",
      "generating big-batches for subgraph-based model 0 934 1\n",
      "generating big-batches for subgraph-based model 0 934 2\n",
      "generating big-batches for subgraph-based model 0 934 3\n",
      "generating big-batches for subgraph-based model 0 934 4\n",
      "generating big-batches for subgraph-based model 0 934 5\n",
      "generating big-batches for subgraph-based model 0 934 6\n",
      "generating big-batches for subgraph-based model 0 934 7\n",
      "generating big-batches for subgraph-based model 0 934 8\n",
      "generating big-batches for subgraph-based model 0 934 9\n",
      "Epoch 1/10\n",
      "14680/14680 [==============================] - 1561s 104ms/step - loss: 0.5072 - binary_accuracy: 0.7731 - val_loss: 0.4159 - val_binary_accuracy: 0.7900\n",
      "Epoch 2/10\n",
      "14680/14680 [==============================] - 1513s 103ms/step - loss: 0.4168 - binary_accuracy: 0.8079 - val_loss: 0.3984 - val_binary_accuracy: 0.8167\n",
      "Epoch 3/10\n",
      "14680/14680 [==============================] - 1515s 103ms/step - loss: 0.4019 - binary_accuracy: 0.8172 - val_loss: 0.3945 - val_binary_accuracy: 0.8053\n",
      "Epoch 4/10\n",
      "14680/14680 [==============================] - 1510s 103ms/step - loss: 0.3956 - binary_accuracy: 0.8214 - val_loss: 0.4221 - val_binary_accuracy: 0.7813\n",
      "Epoch 5/10\n",
      "14680/14680 [==============================] - 1521s 104ms/step - loss: 0.3917 - binary_accuracy: 0.8238 - val_loss: 0.3697 - val_binary_accuracy: 0.8233\n",
      "Epoch 6/10\n",
      "14680/14680 [==============================] - 1039s 71ms/step - loss: 0.3879 - binary_accuracy: 0.8255 - val_loss: 0.3727 - val_binary_accuracy: 0.8250\n",
      "Epoch 7/10\n",
      "14680/14680 [==============================] - 751s 51ms/step - loss: 0.3849 - binary_accuracy: 0.8271 - val_loss: 0.3740 - val_binary_accuracy: 0.8233\n",
      "Epoch 8/10\n",
      "14680/14680 [==============================] - 750s 51ms/step - loss: 0.3828 - binary_accuracy: 0.8282 - val_loss: 0.3648 - val_binary_accuracy: 0.8290\n",
      "Epoch 9/10\n",
      "14680/14680 [==============================] - 771s 53ms/step - loss: 0.3809 - binary_accuracy: 0.8297 - val_loss: 0.3720 - val_binary_accuracy: 0.8277\n",
      "Epoch 10/10\n",
      "14680/14680 [==============================] - 775s 53ms/step - loss: 0.3789 - binary_accuracy: 0.8309 - val_loss: 0.3625 - val_binary_accuracy: 0.8343\n",
      "Save model\n"
     ]
    }
   ],
   "source": [
    "###train the subgraph-based model\n",
    "lower_bd = lower_bound\n",
    "upper_bd = upper_bound_subg\n",
    "num_epoch = 10\n",
    "batch_size = 32\n",
    "\n",
    "Dict_train = store_subgraph_dicts(lower_bd, upper_bd, data, one_hop, s_t_r,\n",
    "                         relation2id, entity2id, id2relation, id2entity)\n",
    "\n",
    "Dict_valid = store_subgraph_dicts(lower_bd, upper_bd, data_valid, one_hop_valid, s_t_r_valid,\n",
    "                         relation2id, entity2id, id2relation, id2entity)\n",
    "        \n",
    "#define the training lists\n",
    "train_s_list, train_t_list, train_r_list, train_y_list = {'1': [], '2': [], '3': [], '4': [], '5': [], '6': []}, {'1': [], '2': [], '3': [], '4': [], '5': [], '6': []}, list(), list()\n",
    "\n",
    "#define the validation lists\n",
    "valid_s_list, valid_t_list, valid_r_list, valid_y_list = {'1': [], '2': [], '3': [], '4': [], '5': [], '6': []}, {'1': [], '2': [], '3': [], '4': [], '5': [], '6': []}, list(), list()\n",
    "\n",
    "#######################################\n",
    "###build the big-batches###############      \n",
    "\n",
    "#fill in the training array list\n",
    "build_big_batches_subgraph(lower_bd, upper_bd, data, one_hop, s_t_r,\n",
    "                      train_s_list, train_t_list, train_r_list, train_y_list, Dict_train,\n",
    "                      relation2id, entity2id, id2relation, id2entity)\n",
    "\n",
    "#fill in the validation array list\n",
    "build_big_batches_subgraph(lower_bd, upper_bd, data_valid, one_hop_valid, s_t_r_valid,\n",
    "                      valid_s_list, valid_t_list, valid_r_list, valid_y_list, Dict_valid,\n",
    "                      relation2id, entity2id, id2relation, id2entity)    \n",
    "\n",
    "#######################################\n",
    "###do the training#####################\n",
    "#sometimes the validation dataset is so small so sparse, \n",
    "#which cannot find three paths between any pair of s and t.\n",
    "#in such a case, we will divide the training big-batch into train and valid\n",
    "if len(valid_y_list) >= 100:\n",
    "    #generate the input arrays\n",
    "    x_train_s_1 = np.asarray(train_s_list['1'], dtype='int')\n",
    "    x_train_s_2 = np.asarray(train_s_list['2'], dtype='int')\n",
    "    x_train_s_3 = np.asarray(train_s_list['3'], dtype='int')\n",
    "    x_train_s_4 = np.asarray(train_s_list['4'], dtype='int')\n",
    "    x_train_s_5 = np.asarray(train_s_list['5'], dtype='int')\n",
    "    x_train_s_6 = np.asarray(train_s_list['6'], dtype='int')\n",
    "\n",
    "    x_train_t_1 = np.asarray(train_t_list['1'], dtype='int')\n",
    "    x_train_t_2 = np.asarray(train_t_list['2'], dtype='int')\n",
    "    x_train_t_3 = np.asarray(train_t_list['3'], dtype='int')\n",
    "    x_train_t_4 = np.asarray(train_t_list['4'], dtype='int')\n",
    "    x_train_t_5 = np.asarray(train_t_list['5'], dtype='int')\n",
    "    x_train_t_6 = np.asarray(train_t_list['6'], dtype='int')\n",
    "\n",
    "    x_train_r = np.asarray(train_r_list, dtype='int')\n",
    "    y_train = np.asarray(train_y_list, dtype='int')\n",
    "\n",
    "    #generate the validation arrays\n",
    "    x_valid_s_1 = np.asarray(valid_s_list['1'], dtype='int')\n",
    "    x_valid_s_2 = np.asarray(valid_s_list['2'], dtype='int')\n",
    "    x_valid_s_3 = np.asarray(valid_s_list['3'], dtype='int')\n",
    "    x_valid_s_4 = np.asarray(valid_s_list['4'], dtype='int')\n",
    "    x_valid_s_5 = np.asarray(valid_s_list['5'], dtype='int')\n",
    "    x_valid_s_6 = np.asarray(valid_s_list['6'], dtype='int')\n",
    "\n",
    "    x_valid_t_1 = np.asarray(valid_t_list['1'], dtype='int')\n",
    "    x_valid_t_2 = np.asarray(valid_t_list['2'], dtype='int')\n",
    "    x_valid_t_3 = np.asarray(valid_t_list['3'], dtype='int')\n",
    "    x_valid_t_4 = np.asarray(valid_t_list['4'], dtype='int')\n",
    "    x_valid_t_5 = np.asarray(valid_t_list['5'], dtype='int')\n",
    "    x_valid_t_6 = np.asarray(valid_t_list['6'], dtype='int')\n",
    "\n",
    "    x_valid_r = np.asarray(valid_r_list, dtype='int')\n",
    "    y_valid = np.asarray(valid_y_list, dtype='int')\n",
    "\n",
    "else:\n",
    "    split = int(len(train_y_list)*0.8)\n",
    "    #generate the input arrays\n",
    "    x_train_s_1 = np.asarray(train_s_list['1'][:split], dtype='int')\n",
    "    x_train_s_2 = np.asarray(train_s_list['2'][:split], dtype='int')\n",
    "    x_train_s_3 = np.asarray(train_s_list['3'][:split], dtype='int')\n",
    "    x_train_s_4 = np.asarray(train_s_list['4'][:split], dtype='int')\n",
    "    x_train_s_5 = np.asarray(train_s_list['5'][:split], dtype='int')\n",
    "    x_train_s_6 = np.asarray(train_s_list['6'][:split], dtype='int')\n",
    "\n",
    "    x_train_t_1 = np.asarray(train_t_list['1'][:split], dtype='int')\n",
    "    x_train_t_2 = np.asarray(train_t_list['2'][:split], dtype='int')\n",
    "    x_train_t_3 = np.asarray(train_t_list['3'][:split], dtype='int')\n",
    "    x_train_t_4 = np.asarray(train_t_list['4'][:split], dtype='int')\n",
    "    x_train_t_5 = np.asarray(train_t_list['5'][:split], dtype='int')\n",
    "    x_train_t_6 = np.asarray(train_t_list['6'][:split], dtype='int')\n",
    "\n",
    "    x_train_r = np.asarray(train_r_list[:split], dtype='int')\n",
    "    y_train = np.asarray(train_y_list[:split], dtype='int')\n",
    "\n",
    "    #generate the validation arrays\n",
    "    x_valid_s_1 = np.asarray(train_s_list['1'][split:], dtype='int')\n",
    "    x_valid_s_2 = np.asarray(train_s_list['2'][split:], dtype='int')\n",
    "    x_valid_s_3 = np.asarray(train_s_list['3'][split:], dtype='int')\n",
    "    x_valid_s_4 = np.asarray(train_s_list['4'][split:], dtype='int')\n",
    "    x_valid_s_5 = np.asarray(train_s_list['5'][split:], dtype='int')\n",
    "    x_valid_s_6 = np.asarray(train_s_list['6'][split:], dtype='int')\n",
    "\n",
    "    x_valid_t_1 = np.asarray(train_t_list['1'][split:], dtype='int')\n",
    "    x_valid_t_2 = np.asarray(train_t_list['2'][split:], dtype='int')\n",
    "    x_valid_t_3 = np.asarray(train_t_list['3'][split:], dtype='int')\n",
    "    x_valid_t_4 = np.asarray(train_t_list['4'][split:], dtype='int')\n",
    "    x_valid_t_5 = np.asarray(train_t_list['5'][split:], dtype='int')\n",
    "    x_valid_t_6 = np.asarray(train_t_list['6'][split:], dtype='int')\n",
    "\n",
    "    x_valid_r = np.asarray(train_r_list[split:], dtype='int')\n",
    "    y_valid = np.asarray(train_y_list[split:], dtype='int')\n",
    "\n",
    "#do the training\n",
    "model_2.fit([x_train_s_1, x_train_s_2, x_train_s_3, x_train_s_4, x_train_s_5, x_train_s_6,\n",
    "             x_train_t_1, x_train_t_2, x_train_t_3, x_train_t_4, x_train_t_5, x_train_t_6,\n",
    "             x_train_r], y_train, \n",
    "          validation_data=([x_valid_s_1, x_valid_s_2, x_valid_s_3, x_valid_s_4, x_valid_s_5, x_valid_s_6,\n",
    "                            x_valid_t_1, x_valid_t_2, x_valid_t_3, x_valid_t_4, x_valid_t_5, x_valid_t_6,\n",
    "                            x_valid_r], y_valid),\n",
    "          batch_size=batch_size, epochs=num_epoch)\n",
    "\n",
    "# Save model and weights\n",
    "one_hop_add_h5 = one_hop_model_name + '.h5'\n",
    "one_hop_save_dir = os.path.join(os.getcwd(), '../weight_bin')\n",
    "\n",
    "if not os.path.isdir(one_hop_save_dir):\n",
    "    os.makedirs(one_hop_save_dir)\n",
    "one_hop_model_path = os.path.join(one_hop_save_dir, one_hop_add_h5)\n",
    "model_2.save(one_hop_model_path)\n",
    "print('Save model')\n",
    "del(model_2, Dict_train, Dict_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a74957c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2178e81c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f9152c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cbcb96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4df421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867e4d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f86bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f1d82b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe787997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da50338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d51f71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9543feff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea551344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8526f0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ef907e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e528e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f4eca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9729af14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b107dff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c689768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14b6bf00",
   "metadata": {},
   "source": [
    "### Result on the testset for inductive link prediction\n",
    "\n",
    "We use the testset for inductive link prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "207ef966",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'fb237_v1'\n",
    "model_id = 'SiaLP_6_new'\n",
    "lower_bound = 1\n",
    "upper_bound_path = 10\n",
    "upper_bound_subg = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7e2a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#difine the names for saving\n",
    "model_name = 'Model_' + model_id + '_' + data_name\n",
    "one_hop_model_name = 'One_hop_model_' + model_id + '_' + data_name\n",
    "ids_name = 'IDs_' + model_id + '_' + data_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c59ddf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IDs_SiaLP_6_new_fb237_v1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae165f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One_hop_model_SiaLP_6_new_fb237_v1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hop_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f87cc2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Model_SiaLP_6_new_fb237_v1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f959af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import opensmile\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37c81c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadKG:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.x = 'Hello'\n",
    "        \n",
    "    def load_train_data(self, data_path, one_hop, data, s_t_r, entity2id, id2entity,\n",
    "                     relation2id, id2relation):\n",
    "        \n",
    "        data_ = set()\n",
    "    \n",
    "        ####load the train, valid and test set##########\n",
    "        with open (data_path, 'r') as f:\n",
    "            \n",
    "            data_ini = f.readlines()\n",
    "                        \n",
    "            for i in range(len(data_ini)):\n",
    "            \n",
    "                x = data_ini[i].split()\n",
    "                \n",
    "                x_ = tuple(x)\n",
    "                \n",
    "                data_.add(x_)\n",
    "        \n",
    "        ####relation dict#################\n",
    "        index = len(relation2id)\n",
    "     \n",
    "        for key in data_:\n",
    "            \n",
    "            if key[1] not in relation2id:\n",
    "                \n",
    "                relation = key[1]\n",
    "                \n",
    "                relation2id[relation] = index\n",
    "                \n",
    "                id2relation[index] = relation\n",
    "                \n",
    "                index += 1\n",
    "                \n",
    "                #the inverse relation\n",
    "                iv_r = '_inverse_' + relation\n",
    "                \n",
    "                relation2id[iv_r] = index\n",
    "                \n",
    "                id2relation[index] = iv_r\n",
    "                \n",
    "                index += 1\n",
    "        \n",
    "        #get the id of the inverse relation, by above definition, initial relation has \n",
    "        #always even id, while inverse relation has always odd id.\n",
    "        def inverse_r(r):\n",
    "            \n",
    "            if r % 2 == 0: #initial relation\n",
    "                \n",
    "                iv_r = r + 1\n",
    "            \n",
    "            else: #inverse relation\n",
    "                \n",
    "                iv_r = r - 1\n",
    "            \n",
    "            return(iv_r)\n",
    "        \n",
    "        ####entity dict###################\n",
    "        index = len(entity2id)\n",
    "        \n",
    "        for key in data_:\n",
    "            \n",
    "            source, target = key[0], key[2]\n",
    "            \n",
    "            if source not in entity2id:\n",
    "                                \n",
    "                entity2id[source] = index\n",
    "                \n",
    "                id2entity[index] = source\n",
    "                \n",
    "                index += 1\n",
    "            \n",
    "            if target not in entity2id:\n",
    "                \n",
    "                entity2id[target] = index\n",
    "                \n",
    "                id2entity[index] = target\n",
    "                \n",
    "                index += 1\n",
    "                \n",
    "        #create the set of triples using id instead of string        \n",
    "        for ele in data_:\n",
    "            \n",
    "            s = entity2id[ele[0]]\n",
    "            \n",
    "            r = relation2id[ele[1]]\n",
    "            \n",
    "            t = entity2id[ele[2]]\n",
    "            \n",
    "            if (s,r,t) not in data:\n",
    "                \n",
    "                data.add((s,r,t))\n",
    "            \n",
    "            s_t_r[(s,t)].add(r)\n",
    "            \n",
    "            if s not in one_hop:\n",
    "                \n",
    "                one_hop[s] = set()\n",
    "            \n",
    "            one_hop[s].add((r,t))\n",
    "            \n",
    "            if t not in one_hop:\n",
    "                \n",
    "                one_hop[t] = set()\n",
    "            \n",
    "            r_inv = inverse_r(r)\n",
    "            \n",
    "            s_t_r[(t,s)].add(r_inv)\n",
    "            \n",
    "            one_hop[t].add((r_inv,s))\n",
    "            \n",
    "        #change each set in one_hop to list\n",
    "        for e in one_hop:\n",
    "            \n",
    "            one_hop[e] = list(one_hop[e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a714e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObtainPathsByDynamicProgramming:\n",
    "\n",
    "    def __init__(self, amount_bd=50, size_bd=50, threshold=20000):\n",
    "        \n",
    "        self.amount_bd = amount_bd #how many Tuples we choose in one_hop[node] for next recursion\n",
    "                        \n",
    "        self.size_bd = size_bd #size bound limit the number of paths to a target entity t\n",
    "        \n",
    "        #number of times paths with specific length been performed for recursion\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    '''\n",
    "    Given an entity s, the function will find the paths from s to other entities, using recursion.\n",
    "    \n",
    "    One may refer to LeetCode Problem 797 for details:\n",
    "        https://leetcode.com/problems/all-paths-from-source-to-target/\n",
    "    '''\n",
    "    def obtain_paths(self, mode, s, t_input, lower_bd, upper_bd, one_hop):\n",
    "\n",
    "        if type(lower_bd) != type(1) or lower_bd < 1:\n",
    "            \n",
    "            raise TypeError(\"!!! invalid lower bound setting, must >= 1 !!!\")\n",
    "            \n",
    "        if type(upper_bd) != type(1) or upper_bd < 1:\n",
    "            \n",
    "            raise TypeError(\"!!! invalid upper bound setting, must >= 1 !!!\")\n",
    "            \n",
    "        if lower_bd > upper_bd:\n",
    "            \n",
    "            raise TypeError(\"!!! lower bound must not exced upper bound !!!\")\n",
    "            \n",
    "        if s not in one_hop:\n",
    "            \n",
    "            raise ValueError('!!! entity not in one_hop. Please work on existing entities')\n",
    "\n",
    "        #here is the result dict. Its key is each entity t sharing paths from s\n",
    "        #The value of each t is a set containing the paths from s to t\n",
    "        #These paths can be either the direct connection r, or a multi-hop path\n",
    "        res = defaultdict(set)\n",
    "        \n",
    "        #qualified_t contains the types of t we want to consider,\n",
    "        #that is, what t will be added to the result set.\n",
    "        qualified_t = set()\n",
    "\n",
    "        #under this mode, we will only consider the direct neighbour of s\n",
    "        if mode == 'direct_neighbour':\n",
    "        \n",
    "            for Tuple in one_hop[s]:\n",
    "            \n",
    "                t = Tuple[1]\n",
    "                \n",
    "                qualified_t.add(t)\n",
    "        \n",
    "        #under this mode, we will only consider one specified entity t\n",
    "        elif mode == 'target_specified':\n",
    "            \n",
    "            qualified_t.add(t_input)\n",
    "        \n",
    "        #under this mode, we will consider any entity\n",
    "        elif mode == 'any_target':\n",
    "            \n",
    "            for s_any in one_hop:\n",
    "                \n",
    "                qualified_t.add(s_any)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            raise ValueError('not a valid mode')\n",
    "        \n",
    "        '''\n",
    "        We use recursion to find the paths\n",
    "        On current node with the path [r1, ..., rk] and on-path entities {s, e1, ..., ek-1, node}\n",
    "        from s to this node, we will further find the direct neighbor t' of this node. \n",
    "        If t' is not an on-path entity (not among s, e1,...ek-1, node), we recursively proceed to t' \n",
    "        '''\n",
    "        def helper(node, path, on_path_en, res, qualified_t, lower_bd, upper_bd, one_hop, count_dict):\n",
    "\n",
    "            #when the current path is within lower_bd and upper_bd, \n",
    "            #and the node is among the qualified t, and it has not been fill of paths w.r.t size_limit,\n",
    "            #we will add this path to the node\n",
    "            if (len(path) >= lower_bd) and (len(path) <= upper_bd) and (\n",
    "                node in qualified_t) and (len(res[node]) < self.size_bd):\n",
    "                \n",
    "                res[node].add(tuple(path))\n",
    "                    \n",
    "            #won't start new recursions if the current path length already reaches upper limit\n",
    "            #or the number of recursions performed on this length has reached the limit\n",
    "            if (len(path) < upper_bd) and (count_dict[len(path)] <= self.threshold):\n",
    "                                \n",
    "                #temp list is the id list for us to go-over one_hop[node]\n",
    "                temp_list = [i for i in range(len(one_hop[node]))]\n",
    "                random.shuffle(temp_list) #so we random-shuffle the list\n",
    "                \n",
    "                #only take 20 recursions if there are too many (r,t)\n",
    "                for i in temp_list[:self.amount_bd]:\n",
    "                    \n",
    "                    #obtain tuple of (r,t)\n",
    "                    Tuple = one_hop[node][i]\n",
    "                    r, t = Tuple[0], Tuple[1]\n",
    "                    \n",
    "                    #add to count_dict even if eventually this step not proceed\n",
    "                    count_dict[len(path)] += 1\n",
    "                    \n",
    "                    #if t not on the path and we not exceed the computation threshold, \n",
    "                    #then finally proceed to next recursion\n",
    "                    if (t not in on_path_en) and (count_dict[len(path)] <= self.threshold):\n",
    "\n",
    "                        helper(t, path + [r], on_path_en.union({t}), res, qualified_t, \n",
    "                               lower_bd, upper_bd, one_hop, count_dict)\n",
    "\n",
    "        length_dict = defaultdict(int)\n",
    "        count_dict = defaultdict(int)\n",
    "        \n",
    "        helper(s, [], {s}, res, qualified_t, lower_bd, upper_bd, one_hop, count_dict)\n",
    "        \n",
    "        return(res, count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0d1f8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the classes\n",
    "Class_1 = LoadKG()\n",
    "Class_2 = ObtainPathsByDynamicProgramming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10f13661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load ids and relation/entity dicts\n",
    "with open('../weight_bin/' + ids_name + '.pickle', 'rb') as handle:\n",
    "    Dict = pickle.load(handle)\n",
    "    \n",
    "#save training data\n",
    "one_hop = Dict['one_hop']\n",
    "data = Dict['data']\n",
    "s_t_r = Dict['s_t_r']\n",
    "\n",
    "#save valid data\n",
    "one_hop_valid = Dict['one_hop_valid']\n",
    "data_valid = Dict['data_valid']\n",
    "s_t_r_valid = Dict['s_t_r_valid']\n",
    "\n",
    "#save test data\n",
    "one_hop_test = Dict['one_hop_test']\n",
    "data_test = Dict['data_test']\n",
    "s_t_r_test = Dict['s_t_r_test']\n",
    "\n",
    "#save shared dictionaries\n",
    "entity2id = Dict['entity2id']\n",
    "id2entity = Dict['id2entity']\n",
    "relation2id = Dict['relation2id']\n",
    "id2relation = Dict['id2relation']\n",
    "\n",
    "#we want to keep the initial entity/relation dicts before adding new entities\n",
    "entity2id_ini = deepcopy(entity2id)\n",
    "id2entity_ini = deepcopy(id2entity)\n",
    "relation2id_ini = deepcopy(relation2id)\n",
    "id2relation_ini = deepcopy(id2relation)\n",
    "\n",
    "num_r = len(id2relation)\n",
    "num_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74048a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IDs_SiaLP_6_new_fb237_v1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "027883e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Model_SiaLP_6_new_fb237_v1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50c64cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 13:17:47.820289: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#load the model\n",
    "model = keras.models.load_model('../weight_bin/' + model_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "378e34e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the one-hop neighbor model\n",
    "model_2 = keras.models.load_model('../weight_bin/' + one_hop_model_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2ea521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train_path = '../data/' + data_name + '_ind/train.txt'\n",
    "ind_valid_path = '../data/' + data_name + '_ind/valid.txt'\n",
    "ind_test_path = '../data/' + data_name + '_ind/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d2e6087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the test dataset\n",
    "one_hop_ind = dict() \n",
    "data_ind = set()\n",
    "s_t_r_ind = defaultdict(set)\n",
    "\n",
    "len_0 = len(relation2id)\n",
    "size_0 = len(entity2id)\n",
    "\n",
    "#fill in the sets and dicts\n",
    "Class_1.load_train_data(ind_train_path, \n",
    "                        one_hop_ind, data_ind, s_t_r_ind,\n",
    "                        entity2id, id2entity, relation2id, id2relation)\n",
    "\n",
    "len_1 = len(relation2id)\n",
    "size_1 = len(entity2id)\n",
    "\n",
    "if len_0 != len_1:\n",
    "    raise ValueError('unseen relation!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a6dfe59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1594 2687 1993\n"
     ]
    }
   ],
   "source": [
    "print(size_0, size_1, len(data_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63cec98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the test dataset\n",
    "one_hop_ind_test = dict() \n",
    "data_ind_test = set()\n",
    "s_t_r_ind_test = defaultdict(set)\n",
    "\n",
    "len_0 = len(relation2id)\n",
    "size_0 = len(entity2id)\n",
    "\n",
    "#fill in the sets and dicts\n",
    "Class_1.load_train_data(ind_test_path, \n",
    "                        one_hop_ind_test, data_ind_test, s_t_r_ind_test,\n",
    "                        entity2id, id2entity, relation2id, id2relation)\n",
    "\n",
    "\n",
    "len_1 = len(relation2id)\n",
    "size_1 = len(entity2id)\n",
    "\n",
    "if len_0 != len_1:\n",
    "    raise ValueError('unseen relation!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d18fee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2687 2687 205\n"
     ]
    }
   ],
   "source": [
    "print(size_0, size_1, len(data_ind_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "757526ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the validation for existing triple removal when ranking\n",
    "one_hop_ind_valid = dict() \n",
    "data_ind_valid = set()\n",
    "s_t_r_ind_valid = defaultdict(set)\n",
    "\n",
    "len_0 = len(relation2id)\n",
    "size_0 = len(entity2id)\n",
    "\n",
    "#fill in the sets and dicts\n",
    "Class_1.load_train_data(ind_valid_path, \n",
    "                        one_hop_ind_valid, data_ind_valid, s_t_r_ind_valid,\n",
    "                        entity2id, id2entity, relation2id, id2relation)\n",
    "\n",
    "len_1 = len(relation2id)\n",
    "size_1 = len(entity2id)\n",
    "\n",
    "if len_0 != len_1:\n",
    "    raise ValueError('unseen relation!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2980a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2687 2687 206\n"
     ]
    }
   ],
   "source": [
    "print(size_0, size_1, len(data_ind_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f17ff08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2687 1594\n"
     ]
    }
   ],
   "source": [
    "print(len(entity2id), len(entity2id_ini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd1cda6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1594 1093 2687\n"
     ]
    }
   ],
   "source": [
    "#obtain all the inital entities and new entities\n",
    "ini_ent_set, new_ent_set, all_ent_set = set(), set(), set()\n",
    "\n",
    "for ID in id2entity:\n",
    "    all_ent_set.add(ID)\n",
    "    if ID in id2entity_ini:\n",
    "        ini_ent_set.add(ID)\n",
    "    else:\n",
    "        new_ent_set.add(ID)\n",
    "        \n",
    "print(len(ini_ent_set), len(new_ent_set), len(all_ent_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "915ad29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we want to check whether there are overlapping \n",
    "#between the entities of train triples and inductive test and valid triples\n",
    "overlapping = 0\n",
    "\n",
    "for ele in data_ind_test:\n",
    "    \n",
    "    s, r, t = ele[0], ele[1], ele[2]\n",
    "    \n",
    "    if s in id2entity_ini or t in id2entity_ini:\n",
    "        \n",
    "        overlapping += 1\n",
    "        \n",
    "overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80eb1e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlapping = 0\n",
    "\n",
    "for ele in data_ind_valid:\n",
    "    \n",
    "    s, r, t = ele[0], ele[1], ele[2]\n",
    "    \n",
    "    if s in id2entity_ini or t in id2entity_ini:\n",
    "        \n",
    "        overlapping += 1\n",
    "        \n",
    "overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a3c9dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we want to check whether there are overlapping \n",
    "#between the entities of train triples and inductive test and valid triples\n",
    "overlapping = 0\n",
    "\n",
    "for ele in data_ind:\n",
    "    \n",
    "    s, r, t = ele[0], ele[1], ele[2]\n",
    "    \n",
    "    if s in id2entity_ini or t in id2entity_ini:\n",
    "        \n",
    "        overlapping += 1\n",
    "        \n",
    "overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83ea5533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the function to do path-based relation scoring\n",
    "def path_based_relation_scoring(s, t, lower_bd, upper_bd, one_hop, id2relation, model):\n",
    "    \n",
    "    path_holder = set()\n",
    "    \n",
    "    for iteration in range(3):\n",
    "    \n",
    "        result, length_dict = Class_2.obtain_paths('target_specified', \n",
    "                                                   s, t, lower_bd, upper_bd, one_hop)\n",
    "        if t in result:\n",
    "            \n",
    "            for path in result[t]:\n",
    "                \n",
    "                path_holder.add(path)\n",
    "                \n",
    "        del(result, length_dict)\n",
    "    \n",
    "    path_holder = list(path_holder)\n",
    "    random.shuffle(path_holder)\n",
    "    \n",
    "    score_dict = defaultdict(float)\n",
    "    count_dict = defaultdict(int)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    if len(path_holder) >= 3:\n",
    "    \n",
    "        #iterate over path_1\n",
    "        while count < 10:\n",
    "\n",
    "            temp_pair = random.sample(path_holder, 3)\n",
    "\n",
    "            path_1, path_2, path_3 = temp_pair[0], temp_pair[1], temp_pair[2]\n",
    "\n",
    "            list_1 = list()\n",
    "            list_2 = list()\n",
    "            list_3 = list()\n",
    "            list_r = list()\n",
    "\n",
    "            for i in range(len(id2relation)):\n",
    "\n",
    "                if i not in id2relation:\n",
    "\n",
    "                    raise ValueError ('error when generating id2relation')\n",
    "                \n",
    "                #only care about initial relations\n",
    "                if i % 2 == 0:\n",
    "\n",
    "                    list_1.append(list(path_1) + [num_r]*abs(len(path_1)-upper_bd))\n",
    "                    list_2.append(list(path_2) + [num_r]*abs(len(path_2)-upper_bd))\n",
    "                    list_3.append(list(path_3) + [num_r]*abs(len(path_3)-upper_bd))\n",
    "                    list_r.append([i])\n",
    "            \n",
    "            #change to arrays\n",
    "            input_1 = np.array(list_1)\n",
    "            input_2 = np.array(list_2)\n",
    "            input_3 = np.array(list_3)\n",
    "            input_r = np.array(list_r)\n",
    "\n",
    "            pred = model.predict([input_1, input_2, input_3, input_r], verbose = 0)\n",
    "\n",
    "            for i in range(pred.shape[0]):\n",
    "                #need to times 2 to go back to relation id from pred position\n",
    "                score_dict[2*i] += float(pred[i])\n",
    "                count_dict[2*i] += 1\n",
    "\n",
    "            count += 1\n",
    "            \n",
    "    #average the score\n",
    "    for r in score_dict:\n",
    "        score_dict[r] = deepcopy(score_dict[r]/float(count_dict[r]))\n",
    "    \n",
    "    print(len(score_dict), len(path_holder))\n",
    "\n",
    "    return(score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1e1b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the function to do path-based triple scoring: input one triple\n",
    "def path_based_triple_scoring(s, r, t, lower_bd, upper_bd, one_hop, id2relation, model):\n",
    "    \n",
    "    path_holder = set()\n",
    "    \n",
    "    for iteration in range(3):\n",
    "    \n",
    "        result, length_dict = Class_2.obtain_paths('target_specified', \n",
    "                                                   s, t, lower_bd, upper_bd, one_hop)\n",
    "        if t in result:\n",
    "            \n",
    "            for path in result[t]:\n",
    "                \n",
    "                path_holder.add(path)\n",
    "                \n",
    "        del(result, length_dict)\n",
    "    \n",
    "    path_holder = list(path_holder)\n",
    "    random.shuffle(path_holder)\n",
    "    \n",
    "    score = 0.\n",
    "    count = 0\n",
    "    \n",
    "    if len(path_holder) >= 3:\n",
    "        \n",
    "        list_1 = list()\n",
    "        list_2 = list()\n",
    "        list_3 = list()\n",
    "        list_r = list()\n",
    "    \n",
    "        #iterate over path_1\n",
    "        while count < 10:\n",
    "\n",
    "            temp_pair = random.sample(path_holder, 3)\n",
    "            path_1, path_2, path_3 = temp_pair[0], temp_pair[1], temp_pair[2]\n",
    "\n",
    "            list_1.append(list(path_1) + [num_r]*abs(len(path_1)-upper_bd))\n",
    "            list_2.append(list(path_2) + [num_r]*abs(len(path_2)-upper_bd))\n",
    "            list_3.append(list(path_3) + [num_r]*abs(len(path_3)-upper_bd))\n",
    "            list_r.append([r])\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "        #change to arrays\n",
    "        input_1 = np.array(list_1)\n",
    "        input_2 = np.array(list_2)\n",
    "        input_3 = np.array(list_3)\n",
    "        input_r = np.array(list_r)\n",
    "\n",
    "        pred = model.predict([input_1, input_2, input_3, input_r], verbose = 0)\n",
    "\n",
    "        for i in range(pred.shape[0]):\n",
    "            score += float(pred[i])\n",
    "            \n",
    "        #average the score\n",
    "        score = score/float(count)\n",
    "\n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8512335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subgraph based relation scoring\n",
    "def subgraph_relation_scoring(s, t, lower_bd, upper_bd, one_hop, id2relation, model_2):\n",
    "    \n",
    "    path_s, path_t = set(), set() #sets holding all the paths from s or t\n",
    "    \n",
    "    for iteration in range(3):\n",
    "    \n",
    "        #obtain the paths out from s or t by \"any target\" mode. That is, \n",
    "        result_s, length_dict_s = Class_2.obtain_paths('any_target', s, 'any', lower_bd, upper_bd, one_hop)\n",
    "        result_t, length_dict_t = Class_2.obtain_paths('any_target', t, 'any', lower_bd, upper_bd, one_hop)\n",
    "\n",
    "        #add paths to the source/target path_set\n",
    "        for e in result_s:\n",
    "            for path in result_s[e]:\n",
    "                path_s.add(path)\n",
    "        for e in result_t:\n",
    "            for path in result_t[e]:\n",
    "                path_t.add(path)\n",
    "                \n",
    "        del(result_s, length_dict_s, result_t, length_dict_t)\n",
    "    \n",
    "    #final output: the score dict\n",
    "    score_dict = defaultdict(float)\n",
    "    count_dict = defaultdict(int)\n",
    "    \n",
    "    #see if both path_s and path_t have at least three paths\n",
    "    if len(path_s) >= 6 and len(path_t) >= 6:\n",
    "\n",
    "        #change to lists\n",
    "        path_s, path_t = list(path_s), list(path_t)\n",
    "        \n",
    "        count = 0\n",
    "        while count < 10:\n",
    "            \n",
    "            #lists holding the input to the network\n",
    "            list_s_1 = list()\n",
    "            list_s_2 = list()\n",
    "            list_s_3 = list()\n",
    "            list_s_4 = list()\n",
    "            list_s_5 = list()\n",
    "            list_s_6 = list()\n",
    "            \n",
    "            list_t_1 = list()\n",
    "            list_t_2 = list()\n",
    "            list_t_3 = list()\n",
    "            list_t_4 = list()\n",
    "            list_t_5 = list()\n",
    "            list_t_6 = list()\n",
    "\n",
    "            list_r = list()\n",
    "\n",
    "            #randomly obtain three paths\n",
    "            temp_s = random.sample(path_s, 6)\n",
    "            temp_t = random.sample(path_t, 6)\n",
    "            s_p_1, s_p_2, s_p_3, s_p_4, s_p_5, s_p_6 = temp_s[0], temp_s[1], temp_s[2], temp_s[3], temp_s[4], temp_s[5]\n",
    "            t_p_1, t_p_2, t_p_3, t_p_4, t_p_5, t_p_6 = temp_t[0], temp_t[1], temp_t[2], temp_t[3], temp_t[4], temp_t[5]\n",
    "\n",
    "            #add all forward (initial relation)\n",
    "            for i in range(len(id2relation)):\n",
    "\n",
    "                if i not in id2relation:\n",
    "\n",
    "                    raise ValueError ('error when generating id2relation')\n",
    "                    \n",
    "                if i % 2 == 0:\n",
    "\n",
    "                    #append the paths: note that we add the space holder id at the end of the shorter path\n",
    "                    list_s_1.append(list(s_p_1) + [num_r]*abs(len(s_p_1)-upper_bd))\n",
    "                    list_s_2.append(list(s_p_2) + [num_r]*abs(len(s_p_2)-upper_bd))\n",
    "                    list_s_3.append(list(s_p_3) + [num_r]*abs(len(s_p_3)-upper_bd))\n",
    "                    list_s_4.append(list(s_p_4) + [num_r]*abs(len(s_p_4)-upper_bd))\n",
    "                    list_s_5.append(list(s_p_5) + [num_r]*abs(len(s_p_5)-upper_bd))\n",
    "                    list_s_6.append(list(s_p_6) + [num_r]*abs(len(s_p_6)-upper_bd))\n",
    "                    \n",
    "                    list_t_1.append(list(t_p_1) + [num_r]*abs(len(t_p_1)-upper_bd))\n",
    "                    list_t_2.append(list(t_p_2) + [num_r]*abs(len(t_p_2)-upper_bd))\n",
    "                    list_t_3.append(list(t_p_3) + [num_r]*abs(len(t_p_3)-upper_bd))                    \n",
    "                    list_t_4.append(list(t_p_4) + [num_r]*abs(len(t_p_4)-upper_bd))                    \n",
    "                    list_t_5.append(list(t_p_5) + [num_r]*abs(len(t_p_5)-upper_bd))                    \n",
    "                    list_t_6.append(list(t_p_6) + [num_r]*abs(len(t_p_6)-upper_bd))                                    \n",
    "                    \n",
    "                    list_r.append([i])\n",
    "                \n",
    "            #change to arrays\n",
    "            input_s_1 = np.array(list_s_1)\n",
    "            input_s_2 = np.array(list_s_2)\n",
    "            input_s_3 = np.array(list_s_3)\n",
    "            input_s_4 = np.array(list_s_4)\n",
    "            input_s_5 = np.array(list_s_5)\n",
    "            input_s_6 = np.array(list_s_6)\n",
    "            \n",
    "            input_t_1 = np.array(list_t_1)\n",
    "            input_t_2 = np.array(list_t_2)\n",
    "            input_t_3 = np.array(list_t_3)\n",
    "            input_t_4 = np.array(list_t_4)\n",
    "            input_t_5 = np.array(list_t_5)\n",
    "            input_t_6 = np.array(list_t_6)\n",
    "            \n",
    "            input_r = np.array(list_r)\n",
    "            \n",
    "            pred = model_2.predict([input_s_1, input_s_2, input_s_3, input_s_4,\n",
    "                                    input_s_5, input_s_6,\n",
    "                                    input_t_1, input_t_2, input_t_3, input_t_4,\n",
    "                                    input_t_5, input_t_6,\n",
    "                                    input_r], verbose = 0)\n",
    "\n",
    "            for i in range(pred.shape[0]):\n",
    "                #need to times 2 to go back to relation id from pred position\n",
    "                score_dict[2*i] += float(pred[i])\n",
    "                count_dict[2*i] += 1\n",
    "\n",
    "            count += 1\n",
    "            \n",
    "    #average the score\n",
    "    for r in score_dict:\n",
    "        score_dict[r] = deepcopy(score_dict[r]/float(count_dict[r]))\n",
    "            \n",
    "    print(len(score_dict), len(path_s), len(path_t))\n",
    "        \n",
    "    return(score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26015662",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subgraph based triple scoring\n",
    "def subgraph_triple_scoring(s, r, t, lower_bd, upper_bd, one_hop, id2relation, model_2):\n",
    "    \n",
    "    path_s, path_t = set(), set() #sets holding all the paths from s or t\n",
    "    \n",
    "    for iteration in range(3):\n",
    "    \n",
    "        #obtain the paths out from s or t by \"any target\" mode. That is, \n",
    "        result_s, length_dict_s = Class_2.obtain_paths('any_target', s, 'any', lower_bd, upper_bd, one_hop)\n",
    "        result_t, length_dict_t = Class_2.obtain_paths('any_target', t, 'any', lower_bd, upper_bd, one_hop)\n",
    "\n",
    "        #add paths to the source/target path_set\n",
    "        for e in result_s:\n",
    "            for path in result_s[e]:\n",
    "                path_s.add(path)\n",
    "        for e in result_t:\n",
    "            for path in result_t[e]:\n",
    "                path_t.add(path)\n",
    "                \n",
    "        del(result_s, length_dict_s, result_t, length_dict_t)\n",
    "    \n",
    "    #final output: the score dict\n",
    "    score = 0.\n",
    "    \n",
    "    #see if both path_s and path_t have at least three paths\n",
    "    if len(path_s) >= 6 and len(path_t) >= 6:\n",
    "\n",
    "        #change to lists\n",
    "        path_s, path_t = list(path_s), list(path_t)\n",
    "        \n",
    "        #lists holding the input to the network\n",
    "        list_s_1 = list()\n",
    "        list_s_2 = list()\n",
    "        list_s_3 = list()\n",
    "        list_s_4 = list()\n",
    "        list_s_5 = list()\n",
    "        list_s_6 = list()\n",
    "\n",
    "        list_t_1 = list()\n",
    "        list_t_2 = list()\n",
    "        list_t_3 = list()\n",
    "        list_t_4 = list()\n",
    "        list_t_5 = list()\n",
    "        list_t_6 = list()\n",
    "        \n",
    "        list_r = list()\n",
    "        \n",
    "        count = 0\n",
    "        while count < 10:\n",
    "\n",
    "            #randomly obtain three paths\n",
    "            temp_s = random.sample(path_s, 6)\n",
    "            temp_t = random.sample(path_t, 6)\n",
    "            s_p_1, s_p_2, s_p_3, s_p_4, s_p_5, s_p_6 = temp_s[0], temp_s[1], temp_s[2], temp_s[3], temp_s[4], temp_s[5]\n",
    "            t_p_1, t_p_2, t_p_3, t_p_4, t_p_5, t_p_6 = temp_t[0], temp_t[1], temp_t[2], temp_t[3], temp_t[4], temp_t[5]\n",
    "\n",
    "            #append the paths: note that we add the space holder id at the end of the shorter path\n",
    "            list_s_1.append(list(s_p_1) + [num_r]*abs(len(s_p_1)-upper_bd))\n",
    "            list_s_2.append(list(s_p_2) + [num_r]*abs(len(s_p_2)-upper_bd))\n",
    "            list_s_3.append(list(s_p_3) + [num_r]*abs(len(s_p_3)-upper_bd))\n",
    "            list_s_4.append(list(s_p_4) + [num_r]*abs(len(s_p_4)-upper_bd))\n",
    "            list_s_5.append(list(s_p_5) + [num_r]*abs(len(s_p_5)-upper_bd))\n",
    "            list_s_6.append(list(s_p_6) + [num_r]*abs(len(s_p_6)-upper_bd))\n",
    "\n",
    "            list_t_1.append(list(t_p_1) + [num_r]*abs(len(t_p_1)-upper_bd))\n",
    "            list_t_2.append(list(t_p_2) + [num_r]*abs(len(t_p_2)-upper_bd))\n",
    "            list_t_3.append(list(t_p_3) + [num_r]*abs(len(t_p_3)-upper_bd))                    \n",
    "            list_t_4.append(list(t_p_4) + [num_r]*abs(len(t_p_4)-upper_bd))                    \n",
    "            list_t_5.append(list(t_p_5) + [num_r]*abs(len(t_p_5)-upper_bd))                    \n",
    "            list_t_6.append(list(t_p_6) + [num_r]*abs(len(t_p_6)-upper_bd))                    \n",
    "\n",
    "            list_r.append([r])\n",
    "            count += 1\n",
    "                \n",
    "        #change to arrays\n",
    "        input_s_1 = np.array(list_s_1)\n",
    "        input_s_2 = np.array(list_s_2)\n",
    "        input_s_3 = np.array(list_s_3)\n",
    "        input_s_4 = np.array(list_s_4)\n",
    "        input_s_5 = np.array(list_s_5)\n",
    "        input_s_6 = np.array(list_s_6)\n",
    "\n",
    "        input_t_1 = np.array(list_t_1)\n",
    "        input_t_2 = np.array(list_t_2)\n",
    "        input_t_3 = np.array(list_t_3)\n",
    "        input_t_4 = np.array(list_t_4)\n",
    "        input_t_5 = np.array(list_t_5)\n",
    "        input_t_6 = np.array(list_t_6)\n",
    "        \n",
    "        input_r = np.array(list_r)\n",
    "\n",
    "        pred = model_2.predict([input_s_1, input_s_2, input_s_3, input_s_4,\n",
    "                                input_s_5, input_s_6, \n",
    "                                input_t_1, input_t_2, input_t_3, input_t_4,\n",
    "                                input_t_5, input_t_6, \n",
    "                                input_r], verbose = 0)\n",
    "\n",
    "        for i in range(pred.shape[0]):\n",
    "            score += float(pred[i])\n",
    "\n",
    "        #average the score\n",
    "        score = score/float(count)\n",
    "        \n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b602d8d",
   "metadata": {},
   "source": [
    "#### Not fine tuned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb84dd20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180 36\n",
      "180 23 30\n",
      "checkcorrect 60 60 real score 1.7556725203990937 Hits@1 1.0 Hits@3 1.0 Hits@10 1.0 MRR 1.0 cur_rank 0 abs_cur_rank 0 total_num 0 205\n",
      "180 117\n",
      "180 455 365\n",
      "checkcorrect 46 46 real score 1.8204255163669587 Hits@1 0.5 Hits@3 1.0 Hits@10 1.0 MRR 0.75 cur_rank 1 abs_cur_rank 1 total_num 1 205\n",
      "0 1\n",
      "180 96 19\n",
      "checkcorrect 46 46 real score 0.7859888553619385 Hits@1 0.6666666666666666 Hits@3 1.0 Hits@10 1.0 MRR 0.8333333333333334 cur_rank 0 abs_cur_rank 0 total_num 2 205\n",
      "180 150\n",
      "180 443 342\n",
      "checkcorrect 132 132 real score 1.7818278193473818 Hits@1 0.75 Hits@3 1.0 Hits@10 1.0 MRR 0.875 cur_rank 0 abs_cur_rank 2 total_num 3 205\n",
      "180 4\n",
      "180 22 26\n",
      "checkcorrect 26 26 real score 1.841943633556366 Hits@1 0.8 Hits@3 1.0 Hits@10 1.0 MRR 0.9 cur_rank 0 abs_cur_rank 1 total_num 4 205\n",
      "0 0\n",
      "0 1 6\n",
      "checkcorrect 346 346 real score 0.0 Hits@1 0.6666666666666666 Hits@3 0.8333333333333334 Hits@10 0.8333333333333334 MRR 0.7509578544061303 cur_rank 173 abs_cur_rank 173 total_num 5 205\n",
      "180 24\n",
      "180 6 46\n",
      "checkcorrect 56 56 real score 1.7103047370910645 Hits@1 0.7142857142857143 Hits@3 0.8571428571428571 Hits@10 0.8571428571428571 MRR 0.786535303776683 cur_rank 0 abs_cur_rank 0 total_num 6 205\n",
      "180 150\n",
      "180 234 336\n",
      "checkcorrect 236 236 real score 1.5665661603212357 Hits@1 0.75 Hits@3 0.875 Hits@10 0.875 MRR 0.8132183908045977 cur_rank 0 abs_cur_rank 0 total_num 7 205\n",
      "180 64\n",
      "180 169 175\n",
      "checkcorrect 18 18 real score 1.95781751871109 Hits@1 0.7777777777777778 Hits@3 0.8888888888888888 Hits@10 0.8888888888888888 MRR 0.8339719029374202 cur_rank 0 abs_cur_rank 0 total_num 8 205\n",
      "180 137\n",
      "180 171 79\n",
      "checkcorrect 88 88 real score 1.7561256110668182 Hits@1 0.8 Hits@3 0.9 Hits@10 0.9 MRR 0.850574712643678 cur_rank 0 abs_cur_rank 0 total_num 9 205\n",
      "180 150\n",
      "180 110 603\n",
      "checkcorrect 30 30 real score 1.9555778563022614 Hits@1 0.8181818181818182 Hits@3 0.9090909090909091 Hits@10 0.9090909090909091 MRR 0.8641588296760709 cur_rank 0 abs_cur_rank 0 total_num 10 205\n",
      "0 0\n",
      "0 54 1\n",
      "checkcorrect 92 92 real score 0.0 Hits@1 0.75 Hits@3 0.8333333333333334 Hits@10 0.8333333333333334 MRR 0.7939186435151218 cur_rank 46 abs_cur_rank 46 total_num 11 205\n",
      "0 1\n",
      "180 135 22\n",
      "checkcorrect 178 178 real score 0.8047817587852478 Hits@1 0.6923076923076923 Hits@3 0.8461538461538461 Hits@10 0.8461538461538461 MRR 0.7713095170908816 cur_rank 1 abs_cur_rank 1 total_num 12 205\n",
      "180 150\n",
      "180 11 136\n",
      "checkcorrect 8 8 real score 1.792520385980606 Hits@1 0.7142857142857143 Hits@3 0.8571428571428571 Hits@10 0.8571428571428571 MRR 0.7876445515843901 cur_rank 0 abs_cur_rank 0 total_num 13 205\n",
      "180 150\n",
      "180 284 603\n",
      "checkcorrect 82 82 real score 1.8482048928737642 Hits@1 0.7333333333333333 Hits@3 0.8666666666666667 Hits@10 0.8666666666666667 MRR 0.8018015814787641 cur_rank 0 abs_cur_rank 0 total_num 14 205\n",
      "180 49\n",
      "180 223 72\n",
      "checkcorrect 54 54 real score 1.9395142674446106 Hits@1 0.75 Hits@3 0.875 Hits@10 0.875 MRR 0.8141889826363413 cur_rank 0 abs_cur_rank 0 total_num 15 205\n",
      "180 145\n",
      "180 100 67\n",
      "checkcorrect 126 126 real score 1.7450302183628081 Hits@1 0.7647058823529411 Hits@3 0.8823529411764706 Hits@10 0.8823529411764706 MRR 0.8251190424812624 cur_rank 0 abs_cur_rank 0 total_num 16 205\n",
      "180 141\n",
      "180 41 107\n",
      "checkcorrect 146 146 real score 1.7912960588932036 Hits@1 0.7777777777777778 Hits@3 0.8888888888888888 Hits@10 0.8888888888888888 MRR 0.8348346512323034 cur_rank 0 abs_cur_rank 0 total_num 17 205\n",
      "180 20\n",
      "180 23 8\n",
      "checkcorrect 210 210 real score 1.900923752784729 Hits@1 0.7894736842105263 Hits@3 0.8947368421052632 Hits@10 0.8947368421052632 MRR 0.84352756432534 cur_rank 0 abs_cur_rank 0 total_num 18 205\n",
      "180 135\n",
      "180 22 38\n",
      "checkcorrect 46 46 real score 1.5692900002002714 Hits@1 0.8 Hits@3 0.9 Hits@10 0.9 MRR 0.851351186109073 cur_rank 0 abs_cur_rank 1 total_num 19 205\n",
      "180 36\n",
      "180 10 31\n",
      "checkcorrect 234 234 real score 0.835228779911995 Hits@1 0.7619047619047619 Hits@3 0.9047619047619048 Hits@10 0.9047619047619048 MRR 0.8346201772467362 cur_rank 1 abs_cur_rank 1 total_num 20 205\n",
      "0 0\n",
      "0 3 8\n",
      "checkcorrect 350 350 real score 0.0 Hits@1 0.7272727272727273 Hits@3 0.8636363636363636 Hits@10 0.8636363636363636 MRR 0.7969411609256035 cur_rank 175 abs_cur_rank 175 total_num 21 205\n",
      "0 1\n",
      "0 3 3\n",
      "checkcorrect 188 188 real score 0.0 Hits@1 0.6956521739130435 Hits@3 0.8260869565217391 Hits@10 0.8260869565217391 MRR 0.7627492111370762 cur_rank 94 abs_cur_rank 94 total_num 22 205\n",
      "180 63\n",
      "180 16 20\n",
      "checkcorrect 76 76 real score 1.6011101126670837 Hits@1 0.6666666666666666 Hits@3 0.8333333333333334 Hits@10 0.8333333333333334 MRR 0.751801327339698 cur_rank 1 abs_cur_rank 1 total_num 23 205\n",
      "180 150\n",
      "180 106 197\n",
      "checkcorrect 8 8 real score 1.3837519988417626 Hits@1 0.68 Hits@3 0.84 Hits@10 0.84 MRR 0.76172927424611 cur_rank 0 abs_cur_rank 0 total_num 24 205\n",
      "180 46\n",
      "180 72 68\n",
      "checkcorrect 72 72 real score 1.7069336414337157 Hits@1 0.6923076923076923 Hits@3 0.8461538461538461 Hits@10 0.8461538461538461 MRR 0.7708935329289519 cur_rank 0 abs_cur_rank 0 total_num 25 205\n",
      "180 150\n",
      "180 158 234\n",
      "checkcorrect 0 0 real score 1.7244643598794938 Hits@1 0.6666666666666666 Hits@3 0.8518518518518519 Hits@10 0.8518518518518519 MRR 0.7608604391167686 cur_rank 1 abs_cur_rank 1 total_num 26 205\n",
      "180 135\n",
      "180 171 82\n",
      "checkcorrect 88 88 real score 1.7427713036537171 Hits@1 0.6785714285714286 Hits@3 0.8571428571428571 Hits@10 0.8571428571428571 MRR 0.7694011377197411 cur_rank 0 abs_cur_rank 0 total_num 27 205\n",
      "180 150\n",
      "180 350 339\n",
      "checkcorrect 18 18 real score 1.9390928089618682 Hits@1 0.6896551724137931 Hits@3 0.8620689655172413 Hits@10 0.8620689655172413 MRR 0.777352822625957 cur_rank 0 abs_cur_rank 0 total_num 28 205\n",
      "180 87\n",
      "180 194 191\n",
      "checkcorrect 18 18 real score 1.887297475337982 Hits@1 0.7 Hits@3 0.8666666666666667 Hits@10 0.8666666666666667 MRR 0.7847743952050917 cur_rank 0 abs_cur_rank 0 total_num 29 205\n",
      "0 0\n",
      "0 215 1\n",
      "checkcorrect 116 116 real score 0.0 Hits@1 0.6774193548387096 Hits@3 0.8387096774193549 Hits@10 0.8387096774193549 MRR 0.7600058389901653 cur_rank 58 abs_cur_rank 58 total_num 30 205\n",
      "180 150\n",
      "180 223 106\n",
      "checkcorrect 10 10 real score 1.8454168379306792 Hits@1 0.6875 Hits@3 0.84375 Hits@10 0.84375 MRR 0.7675056565217226 cur_rank 0 abs_cur_rank 0 total_num 31 205\n",
      "180 99\n",
      "180 110 532\n",
      "checkcorrect 212 212 real score 1.7273969054222107 Hits@1 0.696969696969697 Hits@3 0.8484848484848485 Hits@10 0.8484848484848485 MRR 0.774550939657428 cur_rank 0 abs_cur_rank 1 total_num 32 205\n",
      "180 6\n",
      "180 21 8\n",
      "checkcorrect 212 212 real score 1.5836220324039458 Hits@1 0.7058823529411765 Hits@3 0.8529411764705882 Hits@10 0.8529411764705882 MRR 0.781181794373386 cur_rank 0 abs_cur_rank 0 total_num 33 205\n",
      "0 1\n",
      "180 96 541\n",
      "checkcorrect 212 212 real score 0.040985934296622874 Hits@1 0.6857142857142857 Hits@3 0.8285714285714286 Hits@10 0.8285714285714286 MRR 0.7592239781869493 cur_rank 78 abs_cur_rank 79 total_num 34 205\n",
      "180 127\n",
      "180 24 97\n",
      "checkcorrect 196 196 real score 1.7976891160011292 Hits@1 0.6944444444444444 Hits@3 0.8333333333333334 Hits@10 0.8333333333333334 MRR 0.7659122010150896 cur_rank 0 abs_cur_rank 0 total_num 35 205\n",
      "180 24\n",
      "180 8 37\n",
      "checkcorrect 346 346 real score -0.19781530126929284 Hits@1 0.6756756756756757 Hits@3 0.8108108108108109 Hits@10 0.8108108108108109 MRR 0.7454246824943583 cur_rank 126 abs_cur_rank 126 total_num 36 205\n",
      "180 141\n",
      "180 177 53\n",
      "checkcorrect 342 342 real score 0.8707269489765167 Hits@1 0.6842105263157895 Hits@3 0.8157894736842105 Hits@10 0.8157894736842105 MRR 0.752124032955033 cur_rank 0 abs_cur_rank 0 total_num 37 205\n",
      "180 50\n",
      "180 115 457\n",
      "checkcorrect 122 122 real score 1.581412786245346 Hits@1 0.6923076923076923 Hits@3 0.8205128205128205 Hits@10 0.8205128205128205 MRR 0.7584798269818271 cur_rank 0 abs_cur_rank 0 total_num 38 205\n",
      "180 79\n",
      "180 175 73\n",
      "checkcorrect 68 68 real score 1.816082936525345 Hits@1 0.7 Hits@3 0.825 Hits@10 0.825 MRR 0.7645178313072815 cur_rank 0 abs_cur_rank 0 total_num 39 205\n",
      "180 150\n",
      "180 78 276\n",
      "checkcorrect 262 262 real score 1.795573890209198 Hits@1 0.7073170731707317 Hits@3 0.8292682926829268 Hits@10 0.8292682926829268 MRR 0.7702612988363722 cur_rank 0 abs_cur_rank 0 total_num 40 205\n",
      "180 150\n",
      "180 70 375\n",
      "checkcorrect 2 2 real score 1.930234056711197 Hits@1 0.7142857142857143 Hits@3 0.8333333333333334 Hits@10 0.8333333333333334 MRR 0.7757312679116967 cur_rank 0 abs_cur_rank 0 total_num 41 205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180 150\n",
      "180 6 78\n",
      "checkcorrect 262 262 real score 1.707209324836731 Hits@1 0.7209302325581395 Hits@3 0.8372093023255814 Hits@10 0.8372093023255814 MRR 0.780946819820727 cur_rank 0 abs_cur_rank 0 total_num 42 205\n",
      "180 15\n",
      "180 18 8\n",
      "checkcorrect 210 210 real score 1.8984626889228822 Hits@1 0.7272727272727273 Hits@3 0.8409090909090909 Hits@10 0.8409090909090909 MRR 0.7859253011884377 cur_rank 0 abs_cur_rank 0 total_num 43 205\n",
      "180 150\n",
      "180 249 457\n",
      "checkcorrect 20 20 real score 1.7028020083904267 Hits@1 0.7333333333333333 Hits@3 0.8444444444444444 Hits@10 0.8444444444444444 MRR 0.7906825167175836 cur_rank 0 abs_cur_rank 0 total_num 44 205\n",
      "180 105\n",
      "180 58 218\n",
      "checkcorrect 58 58 real score 1.7642047822475433 Hits@1 0.717391304347826 Hits@3 0.8478260869565217 Hits@10 0.8478260869565217 MRR 0.7843633315715491 cur_rank 1 abs_cur_rank 1 total_num 45 205\n",
      "180 150\n",
      "180 171 544\n",
      "checkcorrect 74 74 real score 1.81217183470726 Hits@1 0.723404255319149 Hits@3 0.851063829787234 Hits@10 0.851063829787234 MRR 0.7889513457934311 cur_rank 0 abs_cur_rank 0 total_num 46 205\n",
      "0 2\n",
      "180 106 313\n",
      "checkcorrect 106 106 real score 0.8219710350036621 Hits@1 0.7083333333333334 Hits@3 0.8541666666666666 Hits@10 0.8541666666666666 MRR 0.7794593038671791 cur_rank 2 abs_cur_rank 2 total_num 47 205\n",
      "180 150\n",
      "180 49 143\n",
      "checkcorrect 56 56 real score 1.4188990890979767 Hits@1 0.7142857142857143 Hits@3 0.8571428571428571 Hits@10 0.8571428571428571 MRR 0.7839601344005019 cur_rank 0 abs_cur_rank 0 total_num 48 205\n",
      "180 73\n",
      "180 127 231\n",
      "checkcorrect 308 308 real score 0.7906963750720024 Hits@1 0.7 Hits@3 0.84 Hits@10 0.86 MRR 0.7705031539347142 cur_rank 8 abs_cur_rank 8 total_num 49 205\n",
      "180 26\n",
      "180 9 31\n",
      "checkcorrect 234 234 real score 1.6804523408412932 Hits@1 0.7058823529411765 Hits@3 0.8431372549019608 Hits@10 0.8627450980392157 MRR 0.7750030920928571 cur_rank 0 abs_cur_rank 0 total_num 50 205\n",
      "180 150\n",
      "180 126 95\n",
      "checkcorrect 204 204 real score 0.5690200589597225 Hits@1 0.6923076923076923 Hits@3 0.8269230769230769 Hits@10 0.8653846153846154 MRR 0.762235938612439 cur_rank 8 abs_cur_rank 8 total_num 51 205\n",
      "180 150\n",
      "180 181 375\n",
      "checkcorrect 2 2 real score 1.5225333854556085 Hits@1 0.6981132075471698 Hits@3 0.8301886792452831 Hits@10 0.8679245283018868 MRR 0.766722052978242 cur_rank 0 abs_cur_rank 0 total_num 52 205\n",
      "180 150\n",
      "180 103 143\n",
      "checkcorrect 56 56 real score 1.777564787864685 Hits@1 0.7037037037037037 Hits@3 0.8333333333333334 Hits@10 0.8703703703703703 MRR 0.7710420149601264 cur_rank 0 abs_cur_rank 0 total_num 53 205\n",
      "180 150\n",
      "180 152 214\n",
      "checkcorrect 10 10 real score 1.9496900975704192 Hits@1 0.7090909090909091 Hits@3 0.8363636363636363 Hits@10 0.8727272727272727 MRR 0.7752048874153968 cur_rank 0 abs_cur_rank 0 total_num 54 205\n",
      "180 150\n",
      "180 603 169\n",
      "checkcorrect 10 10 real score 1.7859350383281707 Hits@1 0.7142857142857143 Hits@3 0.8392857142857143 Hits@10 0.875 MRR 0.7792190858544076 cur_rank 0 abs_cur_rank 0 total_num 55 205\n",
      "180 147\n",
      "180 190 596\n",
      "checkcorrect 34 34 real score 1.989966332912445 Hits@1 0.7192982456140351 Hits@3 0.8421052631578947 Hits@10 0.8771929824561403 MRR 0.7830924352253829 cur_rank 0 abs_cur_rank 0 total_num 56 205\n",
      "180 38\n",
      "180 127 27\n",
      "checkcorrect 204 204 real score 1.0941351383924482 Hits@1 0.7241379310344828 Hits@3 0.8448275862068966 Hits@10 0.8793103448275862 MRR 0.7868322208249453 cur_rank 0 abs_cur_rank 0 total_num 57 205\n",
      "180 150\n",
      "180 193 179\n",
      "checkcorrect 56 56 real score 1.6969685196876525 Hits@1 0.7288135593220338 Hits@3 0.847457627118644 Hits@10 0.8813559322033898 MRR 0.7904452340313021 cur_rank 0 abs_cur_rank 0 total_num 58 205\n",
      "0 0\n",
      "180 8 6\n",
      "checkcorrect 156 156 real score 0.5678565502166748 Hits@1 0.7333333333333333 Hits@3 0.85 Hits@10 0.8833333333333333 MRR 0.7939378134641137 cur_rank 0 abs_cur_rank 0 total_num 59 205\n",
      "180 11\n",
      "180 38 182\n",
      "checkcorrect 54 54 real score 1.552535843849182 Hits@1 0.7377049180327869 Hits@3 0.8524590163934426 Hits@10 0.8852459016393442 MRR 0.7973158820958496 cur_rank 0 abs_cur_rank 0 total_num 60 205\n",
      "180 150\n",
      "180 44 553\n",
      "checkcorrect 20 20 real score 1.6875786066055296 Hits@1 0.7419354838709677 Hits@3 0.8548387096774194 Hits@10 0.8870967741935484 MRR 0.800584980771723 cur_rank 0 abs_cur_rank 0 total_num 61 205\n",
      "180 150\n",
      "180 93 197\n",
      "checkcorrect 8 8 real score 1.7981020689010618 Hits@1 0.746031746031746 Hits@3 0.8571428571428571 Hits@10 0.8888888888888888 MRR 0.8037502985372512 cur_rank 0 abs_cur_rank 0 total_num 62 205\n",
      "180 150\n",
      "180 207 179\n",
      "checkcorrect 88 88 real score 1.8513316094875336 Hits@1 0.75 Hits@3 0.859375 Hits@10 0.890625 MRR 0.8068167001226066 cur_rank 0 abs_cur_rank 0 total_num 63 205\n",
      "180 150\n",
      "180 71 212\n",
      "checkcorrect 94 94 real score 1.7169062316417696 Hits@1 0.7384615384615385 Hits@3 0.8461538461538461 Hits@10 0.8923076923076924 MRR 0.7982502893514897 cur_rank 3 abs_cur_rank 3 total_num 64 205\n",
      "0 1\n",
      "180 37 281\n",
      "checkcorrect 46 46 real score 0.893183434009552 Hits@1 0.7272727272727273 Hits@3 0.8484848484848485 Hits@10 0.8939393939393939 MRR 0.7937313455734367 cur_rank 1 abs_cur_rank 1 total_num 65 205\n",
      "180 15\n",
      "180 8 27\n",
      "checkcorrect 210 210 real score 1.9072923719882966 Hits@1 0.7313432835820896 Hits@3 0.8507462686567164 Hits@10 0.8955223880597015 MRR 0.796809982206669 cur_rank 0 abs_cur_rank 0 total_num 66 205\n",
      "180 13\n",
      "180 14 27\n",
      "checkcorrect 346 346 real score -0.1985407706350088 Hits@1 0.7205882352941176 Hits@3 0.8382352941176471 Hits@10 0.8823529411764706 MRR 0.7852053105226343 cur_rank 129 abs_cur_rank 129 total_num 67 205\n",
      "180 26\n",
      "180 65 10\n",
      "checkcorrect 234 234 real score 1.3395705014467238 Hits@1 0.7246376811594203 Hits@3 0.8405797101449275 Hits@10 0.8840579710144928 MRR 0.788318277036799 cur_rank 0 abs_cur_rank 0 total_num 68 205\n",
      "180 19\n",
      "180 14 37\n",
      "checkcorrect 346 346 real score -0.19811352603137494 Hits@1 0.7142857142857143 Hits@3 0.8285714285714286 Hits@10 0.8714285714285714 MRR 0.7771690733041023 cur_rank 126 abs_cur_rank 126 total_num 69 205\n",
      "180 150\n",
      "180 52 596\n",
      "checkcorrect 2 2 real score 1.9624000430107116 Hits@1 0.7183098591549296 Hits@3 0.8309859154929577 Hits@10 0.8732394366197183 MRR 0.7803075370603826 cur_rank 0 abs_cur_rank 0 total_num 70 205\n",
      "180 137\n",
      "180 152 52\n",
      "checkcorrect 342 342 real score 0.6956468813121319 Hits@1 0.7083333333333334 Hits@3 0.8194444444444444 Hits@10 0.875 MRR 0.7722477101567662 cur_rank 4 abs_cur_rank 4 total_num 71 205\n",
      "180 16\n",
      "180 128 10\n",
      "checkcorrect 292 292 real score 1.360778045654297 Hits@1 0.7123287671232876 Hits@3 0.821917808219178 Hits@10 0.8767123287671232 MRR 0.7753676045381803 cur_rank 0 abs_cur_rank 0 total_num 72 205\n",
      "180 150\n",
      "180 407 276\n",
      "checkcorrect 46 46 real score 1.8399946630001067 Hits@1 0.7162162162162162 Hits@3 0.8243243243243243 Hits@10 0.8783783783783784 MRR 0.7784031774498266 cur_rank 0 abs_cur_rank 0 total_num 73 205\n",
      "180 150\n",
      "180 603 342\n",
      "checkcorrect 10 10 real score 1.9099408090114594 Hits@1 0.72 Hits@3 0.8266666666666667 Hits@10 0.88 MRR 0.7813578017504955 cur_rank 0 abs_cur_rank 0 total_num 74 205\n",
      "180 23\n",
      "180 27 66\n",
      "checkcorrect 106 106 real score 1.8458122730255129 Hits@1 0.7236842105263158 Hits@3 0.8289473684210527 Hits@10 0.881578947368421 MRR 0.7842346727800943 cur_rank 0 abs_cur_rank 0 total_num 75 205\n",
      "180 36\n",
      "180 77 20\n",
      "checkcorrect 46 46 real score 1.879314786195755 Hits@1 0.7142857142857143 Hits@3 0.8311688311688312 Hits@10 0.8831168831168831 MRR 0.7805433133933398 cur_rank 1 abs_cur_rank 1 total_num 76 205\n",
      "180 137\n",
      "180 46 179\n",
      "checkcorrect 56 56 real score 1.2989107325673102 Hits@1 0.717948717948718 Hits@3 0.8333333333333334 Hits@10 0.8846153846153846 MRR 0.7833568606575277 cur_rank 0 abs_cur_rank 0 total_num 77 205\n",
      "180 110\n",
      "180 35 49\n",
      "checkcorrect 344 344 real score 0.15630121119320395 Hits@1 0.7088607594936709 Hits@3 0.8227848101265823 Hits@10 0.8734177215189873 MRR 0.7739472801428755 cur_rank 24 abs_cur_rank 25 total_num 78 205\n",
      "180 16\n",
      "180 24 7\n",
      "checkcorrect 188 188 real score 1.7285629153251647 Hits@1 0.7125 Hits@3 0.825 Hits@10 0.875 MRR 0.7767729391410896 cur_rank 0 abs_cur_rank 0 total_num 79 205\n",
      "180 150\n",
      "180 15 179\n",
      "checkcorrect 56 56 real score 1.5992527604103088 Hits@1 0.7160493827160493 Hits@3 0.8271604938271605 Hits@10 0.8765432098765432 MRR 0.779528828781323 cur_rank 0 abs_cur_rank 0 total_num 80 205\n",
      "180 150\n",
      "180 50 512\n",
      "checkcorrect 310 310 real score 0.7082564786076546 Hits@1 0.7073170731707317 Hits@3 0.8170731707317073 Hits@10 0.8780487804878049 MRR 0.7724614040400875 cur_rank 4 abs_cur_rank 4 total_num 81 205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "180 95 13\n",
      "checkcorrect 18 18 real score 0.6131547212600708 Hits@1 0.7108433734939759 Hits@3 0.8192771084337349 Hits@10 0.8795180722891566 MRR 0.7752028329070744 cur_rank 0 abs_cur_rank 0 total_num 82 205\n",
      "180 81\n",
      "180 143 417\n",
      "checkcorrect 46 46 real score 1.8886833250522614 Hits@1 0.7023809523809523 Hits@3 0.8214285714285714 Hits@10 0.8809523809523809 MRR 0.7719266087057997 cur_rank 1 abs_cur_rank 1 total_num 83 205\n",
      "0 1\n",
      "0 4 31\n",
      "checkcorrect 46 46 real score 0.0 Hits@1 0.6941176470588235 Hits@3 0.8117647058823529 Hits@10 0.8705882352941177 MRR 0.7633353152700452 cur_rank 23 abs_cur_rank 23 total_num 84 205\n",
      "180 150\n",
      "180 8 542\n",
      "checkcorrect 74 74 real score 1.4694504946470262 Hits@1 0.6976744186046512 Hits@3 0.813953488372093 Hits@10 0.872093023255814 MRR 0.7660872302087656 cur_rank 0 abs_cur_rank 0 total_num 85 205\n",
      "180 49\n",
      "180 132 40\n",
      "checkcorrect 110 110 real score 1.6720180094242096 Hits@1 0.7011494252873564 Hits@3 0.8160919540229885 Hits@10 0.8735632183908046 MRR 0.7687758827351017 cur_rank 0 abs_cur_rank 0 total_num 86 205\n",
      "0 1\n",
      "0 1 1\n",
      "checkcorrect 66 66 real score 0.0 Hits@1 0.6931818181818182 Hits@3 0.8068181818181818 Hits@10 0.8636363636363636 MRR 0.7603740177574969 cur_rank 33 abs_cur_rank 33 total_num 87 205\n",
      "180 14\n",
      "180 31 50\n",
      "checkcorrect 26 26 real score 1.7106142580509185 Hits@1 0.6966292134831461 Hits@3 0.8089887640449438 Hits@10 0.8651685393258427 MRR 0.7630664445242665 cur_rank 0 abs_cur_rank 1 total_num 88 205\n",
      "180 150\n",
      "180 119 201\n",
      "checkcorrect 212 212 real score 1.5583209156990052 Hits@1 0.6888888888888889 Hits@3 0.8111111111111111 Hits@10 0.8666666666666667 MRR 0.7601434840295525 cur_rank 1 abs_cur_rank 1 total_num 89 205\n",
      "180 93\n",
      "180 244 56\n",
      "checkcorrect 18 18 real score 1.952758365869522 Hits@1 0.6923076923076923 Hits@3 0.8131868131868132 Hits@10 0.8681318681318682 MRR 0.7627792699193376 cur_rank 0 abs_cur_rank 0 total_num 90 205\n",
      "180 150\n",
      "180 223 184\n",
      "checkcorrect 10 10 real score 1.9158519327640533 Hits@1 0.6956521739130435 Hits@3 0.8152173913043478 Hits@10 0.8695652173913043 MRR 0.7653577561158665 cur_rank 0 abs_cur_rank 0 total_num 91 205\n",
      "180 13\n",
      "180 18 15\n",
      "checkcorrect 188 188 real score 1.7039792358875276 Hits@1 0.6989247311827957 Hits@3 0.8172043010752689 Hits@10 0.8709677419354839 MRR 0.7678807909963411 cur_rank 0 abs_cur_rank 0 total_num 92 205\n",
      "0 0\n",
      "0 170 1\n",
      "checkcorrect 88 88 real score 0.0 Hits@1 0.6914893617021277 Hits@3 0.8085106382978723 Hits@10 0.8617021276595744 MRR 0.759948253030659 cur_rank 44 abs_cur_rank 44 total_num 93 205\n",
      "180 128\n",
      "180 29 38\n",
      "checkcorrect 126 126 real score 1.8526290893554687 Hits@1 0.6947368421052632 Hits@3 0.8105263157894737 Hits@10 0.8631578947368421 MRR 0.7624751135250731 cur_rank 0 abs_cur_rank 0 total_num 94 205\n",
      "180 63\n",
      "180 106 49\n",
      "checkcorrect 50 50 real score 1.8871073961257934 Hits@1 0.6979166666666666 Hits@3 0.8125 Hits@10 0.8645833333333334 MRR 0.7649493310925203 cur_rank 0 abs_cur_rank 0 total_num 95 205\n",
      "0 0\n",
      "0 1 237\n",
      "checkcorrect 312 312 real score 0.0 Hits@1 0.6907216494845361 Hits@3 0.8041237113402062 Hits@10 0.8556701030927835 MRR 0.7571289197075622 cur_rank 156 abs_cur_rank 156 total_num 96 205\n",
      "0 1\n",
      "0 9 5\n",
      "checkcorrect 224 224 real score 0.0 Hits@1 0.6836734693877551 Hits@3 0.7959183673469388 Hits@10 0.8469387755102041 MRR 0.7494942222761439 cur_rank 111 abs_cur_rank 112 total_num 97 205\n",
      "180 101\n",
      "180 28 104\n",
      "checkcorrect 188 188 real score 1.719599962234497 Hits@1 0.6868686868686869 Hits@3 0.797979797979798 Hits@10 0.8484848484848485 MRR 0.7520245836672941 cur_rank 0 abs_cur_rank 0 total_num 98 205\n",
      "180 150\n",
      "180 151 222\n",
      "checkcorrect 76 76 real score 1.1982320841401815 Hits@1 0.69 Hits@3 0.8 Hits@10 0.85 MRR 0.7545043378306211 cur_rank 0 abs_cur_rank 0 total_num 99 205\n",
      "180 150\n",
      "180 11 94\n",
      "checkcorrect 8 8 real score 1.777067881822586 Hits@1 0.693069306930693 Hits@3 0.801980198019802 Hits@10 0.8514851485148515 MRR 0.75693498795111 cur_rank 0 abs_cur_rank 0 total_num 100 205\n",
      "180 150\n",
      "180 205 197\n",
      "checkcorrect 8 8 real score 1.8776176989078524 Hits@1 0.696078431372549 Hits@3 0.803921568627451 Hits@10 0.8529411764705882 MRR 0.7593179782653148 cur_rank 0 abs_cur_rank 0 total_num 101 205\n",
      "180 49\n",
      "180 95 31\n",
      "checkcorrect 234 234 real score 1.063337478041649 Hits@1 0.6990291262135923 Hits@3 0.8058252427184466 Hits@10 0.8543689320388349 MRR 0.7616546969229331 cur_rank 0 abs_cur_rank 0 total_num 102 205\n",
      "180 150\n",
      "180 603 106\n",
      "checkcorrect 10 10 real score 1.8641872525215148 Hits@1 0.7019230769230769 Hits@3 0.8076923076923077 Hits@10 0.8557692307692307 MRR 0.7639464786832895 cur_rank 0 abs_cur_rank 0 total_num 103 205\n",
      "180 123\n",
      "180 206 93\n",
      "checkcorrect 88 88 real score 1.5663627892732621 Hits@1 0.7047619047619048 Hits@3 0.8095238095238095 Hits@10 0.8571428571428571 MRR 0.7661946074577344 cur_rank 0 abs_cur_rank 0 total_num 104 205\n",
      "0 0\n",
      "0 305 1\n",
      "checkcorrect 310 310 real score 0.0 Hits@1 0.6981132075471698 Hits@3 0.8018867924528302 Hits@10 0.8490566037735849 MRR 0.7590268305610601 cur_rank 155 abs_cur_rank 155 total_num 105 205\n",
      "180 50\n",
      "180 22 77\n",
      "checkcorrect 18 18 real score 1.7428618013858794 Hits@1 0.7009345794392523 Hits@3 0.8037383177570093 Hits@10 0.8504672897196262 MRR 0.7612789162567511 cur_rank 0 abs_cur_rank 0 total_num 106 205\n",
      "180 150\n",
      "180 171 78\n",
      "checkcorrect 6 6 real score 1.9234957098960876 Hits@1 0.7037037037037037 Hits@3 0.8055555555555556 Hits@10 0.8518518518518519 MRR 0.7634892966617812 cur_rank 0 abs_cur_rank 0 total_num 107 205\n",
      "180 150\n",
      "180 318 219\n",
      "checkcorrect 10 10 real score 1.8963235914707184 Hits@1 0.7064220183486238 Hits@3 0.8073394495412844 Hits@10 0.8532110091743119 MRR 0.7656591196281869 cur_rank 0 abs_cur_rank 0 total_num 108 205\n",
      "180 150\n",
      "180 47 153\n",
      "checkcorrect 30 30 real score 1.4557253062725066 Hits@1 0.7090909090909091 Hits@3 0.8090909090909091 Hits@10 0.8545454545454545 MRR 0.7677894912679306 cur_rank 0 abs_cur_rank 0 total_num 109 205\n",
      "180 150\n",
      "180 719 457\n",
      "checkcorrect 26 26 real score 1.6158901035785673 Hits@1 0.7117117117117117 Hits@3 0.8108108108108109 Hits@10 0.8558558558558559 MRR 0.7698814778330845 cur_rank 0 abs_cur_rank 1 total_num 110 205\n",
      "180 150\n",
      "180 207 99\n",
      "checkcorrect 88 88 real score 1.6586197793483735 Hits@1 0.7142857142857143 Hits@3 0.8125 Hits@10 0.8571428571428571 MRR 0.7719361074952891 cur_rank 0 abs_cur_rank 0 total_num 111 205\n",
      "180 11\n",
      "180 67 10\n",
      "checkcorrect 152 152 real score 1.6721101999282837 Hits@1 0.7168141592920354 Hits@3 0.8141592920353983 Hits@10 0.8584070796460177 MRR 0.7739543720307289 cur_rank 0 abs_cur_rank 0 total_num 112 205\n",
      "180 150\n",
      "180 225 433\n",
      "checkcorrect 2 2 real score 1.8642581045627593 Hits@1 0.7192982456140351 Hits@3 0.8157894736842105 Hits@10 0.8596491228070176 MRR 0.7759372284164243 cur_rank 0 abs_cur_rank 0 total_num 113 205\n",
      "180 148\n",
      "180 407 603\n",
      "checkcorrect 30 30 real score 1.7747075378894805 Hits@1 0.7217391304347827 Hits@3 0.8173913043478261 Hits@10 0.8608695652173913 MRR 0.7778856003432381 cur_rank 0 abs_cur_rank 0 total_num 114 205\n",
      "180 76\n",
      "180 57 50\n",
      "checkcorrect 50 50 real score 1.8909190833568572 Hits@1 0.7241379310344828 Hits@3 0.8189655172413793 Hits@10 0.8620689655172413 MRR 0.7798003796506239 cur_rank 0 abs_cur_rank 0 total_num 115 205\n",
      "180 150\n",
      "180 74 148\n",
      "checkcorrect 108 108 real score 1.9316344797611236 Hits@1 0.7264957264957265 Hits@3 0.8205128205128205 Hits@10 0.8632478632478633 MRR 0.781682427687798 cur_rank 0 abs_cur_rank 0 total_num 116 205\n",
      "180 150\n",
      "180 719 274\n",
      "checkcorrect 170 170 real score 0.8984091111458838 Hits@1 0.7203389830508474 Hits@3 0.8135593220338984 Hits@10 0.864406779661017 MRR 0.7767529155887489 cur_rank 4 abs_cur_rank 5 total_num 117 205\n",
      "0 1\n",
      "180 13 52\n",
      "checkcorrect 18 18 real score 0.9389957666397095 Hits@1 0.7226890756302521 Hits@3 0.8151260504201681 Hits@10 0.865546218487395 MRR 0.7786289415081712 cur_rank 0 abs_cur_rank 0 total_num 118 205\n",
      "180 142\n",
      "180 222 154\n",
      "checkcorrect 30 30 real score 1.807602697610855 Hits@1 0.725 Hits@3 0.8166666666666667 Hits@10 0.8666666666666667 MRR 0.7804737003289365 cur_rank 0 abs_cur_rank 0 total_num 119 205\n",
      "180 22\n",
      "180 23 18\n",
      "checkcorrect 210 210 real score 1.9074967324733734 Hits@1 0.7272727272727273 Hits@3 0.8181818181818182 Hits@10 0.8677685950413223 MRR 0.7822879672683667 cur_rank 0 abs_cur_rank 0 total_num 120 205\n",
      "180 120\n",
      "180 78 39\n",
      "checkcorrect 28 28 real score 1.8422954022884368 Hits@1 0.7295081967213115 Hits@3 0.819672131147541 Hits@10 0.8688524590163934 MRR 0.7840724921268227 cur_rank 0 abs_cur_rank 0 total_num 121 205\n",
      "0 1\n",
      "0 1 1\n",
      "checkcorrect 68 68 real score 0.0 Hits@1 0.7235772357723578 Hits@3 0.8130081300813008 Hits@10 0.8617886178861789 MRR 0.7779302070572667 cur_rank 34 abs_cur_rank 34 total_num 122 205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180 150\n",
      "180 136 142\n",
      "checkcorrect 292 292 real score 1.6443634569644927 Hits@1 0.7258064516129032 Hits@3 0.8145161290322581 Hits@10 0.8629032258064516 MRR 0.7797210924842242 cur_rank 0 abs_cur_rank 0 total_num 123 205\n",
      "180 58\n",
      "180 191 205\n",
      "checkcorrect 18 18 real score 1.8635748684406281 Hits@1 0.728 Hits@3 0.816 Hits@10 0.864 MRR 0.7814833237443504 cur_rank 0 abs_cur_rank 0 total_num 124 205\n",
      "180 125\n",
      "180 17 517\n",
      "checkcorrect 74 74 real score 0.4832372698932886 Hits@1 0.7222222222222222 Hits@3 0.8095238095238095 Hits@10 0.8571428571428571 MRR 0.7759424508045804 cur_rank 11 abs_cur_rank 11 total_num 125 205\n",
      "180 150\n",
      "180 171 457\n",
      "checkcorrect 152 152 real score 1.2348051220178604 Hits@1 0.7244094488188977 Hits@3 0.8110236220472441 Hits@10 0.8582677165354331 MRR 0.7777066834754105 cur_rank 0 abs_cur_rank 0 total_num 126 205\n",
      "180 148\n",
      "180 456 115\n",
      "checkcorrect 72 72 real score 1.3974479138851166 Hits@1 0.7265625 Hits@3 0.8125 Hits@10 0.859375 MRR 0.7794433500107588 cur_rank 0 abs_cur_rank 0 total_num 127 205\n",
      "180 150\n",
      "180 197 433\n",
      "checkcorrect 2 2 real score 1.4450410544872283 Hits@1 0.7209302325581395 Hits@3 0.813953488372093 Hits@10 0.8604651162790697 MRR 0.7772771224912955 cur_rank 1 abs_cur_rank 1 total_num 128 205\n",
      "0 1\n",
      "0 5 9\n",
      "checkcorrect 18 18 real score 0.0 Hits@1 0.7153846153846154 Hits@3 0.8076923076923077 Hits@10 0.8615384615384616 MRR 0.7720672984721317 cur_rank 9 abs_cur_rank 9 total_num 129 205\n",
      "180 145\n",
      "180 23 27\n",
      "checkcorrect 126 126 real score 1.8131962537765502 Hits@1 0.7175572519083969 Hits@3 0.8091603053435115 Hits@10 0.8625954198473282 MRR 0.773807242758604 cur_rank 0 abs_cur_rank 0 total_num 130 205\n",
      "180 105\n",
      "180 57 124\n",
      "checkcorrect 18 18 real score 1.4536853387951851 Hits@1 0.7196969696969697 Hits@3 0.8106060606060606 Hits@10 0.8636363636363636 MRR 0.775520824252857 cur_rank 0 abs_cur_rank 0 total_num 131 205\n",
      "180 94\n",
      "180 38 16\n",
      "checkcorrect 56 56 real score 1.7156394898891447 Hits@1 0.7218045112781954 Hits@3 0.8120300751879699 Hits@10 0.8646616541353384 MRR 0.7772086376043392 cur_rank 0 abs_cur_rank 0 total_num 132 205\n",
      "180 8\n",
      "180 11 22\n",
      "checkcorrect 196 196 real score 1.0617419213056565 Hits@1 0.7238805970149254 Hits@3 0.8134328358208955 Hits@10 0.8656716417910447 MRR 0.7788712597117696 cur_rank 0 abs_cur_rank 0 total_num 133 205\n",
      "180 33\n",
      "180 237 74\n",
      "checkcorrect 18 18 real score 1.4991815388202667 Hits@1 0.7185185185185186 Hits@3 0.8148148148148148 Hits@10 0.8666666666666667 MRR 0.7768055466768676 cur_rank 1 abs_cur_rank 1 total_num 134 205\n",
      "180 67\n",
      "180 100 30\n",
      "checkcorrect 126 126 real score 1.464237594604492 Hits@1 0.7205882352941176 Hits@3 0.8161764705882353 Hits@10 0.8676470588235294 MRR 0.7784466823630671 cur_rank 0 abs_cur_rank 0 total_num 135 205\n",
      "0 0\n",
      "0 3 10\n",
      "checkcorrect 32 32 real score 0.0 Hits@1 0.7153284671532847 Hits@3 0.8102189781021898 Hits@10 0.8613138686131386 MRR 0.7731939586188971 cur_rank 16 abs_cur_rank 16 total_num 136 205\n",
      "180 150\n",
      "180 155 135\n",
      "checkcorrect 6 6 real score 1.9521553814411163 Hits@1 0.717391304347826 Hits@3 0.8115942028985508 Hits@10 0.8623188405797102 MRR 0.7748374806578905 cur_rank 0 abs_cur_rank 0 total_num 137 205\n",
      "180 115\n",
      "180 41 268\n",
      "checkcorrect 46 46 real score 0.4841759461909532 Hits@1 0.7122302158273381 Hits@3 0.8057553956834532 Hits@10 0.8633093525179856 MRR 0.7701623908689849 cur_rank 7 abs_cur_rank 7 total_num 138 205\n",
      "180 150\n",
      "180 160 163\n",
      "checkcorrect 332 332 real score 0.15772813120856882 Hits@1 0.7071428571428572 Hits@3 0.8 Hits@10 0.8571428571428571 MRR 0.7648040880770635 cur_rank 49 abs_cur_rank 49 total_num 139 205\n",
      "180 150\n",
      "180 41 152\n",
      "checkcorrect 146 146 real score 1.7589811086654663 Hits@1 0.7092198581560284 Hits@3 0.8014184397163121 Hits@10 0.8581560283687943 MRR 0.7664721441899921 cur_rank 0 abs_cur_rank 0 total_num 140 205\n",
      "180 150\n",
      "180 120 363\n",
      "checkcorrect 50 50 real score 1.8588168263435363 Hits@1 0.7112676056338029 Hits@3 0.8028169014084507 Hits@10 0.8591549295774648 MRR 0.7681167065548513 cur_rank 0 abs_cur_rank 0 total_num 141 205\n",
      "180 39\n",
      "180 280 249\n",
      "checkcorrect 132 132 real score 1.8234639763832092 Hits@1 0.7062937062937062 Hits@3 0.8041958041958042 Hits@10 0.8601398601398601 MRR 0.7662417645509713 cur_rank 1 abs_cur_rank 2 total_num 142 205\n",
      "180 3\n",
      "180 29 10\n",
      "checkcorrect 212 212 real score 1.626000964641571 Hits@1 0.7083333333333334 Hits@3 0.8055555555555556 Hits@10 0.8611111111111112 MRR 0.7678650856304784 cur_rank 0 abs_cur_rank 1 total_num 143 205\n",
      "180 101\n",
      "180 205 230\n",
      "checkcorrect 132 132 real score 1.6919501304626465 Hits@1 0.7034482758620689 Hits@3 0.8068965517241379 Hits@10 0.8620689655172413 MRR 0.7648683149249808 cur_rank 2 abs_cur_rank 2 total_num 144 205\n",
      "0 1\n",
      "0 1 1\n",
      "checkcorrect 74 74 real score 0.0 Hits@1 0.6986301369863014 Hits@3 0.8013698630136986 Hits@10 0.8561643835616438 MRR 0.7598097359835335 cur_rank 37 abs_cur_rank 37 total_num 145 205\n",
      "180 23\n",
      "180 6 13\n",
      "checkcorrect 76 76 real score 1.4639433085918427 Hits@1 0.6938775510204082 Hits@3 0.8027210884353742 Hits@10 0.8571428571428571 MRR 0.7580423228135775 cur_rank 1 abs_cur_rank 1 total_num 146 205\n",
      "180 150\n",
      "180 603 78\n",
      "checkcorrect 6 6 real score 1.9461721658706663 Hits@1 0.6959459459459459 Hits@3 0.8040540540540541 Hits@10 0.8581081081081081 MRR 0.7596771719837561 cur_rank 0 abs_cur_rank 0 total_num 147 205\n",
      "0 1\n",
      "0 1 1\n",
      "checkcorrect 68 68 real score 0.0 Hits@1 0.6912751677852349 Hits@3 0.7986577181208053 Hits@10 0.8523489932885906 MRR 0.7547704220279686 cur_rank 34 abs_cur_rank 34 total_num 148 205\n",
      "180 107\n",
      "180 77 107\n",
      "checkcorrect 26 26 real score 1.812812399864197 Hits@1 0.6933333333333334 Hits@3 0.8 Hits@10 0.8533333333333334 MRR 0.7564052858811156 cur_rank 0 abs_cur_rank 1 total_num 149 205\n",
      "180 79\n",
      "180 24 46\n",
      "checkcorrect 56 56 real score 1.5943531036376952 Hits@1 0.695364238410596 Hits@3 0.8013245033112583 Hits@10 0.8543046357615894 MRR 0.7580184959083929 cur_rank 0 abs_cur_rank 0 total_num 150 205\n",
      "180 150\n",
      "180 457 603\n",
      "checkcorrect 30 30 real score 1.7832390785217287 Hits@1 0.6973684210526315 Hits@3 0.8026315789473685 Hits@10 0.8552631578947368 MRR 0.7596104794879429 cur_rank 0 abs_cur_rank 0 total_num 151 205\n",
      "180 6\n",
      "180 47 24\n",
      "checkcorrect 18 18 real score 1.7806640982627868 Hits@1 0.6993464052287581 Hits@3 0.803921568627451 Hits@10 0.8562091503267973 MRR 0.761181652824623 cur_rank 0 abs_cur_rank 0 total_num 152 205\n",
      "180 10\n",
      "180 72 6\n",
      "checkcorrect 346 346 real score -0.2050228014588356 Hits@1 0.6948051948051948 Hits@3 0.7987012987012987 Hits@10 0.8506493506493507 MRR 0.7562888648692184 cur_rank 129 abs_cur_rank 129 total_num 153 205\n",
      "0 1\n",
      "180 147 29\n",
      "checkcorrect 110 110 real score 0.8726573169231415 Hits@1 0.6967741935483871 Hits@3 0.8 Hits@10 0.8516129032258064 MRR 0.757861194773288 cur_rank 0 abs_cur_rank 0 total_num 154 205\n",
      "180 150\n",
      "180 553 188\n",
      "checkcorrect 160 160 real score 1.6135084480047226 Hits@1 0.6987179487179487 Hits@3 0.8012820512820513 Hits@10 0.8525641025641025 MRR 0.7594133666016644 cur_rank 0 abs_cur_rank 0 total_num 155 205\n",
      "180 122\n",
      "180 79 84\n",
      "checkcorrect 246 246 real score 0.5974539465270937 Hits@1 0.6942675159235668 Hits@3 0.7961783439490446 Hits@10 0.8535031847133758 MRR 0.7554862568962852 cur_rank 6 abs_cur_rank 6 total_num 156 205\n",
      "180 150\n",
      "180 47 147\n",
      "checkcorrect 184 184 real score 1.5557315349578857 Hits@1 0.689873417721519 Hits@3 0.7911392405063291 Hits@10 0.8544303797468354 MRR 0.7522869767893466 cur_rank 3 abs_cur_rank 3 total_num 157 205\n",
      "180 150\n",
      "180 274 552\n",
      "checkcorrect 20 20 real score 1.6945919275283812 Hits@1 0.6918238993710691 Hits@3 0.7924528301886793 Hits@10 0.8553459119496856 MRR 0.753844920331552 cur_rank 0 abs_cur_rank 0 total_num 158 205\n",
      "180 4\n",
      "180 114 13\n",
      "checkcorrect 18 18 real score 1.5809481978416442 Hits@1 0.69375 Hits@3 0.79375 Hits@10 0.85625 MRR 0.7553833895794798 cur_rank 0 abs_cur_rank 0 total_num 159 205\n",
      "180 9\n",
      "0 5 46\n",
      "checkcorrect 56 56 real score 0.9829179584980011 Hits@1 0.6894409937888198 Hits@3 0.7950310559006211 Hits@10 0.8571428571428571 MRR 0.7537971573460669 cur_rank 1 abs_cur_rank 1 total_num 160 205\n",
      "180 59\n",
      "180 110 33\n",
      "checkcorrect 242 242 real score 1.323585432767868 Hits@1 0.6851851851851852 Hits@3 0.7901234567901234 Hits@10 0.8580246913580247 MRR 0.7501728950579225 cur_rank 5 abs_cur_rank 5 total_num 161 205\n",
      "180 81\n",
      "180 237 47\n",
      "checkcorrect 92 92 real score 0.5404373522847891 Hits@1 0.6809815950920245 Hits@3 0.7852760736196319 Hits@10 0.8588957055214724 MRR 0.7463374785238248 cur_rank 7 abs_cur_rank 7 total_num 162 205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180 150\n",
      "180 93 49\n",
      "checkcorrect 262 262 real score 1.6200058162212372 Hits@1 0.6829268292682927 Hits@3 0.7865853658536586 Hits@10 0.8597560975609756 MRR 0.7478842012157527 cur_rank 0 abs_cur_rank 0 total_num 163 205\n",
      "180 23\n",
      "180 7 27\n",
      "checkcorrect 346 346 real score -0.21699929460883138 Hits@1 0.6787878787878788 Hits@3 0.7818181818181819 Hits@10 0.8545454545454545 MRR 0.7433978338616346 cur_rank 130 abs_cur_rank 130 total_num 164 205\n",
      "180 150\n",
      "180 6 184\n",
      "checkcorrect 272 272 real score 1.63909153342247 Hits@1 0.6746987951807228 Hits@3 0.7831325301204819 Hits@10 0.8554216867469879 MRR 0.7419315818504199 cur_rank 1 abs_cur_rank 1 total_num 165 205\n",
      "180 140\n",
      "180 101 146\n",
      "checkcorrect 76 76 real score 0.9253508351743222 Hits@1 0.6706586826347305 Hits@3 0.7844311377245509 Hits@10 0.8562874251497006 MRR 0.7404828897435312 cur_rank 1 abs_cur_rank 1 total_num 166 205\n",
      "180 12\n",
      "180 70 20\n",
      "checkcorrect 6 6 real score 1.951624971628189 Hits@1 0.6726190476190477 Hits@3 0.7857142857142857 Hits@10 0.8571428571428571 MRR 0.7420276344474387 cur_rank 0 abs_cur_rank 0 total_num 167 205\n",
      "180 150\n",
      "180 286 197\n",
      "checkcorrect 8 8 real score 1.9062981724739074 Hits@1 0.6745562130177515 Hits@3 0.7869822485207101 Hits@10 0.8579881656804734 MRR 0.7435540981489331 cur_rank 0 abs_cur_rank 0 total_num 168 205\n",
      "180 80\n",
      "180 20 336\n",
      "checkcorrect 130 130 real score 1.8845465064048765 Hits@1 0.6764705882352942 Hits@3 0.788235294117647 Hits@10 0.8588235294117647 MRR 0.7450626034539395 cur_rank 0 abs_cur_rank 0 total_num 169 205\n",
      "0 1\n",
      "0 19 4\n",
      "checkcorrect 144 144 real score 0.0 Hits@1 0.672514619883041 Hits@3 0.783625730994152 Hits@10 0.8538011695906432 MRR 0.7407856211538403 cur_rank 72 abs_cur_rank 72 total_num 170 205\n",
      "180 150\n",
      "180 244 206\n",
      "checkcorrect 18 18 real score 1.8003220081329345 Hits@1 0.6744186046511628 Hits@3 0.7848837209302325 Hits@10 0.8546511627906976 MRR 0.7422926814959692 cur_rank 0 abs_cur_rank 0 total_num 171 205\n",
      "180 150\n",
      "180 138 115\n",
      "checkcorrect 34 34 real score 1.8323522090911866 Hits@1 0.6763005780346821 Hits@3 0.7861271676300579 Hits@10 0.8554913294797688 MRR 0.7437823191751831 cur_rank 0 abs_cur_rank 0 total_num 172 205\n",
      "180 150\n",
      "180 718 80\n",
      "checkcorrect 116 116 real score 0.06567405015230177 Hits@1 0.6724137931034483 Hits@3 0.7816091954022989 Hits@10 0.8505747126436781 MRR 0.7396413622486216 cur_rank 42 abs_cur_rank 42 total_num 173 205\n",
      "180 97\n",
      "180 22 38\n",
      "checkcorrect 50 50 real score 1.568380284309387 Hits@1 0.6742857142857143 Hits@3 0.7828571428571428 Hits@10 0.8514285714285714 MRR 0.7411291258929152 cur_rank 0 abs_cur_rank 0 total_num 174 205\n",
      "180 117\n",
      "180 52 56\n",
      "checkcorrect 26 26 real score 1.8768022298812865 Hits@1 0.6761363636363636 Hits@3 0.7840909090909091 Hits@10 0.8522727272727273 MRR 0.74259998313216 cur_rank 0 abs_cur_rank 1 total_num 175 205\n",
      "180 150\n",
      "180 309 339\n",
      "checkcorrect 18 18 real score 1.9185052514076233 Hits@1 0.6779661016949152 Hits@3 0.7853107344632768 Hits@10 0.8531073446327684 MRR 0.7440542205155941 cur_rank 0 abs_cur_rank 0 total_num 176 205\n",
      "180 52\n",
      "180 9 20\n",
      "checkcorrect 126 126 real score 1.5784151792526244 Hits@1 0.6797752808988764 Hits@3 0.7865168539325843 Hits@10 0.8539325842696629 MRR 0.745492118153147 cur_rank 0 abs_cur_rank 0 total_num 177 205\n",
      "180 150\n",
      "180 105 200\n",
      "checkcorrect 212 212 real score 1.6775995910167694 Hits@1 0.6815642458100558 Hits@3 0.7877094972067039 Hits@10 0.8547486033519553 MRR 0.7469139498953081 cur_rank 0 abs_cur_rank 1 total_num 178 205\n",
      "180 143\n",
      "180 11 80\n",
      "checkcorrect 216 216 real score 0.004244791343808174 Hits@1 0.6777777777777778 Hits@3 0.7833333333333333 Hits@10 0.85 MRR 0.7428449432493519 cur_rank 68 abs_cur_rank 68 total_num 179 205\n",
      "180 150\n",
      "180 95 139\n",
      "checkcorrect 46 46 real score 0.46222318783402444 Hits@1 0.6740331491712708 Hits@3 0.7790055248618785 Hits@10 0.8453038674033149 MRR 0.7392430877115604 cur_rank 10 abs_cur_rank 10 total_num 180 205\n",
      "180 136\n",
      "180 147 157\n",
      "checkcorrect 74 74 real score 1.7304003953933715 Hits@1 0.6758241758241759 Hits@3 0.7802197802197802 Hits@10 0.8461538461538461 MRR 0.7406758179988595 cur_rank 0 abs_cur_rank 0 total_num 181 205\n",
      "180 91\n",
      "180 16 71\n",
      "checkcorrect 126 126 real score 1.5196348667144775 Hits@1 0.6775956284153005 Hits@3 0.7814207650273224 Hits@10 0.8469945355191257 MRR 0.7420928900316527 cur_rank 0 abs_cur_rank 0 total_num 182 205\n",
      "180 150\n",
      "0 3 553\n",
      "checkcorrect 20 20 real score 0.9845175325870514 Hits@1 0.6739130434782609 Hits@3 0.782608695652174 Hits@10 0.8478260869565217 MRR 0.7407771678032198 cur_rank 1 abs_cur_rank 1 total_num 183 205\n",
      "180 110\n",
      "180 243 170\n",
      "checkcorrect 170 170 real score 0.8816208452451973 Hits@1 0.6756756756756757 Hits@3 0.7837837837837838 Hits@10 0.8486486486486486 MRR 0.7421783723015808 cur_rank 0 abs_cur_rank 0 total_num 184 205\n",
      "180 56\n",
      "180 175 169\n",
      "checkcorrect 26 26 real score 1.9390715241432188 Hits@1 0.6720430107526881 Hits@3 0.7849462365591398 Hits@10 0.8494623655913979 MRR 0.7408763380418948 cur_rank 1 abs_cur_rank 1 total_num 185 205\n",
      "180 150\n",
      "180 94 457\n",
      "checkcorrect 176 176 real score 0.48834711331874137 Hits@1 0.6684491978609626 Hits@3 0.7807486631016043 Hits@10 0.8449197860962567 MRR 0.737296403461075 cur_rank 13 abs_cur_rank 13 total_num 186 205\n",
      "180 147\n",
      "180 40 99\n",
      "checkcorrect 50 50 real score 1.828198629617691 Hits@1 0.6702127659574468 Hits@3 0.7819148936170213 Hits@10 0.8457446808510638 MRR 0.7386937630171331 cur_rank 0 abs_cur_rank 0 total_num 187 205\n",
      "180 76\n",
      "180 174 400\n",
      "checkcorrect 26 26 real score 1.8941561758518217 Hits@1 0.6666666666666666 Hits@3 0.783068783068783 Hits@10 0.8465608465608465 MRR 0.7374308330540795 cur_rank 1 abs_cur_rank 1 total_num 188 205\n",
      "180 150\n",
      "180 249 103\n",
      "checkcorrect 310 310 real score 0.9251417770981789 Hits@1 0.6631578947368421 Hits@3 0.7789473684210526 Hits@10 0.8473684210526315 MRR 0.7348654076169527 cur_rank 3 abs_cur_rank 3 total_num 189 205\n",
      "180 150\n",
      "180 128 261\n",
      "checkcorrect 94 94 real score 1.385628180205822 Hits@1 0.6649214659685864 Hits@3 0.7801047120418848 Hits@10 0.8481675392670157 MRR 0.7362535468440892 cur_rank 0 abs_cur_rank 0 total_num 190 205\n",
      "180 114\n",
      "0 5 197\n",
      "checkcorrect 8 8 real score 0.9951452136039733 Hits@1 0.6666666666666666 Hits@3 0.78125 Hits@10 0.8489583333333334 MRR 0.7376272262876095 cur_rank 0 abs_cur_rank 0 total_num 191 205\n",
      "180 149\n",
      "180 14 94\n",
      "checkcorrect 8 8 real score 1.8483511209487915 Hits@1 0.6683937823834197 Hits@3 0.7823834196891192 Hits@10 0.8497409326424871 MRR 0.7389866707109898 cur_rank 0 abs_cur_rank 0 total_num 192 205\n",
      "180 106\n",
      "180 13 114\n",
      "checkcorrect 18 18 real score 1.6404041588306426 Hits@1 0.6701030927835051 Hits@3 0.7835051546391752 Hits@10 0.8505154639175257 MRR 0.7403321002434073 cur_rank 0 abs_cur_rank 0 total_num 193 205\n",
      "180 150\n",
      "180 242 126\n",
      "checkcorrect 170 170 real score 1.4986769616603852 Hits@1 0.6717948717948717 Hits@3 0.7846153846153846 Hits@10 0.8512820512820513 MRR 0.7416637304985694 cur_rank 0 abs_cur_rank 0 total_num 194 205\n",
      "180 29\n",
      "180 149 169\n",
      "checkcorrect 18 18 real score 1.8116413712501527 Hits@1 0.673469387755102 Hits@3 0.7857142857142857 Hits@10 0.8520408163265306 MRR 0.7429817726899032 cur_rank 0 abs_cur_rank 0 total_num 195 205\n",
      "180 150\n",
      "180 79 268\n",
      "checkcorrect 46 46 real score 1.8629408299922943 Hits@1 0.6751269035532995 Hits@3 0.7868020304568528 Hits@10 0.8527918781725888 MRR 0.7442864337422387 cur_rank 0 abs_cur_rank 1 total_num 196 205\n",
      "180 150\n",
      "180 73 103\n",
      "checkcorrect 310 310 real score 0.8951661467552186 Hits@1 0.6717171717171717 Hits@3 0.7828282828282829 Hits@10 0.8535353535353535 MRR 0.7417900376122274 cur_rank 3 abs_cur_rank 3 total_num 197 205\n",
      "180 150\n",
      "180 402 312\n",
      "checkcorrect 18 18 real score 1.9400626838207244 Hits@1 0.6733668341708543 Hits@3 0.7839195979899497 Hits@10 0.8542713567839196 MRR 0.7430875751116635 cur_rank 0 abs_cur_rank 0 total_num 198 205\n",
      "180 8\n",
      "180 17 11\n",
      "checkcorrect 42 42 real score 1.97503901720047 Hits@1 0.675 Hits@3 0.785 Hits@10 0.855 MRR 0.7443721372361051 cur_rank 0 abs_cur_rank 0 total_num 199 205\n",
      "180 150\n",
      "180 159 268\n",
      "checkcorrect 50 50 real score 1.9132766485214234 Hits@1 0.6766169154228856 Hits@3 0.7860696517412935 Hits@10 0.8557213930348259 MRR 0.7456439176478658 cur_rank 0 abs_cur_rank 0 total_num 200 205\n",
      "180 7\n",
      "180 69 72\n",
      "checkcorrect 18 18 real score 1.740759253501892 Hits@1 0.6782178217821783 Hits@3 0.7871287128712872 Hits@10 0.8564356435643564 MRR 0.7469031061743615 cur_rank 0 abs_cur_rank 0 total_num 201 205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180 33\n",
      "180 6 59\n",
      "checkcorrect 76 76 real score 1.4994848608970641 Hits@1 0.6748768472906403 Hits@3 0.7881773399014779 Hits@10 0.8571428571428571 MRR 0.7456868347153744 cur_rank 1 abs_cur_rank 1 total_num 202 205\n",
      "180 105\n",
      "180 22 595\n",
      "checkcorrect 84 84 real score 1.7412630081176759 Hits@1 0.6764705882352942 Hits@3 0.7892156862745098 Hits@10 0.8578431372549019 MRR 0.7469334678785344 cur_rank 0 abs_cur_rank 0 total_num 203 205\n",
      "180 53\n",
      "180 49 28\n",
      "checkcorrect 50 50 real score 1.8501790463924408 Hits@1 0.6780487804878049 Hits@3 0.7902439024390244 Hits@10 0.8585365853658536 MRR 0.7481679387669318 cur_rank 0 abs_cur_rank 0 total_num 204 205\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "#obtain the Hits@N for relation prediction##############\n",
    "\n",
    "#we select all the triples in the inductive test set\n",
    "selected = list(data_ind_test)\n",
    "\n",
    "###Hit at 1#############################\n",
    "#generate the negative samples by randomly replace relation with all the other relaiton\n",
    "Hits_at_1 = 0\n",
    "Hits_at_3 = 0\n",
    "Hits_at_10 = 0\n",
    "MRR_raw = 0.\n",
    "\n",
    "for i in range(len(selected)):\n",
    "    \n",
    "    s_true, r_true, t_true = selected[i][0], selected[i][1], selected[i][2]\n",
    "    \n",
    "    #run the path-based scoring\n",
    "    score_dict_path = path_based_relation_scoring(s_true, t_true, lower_bound, upper_bound_path, one_hop_ind, id2relation, model)\n",
    "    \n",
    "    #run the one-hop neighbour based scoring\n",
    "    score_dict_subg = subgraph_relation_scoring(s_true, t_true, lower_bound, upper_bound_subg, one_hop_ind, id2relation, model_2)\n",
    "    \n",
    "    #final score dict\n",
    "    score_dict = defaultdict(float)\n",
    "    \n",
    "    for r in score_dict_path:\n",
    "        score_dict[r] += score_dict_path[r]\n",
    "    for r in score_dict_subg:\n",
    "        score_dict[r] += score_dict_subg[r]\n",
    "    \n",
    "    #[... [score, r], ...]\n",
    "    temp_list = list()\n",
    "    \n",
    "    for r in id2relation:\n",
    "        \n",
    "        #again, we only care about initial relation prediciton\n",
    "        if r % 2 == 0:\n",
    "        \n",
    "            if r in score_dict:\n",
    "\n",
    "                temp_list.append([score_dict[r], r])\n",
    "\n",
    "            else:\n",
    "\n",
    "                temp_list.append([0.0, r])\n",
    "        \n",
    "    sorted_list = sorted(temp_list, key = lambda x: x[0], reverse=True)\n",
    "    \n",
    "    p = 0\n",
    "    exist_tri = 0\n",
    "    \n",
    "    while p < len(sorted_list) and sorted_list[p][1] != r_true:\n",
    "        \n",
    "        #moreover, we want to remove existing triples\n",
    "        if ((s_true, sorted_list[p][1], t_true) in data_test) or (\n",
    "            (s_true, sorted_list[p][1], t_true) in data_valid) or (\n",
    "            (s_true, sorted_list[p][1], t_true) in data) or (\n",
    "            (s_true, sorted_list[p][1], t_true) in data_ind) or (\n",
    "            (s_true, sorted_list[p][1], t_true) in data_ind_valid) or (\n",
    "            (s_true, sorted_list[p][1], t_true) in data_ind_test):\n",
    "            \n",
    "            exist_tri += 1\n",
    "            \n",
    "        p += 1\n",
    "    \n",
    "    if p - exist_tri == 0:\n",
    "        \n",
    "        Hits_at_1 += 1\n",
    "        \n",
    "    if p - exist_tri < 3:\n",
    "        \n",
    "        Hits_at_3 += 1\n",
    "        \n",
    "    if p - exist_tri < 10:\n",
    "        \n",
    "        Hits_at_10 += 1\n",
    "        \n",
    "    MRR_raw += 1./float(p - exist_tri + 1.) \n",
    "        \n",
    "    print('checkcorrect', r_true, sorted_list[p][1],\n",
    "          'real score', sorted_list[p][0],\n",
    "          'Hits@1', Hits_at_1/(i+1),\n",
    "          'Hits@3', Hits_at_3/(i+1),\n",
    "          'Hits@10', Hits_at_10/(i+1),\n",
    "          'MRR', MRR_raw/(i+1),\n",
    "          'cur_rank', p - exist_tri,\n",
    "          'abs_cur_rank', p,\n",
    "          'total_num', i, len(selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1f0b73c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating scores 20 410\n",
      "AUC-PR is: 0.8741428052702565\n",
      "evaluating scores 40 410\n",
      "AUC-PR is: 0.7842364843883061\n",
      "evaluating scores 60 410\n",
      "AUC-PR is: 0.7587296708300627\n",
      "evaluating scores 80 410\n",
      "AUC-PR is: 0.7818360411266243\n",
      "evaluating scores 100 410\n",
      "AUC-PR is: 0.8008851763523269\n",
      "evaluating scores 120 410\n",
      "AUC-PR is: 0.7218728597448315\n",
      "evaluating scores 140 410\n",
      "AUC-PR is: 0.7176704022692404\n",
      "evaluating scores 160 410\n",
      "AUC-PR is: 0.7311827270511708\n",
      "evaluating scores 180 410\n",
      "AUC-PR is: 0.7309562986147167\n",
      "evaluating scores 200 410\n",
      "AUC-PR is: 0.7544420829034897\n",
      "evaluating scores 220 410\n",
      "AUC-PR is: 0.7620870907024476\n",
      "evaluating scores 240 410\n",
      "AUC-PR is: 0.7611694006422548\n",
      "evaluating scores 260 410\n",
      "AUC-PR is: 0.7728997824632657\n",
      "evaluating scores 280 410\n",
      "AUC-PR is: 0.7545686275709245\n",
      "evaluating scores 300 410\n",
      "AUC-PR is: 0.7628725171467031\n",
      "evaluating scores 320 410\n",
      "AUC-PR is: 0.778922387212048\n",
      "evaluating scores 340 410\n",
      "AUC-PR is: 0.78901945556023\n",
      "evaluating scores 360 410\n",
      "AUC-PR is: 0.7962318405151569\n",
      "evaluating scores 380 410\n",
      "AUC-PR is: 0.8024737768665975\n",
      "evaluating scores 400 410\n",
      "AUC-PR is: 0.7810686525015474\n",
      "AUC-PR is: 0.7867026856477526\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "##obtain the AUC-PR for the test triples###\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.metrics import auc, plot_precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#we select all the triples in the inductive test set\n",
    "pos_triples = list(data_ind_test)\n",
    "\n",
    "#we build the negative samples by randomly replace head or tail entity in the triple.\n",
    "neg_triples = list()\n",
    "\n",
    "for i in range(len(pos_triples)):\n",
    "    \n",
    "    s_pos, r_pos, t_pos = pos_triples[i][0], pos_triples[i][1], pos_triples[i][2]\n",
    "    \n",
    "    #decide to replace the head or tail entity\n",
    "    number_0 = random.uniform(0, 1)\n",
    "    \n",
    "    if number_0 < 0.5: #replace head entity\n",
    "        s_neg = random.choice(list(new_ent_set))\n",
    "        \n",
    "        #filter out the existing triples\n",
    "        while ((s_neg, r_pos, t_pos) in data_test) or (\n",
    "               (s_neg, r_pos, t_pos) in data_valid) or (\n",
    "               (s_neg, r_pos, t_pos) in data) or (\n",
    "               (s_neg, r_pos, t_pos) in data_ind) or (\n",
    "               (s_neg, r_pos, t_pos) in data_ind_valid) or (\n",
    "               (s_neg, r_pos, t_pos) in data_ind_test):\n",
    "            \n",
    "            s_neg = random.choice(list(new_ent_set))\n",
    "        \n",
    "        neg_triples.append((s_neg, r_pos, t_pos))\n",
    "    \n",
    "    else: #replace tail entity\n",
    "        t_neg = random.choice(list(new_ent_set))\n",
    "        \n",
    "        #filter out the existing triples\n",
    "        while ((s_pos, r_pos, t_neg) in data_test) or (\n",
    "               (s_pos, r_pos, t_neg) in data_valid) or (\n",
    "               (s_pos, r_pos, t_neg) in data) or (\n",
    "               (s_pos, r_pos, t_neg) in data_ind) or (\n",
    "               (s_pos, r_pos, t_neg) in data_ind_valid) or (\n",
    "               (s_pos, r_pos, t_neg) in data_ind_test):\n",
    "            \n",
    "            t_neg = random.choice(list(new_ent_set))\n",
    "        \n",
    "        neg_triples.append((s_pos, r_pos, t_neg))\n",
    "\n",
    "if len(pos_triples) != len(neg_triples):\n",
    "    raise ValueError('error when generating negative triples')\n",
    "        \n",
    "#combine all triples\n",
    "all_triples = pos_triples + neg_triples\n",
    "\n",
    "#obtain the label array\n",
    "arr1 = np.ones((len(pos_triples),))\n",
    "arr2 = np.zeros((len(neg_triples),))\n",
    "y_test = np.concatenate((arr1, arr2))\n",
    "\n",
    "#shuffle positive and negative triples (optional)\n",
    "all_triples, y_test = shuffle(all_triples, y_test)\n",
    "\n",
    "#obtain the score aray\n",
    "y_score = np.zeros((len(y_test),))\n",
    "\n",
    "#implement the scoring\n",
    "for i in range(len(all_triples)):\n",
    "    \n",
    "    s, r, t = all_triples[i][0], all_triples[i][1], all_triples[i][2]\n",
    "    \n",
    "    path_score = path_based_triple_scoring(s, r, t, lower_bound, upper_bound_path, one_hop_ind, id2relation, model)\n",
    "    \n",
    "    subg_score = subgraph_triple_scoring(s, r, t, lower_bound, upper_bound_subg, one_hop_ind, id2relation, model_2)\n",
    "    \n",
    "    ave_score = (path_score + subg_score)/float(2)\n",
    "    \n",
    "    y_score[i] = ave_score\n",
    "    \n",
    "    if i % 20 == 0 and i > 0:\n",
    "        print('evaluating scores', i, len(all_triples))\n",
    "        \n",
    "        # Data to plot precision - recall curve\n",
    "        precision, recall, thresholds = precision_recall_curve(y_test[:i], y_score[:i])\n",
    "        # Use AUC function to calculate the area under the curve of precision recall curve\n",
    "        auc_precision_recall = auc(recall, precision)\n",
    "        print('AUC-PR is:', auc_precision_recall)\n",
    "        \n",
    "        \n",
    "# Data to plot precision - recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_score)\n",
    "# Use AUC function to calculate the area under the curve of precision recall curve\n",
    "auc_precision_recall = auc(recall, precision)\n",
    "print('AUC-PR is:', auc_precision_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a4cc917",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating scores 20 410\n",
      "auc, auc-pr 0.9066666666666667 0.9632159507159507\n",
      "evaluating scores 40 410\n",
      "auc, auc-pr 0.8832417582417582 0.9383358402172485\n",
      "evaluating scores 60 410\n",
      "auc, auc-pr 0.8868778280542986 0.9187169711778878\n",
      "evaluating scores 80 410\n",
      "auc, auc-pr 0.8574168797953964 0.8832984859066255\n",
      "evaluating scores 100 410\n",
      "auc, auc-pr 0.8571428571428572 0.8804859470970942\n",
      "evaluating scores 120 410\n",
      "auc, auc-pr 0.8565034965034964 0.878219716761681\n",
      "evaluating scores 140 410\n",
      "auc, auc-pr 0.8374565528521773 0.8495285032602222\n",
      "evaluating scores 160 410\n",
      "auc, auc-pr 0.8419324577861161 0.8544400935742553\n",
      "evaluating scores 180 410\n",
      "auc, auc-pr 0.846096837944664 0.85367862231622\n",
      "evaluating scores 200 410\n",
      "auc, auc-pr 0.8470388155262104 0.8570755722260028\n",
      "evaluating scores 220 410\n",
      "auc, auc-pr 0.844843271855099 0.8602024085102957\n",
      "evaluating scores 240 410\n",
      "auc, auc-pr 0.851111111111111 0.8608822771895843\n",
      "evaluating scores 260 410\n",
      "auc, auc-pr 0.8595701853057841 0.8734533142359603\n",
      "evaluating scores 280 410\n",
      "auc, auc-pr 0.8474775326797386 0.8662905132079298\n",
      "evaluating scores 300 410\n",
      "auc, auc-pr 0.8415366146458583 0.8494232797720351\n",
      "evaluating scores 320 410\n",
      "auc, auc-pr 0.832140762463343 0.8424042751628847\n",
      "evaluating scores 340 410\n",
      "auc, auc-pr 0.8404347374614932 0.8483074032470602\n",
      "evaluating scores 360 410\n",
      "auc, auc-pr 0.8422677865612648 0.8550643490787154\n",
      "evaluating scores 380 410\n",
      "auc, auc-pr 0.8337073049114935 0.8358265767279701\n",
      "evaluating scores 400 410\n",
      "auc, auc-pr 0.8327708192704819 0.8312522724841698\n",
      "evaluating scores 409 410\n",
      "(final) auc, auc-pr 0.823402736466389 0.8243678268895583\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "##obtain the AUC-PR for the test triples, using sklearn###\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.metrics import auc, plot_precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#we select all the triples in the inductive test set\n",
    "pos_triples = list(data_ind_test)\n",
    "\n",
    "#we build the negative samples by randomly replace head or tail entity in the triple.\n",
    "neg_triples = list()\n",
    "\n",
    "for i in range(len(pos_triples)):\n",
    "    \n",
    "    s_pos, r_pos, t_pos = pos_triples[i][0], pos_triples[i][1], pos_triples[i][2]\n",
    "    \n",
    "    #decide to replace the head or tail entity\n",
    "    number_0 = random.uniform(0, 1)\n",
    "    \n",
    "    if number_0 < 0.5: #replace head entity\n",
    "        s_neg = random.choice(list(new_ent_set))\n",
    "        \n",
    "        #filter out the existing triples\n",
    "        while ((s_neg, r_pos, t_pos) in data_test) or (\n",
    "               (s_neg, r_pos, t_pos) in data_valid) or (\n",
    "               (s_neg, r_pos, t_pos) in data) or (\n",
    "               (s_neg, r_pos, t_pos) in data_ind) or (\n",
    "               (s_neg, r_pos, t_pos) in data_ind_valid) or (\n",
    "               (s_neg, r_pos, t_pos) in data_ind_test):\n",
    "            \n",
    "            s_neg = random.choice(list(new_ent_set))\n",
    "        \n",
    "        neg_triples.append((s_neg, r_pos, t_pos))\n",
    "    \n",
    "    else: #replace tail entity\n",
    "        t_neg = random.choice(list(new_ent_set))\n",
    "        \n",
    "        #filter out the existing triples\n",
    "        while ((s_pos, r_pos, t_neg) in data_test) or (\n",
    "               (s_pos, r_pos, t_neg) in data_valid) or (\n",
    "               (s_pos, r_pos, t_neg) in data) or (\n",
    "               (s_pos, r_pos, t_neg) in data_ind) or (\n",
    "               (s_pos, r_pos, t_neg) in data_ind_valid) or (\n",
    "               (s_pos, r_pos, t_neg) in data_ind_test):\n",
    "            \n",
    "            t_neg = random.choice(list(new_ent_set))\n",
    "        \n",
    "        neg_triples.append((s_pos, r_pos, t_neg))\n",
    "\n",
    "if len(pos_triples) != len(neg_triples):\n",
    "    raise ValueError('error when generating negative triples')\n",
    "        \n",
    "#combine all triples\n",
    "all_triples = pos_triples + neg_triples\n",
    "\n",
    "#obtain the label array\n",
    "arr1 = np.ones((len(pos_triples),))\n",
    "arr2 = np.zeros((len(neg_triples),))\n",
    "y_test = np.concatenate((arr1, arr2))\n",
    "\n",
    "#shuffle positive and negative triples (optional)\n",
    "all_triples, y_test = shuffle(all_triples, y_test)\n",
    "\n",
    "#obtain the score aray\n",
    "y_score = np.zeros((len(y_test),))\n",
    "\n",
    "#implement the scoring\n",
    "for i in range(len(all_triples)):\n",
    "    \n",
    "    s, r, t = all_triples[i][0], all_triples[i][1], all_triples[i][2]\n",
    "    \n",
    "    path_score = path_based_triple_scoring(s, r, t, lower_bound, upper_bound_path, one_hop_ind, id2relation, model)\n",
    "    \n",
    "    subg_score = subgraph_triple_scoring(s, r, t, lower_bound, upper_bound_subg, one_hop_ind, id2relation, model_2)\n",
    "    \n",
    "    ave_score = (path_score + subg_score)/float(2)\n",
    "    \n",
    "    y_score[i] = ave_score\n",
    "    \n",
    "    if i % 20 == 0 and i > 0:\n",
    "        print('evaluating scores', i, len(all_triples))\n",
    "        auc = metrics.roc_auc_score(y_test[:i], y_score[:i])\n",
    "        auc_pr = metrics.average_precision_score(y_test[:i], y_score[:i])\n",
    "        print('auc, auc-pr', auc, auc_pr)\n",
    "        \n",
    "print('evaluating scores', i, len(all_triples))\n",
    "auc = metrics.roc_auc_score(y_test, y_score)\n",
    "auc_pr = metrics.average_precision_score(y_test, y_score)\n",
    "print('(final) auc, auc-pr', auc, auc_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6403a2e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkcorrect (2609, 60, 1611) (2609, 60, 1611) real score 0.8740286946296691 Hits@1 0.0 Hits@3 1.0 Hits@10 1.0 MRR 0.5 rank 1 total_num 0 205\n",
      "checkcorrect (1760, 46, 1759) (1760, 46, 1759) real score 0.9076382040977478 Hits@1 0.0 Hits@3 1.0 Hits@10 1.0 MRR 0.5 rank 1 total_num 1 205\n",
      "checkcorrect (2064, 46, 2227) (2064, 46, 2227) real score 0.40898166298866273 Hits@1 0.0 Hits@3 0.6666666666666666 Hits@10 0.6666666666666666 MRR 0.35714285714285715 rank 13 total_num 2 205\n",
      "checkcorrect (1600, 132, 2129) (1600, 132, 2129) real score 0.895224717259407 Hits@1 0.0 Hits@3 0.75 Hits@10 0.75 MRR 0.39285714285714285 rank 1 total_num 3 205\n",
      "checkcorrect (2163, 26, 2162) (2163, 26, 2162) real score 0.9418887048959732 Hits@1 0.2 Hits@3 0.8 Hits@10 0.8 MRR 0.5142857142857142 rank 0 total_num 4 205\n",
      "checkcorrect (1912, 346, 2330) (1912, 346, 2330) real score 0.0 Hits@1 0.16666666666666666 Hits@3 0.6666666666666666 Hits@10 0.6666666666666666 MRR 0.4437229437229437 rank 10 total_num 5 205\n",
      "checkcorrect (2587, 56, 1629) (2587, 56, 1629) real score 0.8455052703619004 Hits@1 0.14285714285714285 Hits@3 0.5714285714285714 Hits@10 0.7142857142857143 MRR 0.41604823747680886 rank 3 total_num 6 205\n",
      "checkcorrect (2112, 236, 1689) (2112, 236, 1689) real score 0.8061439514160156 Hits@1 0.125 Hits@3 0.5 Hits@10 0.75 MRR 0.39529220779220775 rank 3 total_num 7 205\n",
      "checkcorrect (1702, 18, 1708) (1702, 18, 1708) real score 0.9792613506317138 Hits@1 0.2222222222222222 Hits@3 0.5555555555555556 Hits@10 0.7777777777777778 MRR 0.46248196248196244 rank 0 total_num 8 205\n",
      "checkcorrect (1677, 88, 1794) (1677, 88, 1794) real score 0.8575569033622742 Hits@1 0.2 Hits@3 0.5 Hits@10 0.8 MRR 0.4329004329004329 rank 5 total_num 9 205\n",
      "checkcorrect (2541, 30, 1753) (2541, 30, 1753) real score 0.9799413502216339 Hits@1 0.2727272727272727 Hits@3 0.5454545454545454 Hits@10 0.8181818181818182 MRR 0.48445493900039355 rank 0 total_num 10 205\n",
      "checkcorrect (1633, 92, 2341) (1633, 92, 2341) real score 0.0 Hits@1 0.25 Hits@3 0.5 Hits@10 0.75 MRR 0.44606782106782106 rank 41 total_num 11 205\n",
      "checkcorrect (2171, 178, 2200) (2171, 178, 2200) real score 0.39719219207763673 Hits@1 0.23076923076923078 Hits@3 0.46153846153846156 Hits@10 0.6923076923076923 MRR 0.4187479187479187 rank 10 total_num 12 205\n",
      "checkcorrect (2553, 8, 2015) (2553, 8, 2015) real score 0.8877662569284439 Hits@1 0.21428571428571427 Hits@3 0.42857142857142855 Hits@10 0.7142857142857143 MRR 0.4031230674087817 rank 4 total_num 13 205\n",
      "checkcorrect (1898, 82, 1753) (1898, 82, 1753) real score 0.8998787105083466 Hits@1 0.2 Hits@3 0.4666666666666667 Hits@10 0.7333333333333333 MRR 0.4095815295815296 rank 1 total_num 14 205\n",
      "checkcorrect (1852, 54, 2121) (1852, 54, 2121) real score 0.9744530498981476 Hits@1 0.25 Hits@3 0.5 Hits@10 0.75 MRR 0.446482683982684 rank 0 total_num 15 205\n",
      "checkcorrect (1720, 126, 1980) (1720, 126, 1980) real score 0.8843632787466049 Hits@1 0.23529411764705882 Hits@3 0.47058823529411764 Hits@10 0.7647058823529411 MRR 0.43198370257193786 rank 4 total_num 16 205\n",
      "checkcorrect (2156, 146, 2196) (2156, 146, 2196) real score 0.90175302028656 Hits@1 0.2777777777777778 Hits@3 0.5 Hits@10 0.7777777777777778 MRR 0.4635401635401636 rank 0 total_num 17 205\n",
      "checkcorrect (1895, 210, 2659) (1895, 210, 2659) real score 0.9506143510341645 Hits@1 0.3157894736842105 Hits@3 0.5263157894736842 Hits@10 0.7894736842105263 MRR 0.4917748917748918 rank 0 total_num 18 205\n",
      "checkcorrect (2622, 46, 2463) (2622, 46, 2463) real score 0.7815201550722122 Hits@1 0.3 Hits@3 0.5 Hits@10 0.8 MRR 0.47343614718614724 rank 7 total_num 19 205\n",
      "checkcorrect (2449, 234, 1834) (2449, 234, 1834) real score 0.3265367589890957 Hits@1 0.2857142857142857 Hits@3 0.47619047619047616 Hits@10 0.8095238095238095 MRR 0.4561825740397169 rank 8 total_num 20 205\n",
      "checkcorrect (1869, 350, 1947) (1869, 350, 1947) real score 0.0 Hits@1 0.2727272727272727 Hits@3 0.45454545454545453 Hits@10 0.7727272727272727 MRR 0.4368244129607767 rank 32 total_num 21 205\n",
      "checkcorrect (2033, 188, 2582) (2033, 188, 2582) real score 0.0 Hits@1 0.2608695652173913 Hits@3 0.43478260869565216 Hits@10 0.7391304347826086 MRR 0.4187016123972646 rank 49 total_num 22 205\n",
      "checkcorrect (1641, 76, 2371) (1641, 76, 2371) real score 0.7921829491853714 Hits@1 0.2916666666666667 Hits@3 0.4583333333333333 Hits@10 0.75 MRR 0.4429223785473786 rank 0 total_num 23 205\n",
      "checkcorrect (1783, 8, 1609) (1783, 8, 1609) real score 0.8224147975444793 Hits@1 0.28 Hits@3 0.44 Hits@10 0.76 MRR 0.43520548340548343 rank 3 total_num 24 205\n",
      "checkcorrect (1663, 72, 2075) (1663, 72, 2075) real score 0.8726520478725434 Hits@1 0.2692307692307692 Hits@3 0.4230769230769231 Hits@10 0.7692307692307693 MRR 0.4223129648129648 rank 9 total_num 25 205\n",
      "checkcorrect (1938, 0, 2112) (1938, 0, 2112) real score 0.971152338385582 Hits@1 0.2962962962962963 Hits@3 0.4444444444444444 Hits@10 0.7777777777777778 MRR 0.4437087809310032 rank 0 total_num 26 205\n",
      "checkcorrect (1677, 88, 2223) (1677, 88, 2223) real score 0.8665333122015 Hits@1 0.2857142857142857 Hits@3 0.4642857142857143 Hits@10 0.7857142857142857 MRR 0.4457191816120388 rank 1 total_num 27 205\n",
      "checkcorrect (1682, 18, 1827) (1682, 18, 1827) real score 0.9692032605409622 Hits@1 0.3103448275862069 Hits@3 0.4827586206896552 Hits@10 0.7931034482758621 MRR 0.46483231328058916 rank 0 total_num 28 205\n",
      "checkcorrect (1906, 18, 1861) (1906, 18, 1861) real score 0.9583737045526504 Hits@1 0.3 Hits@3 0.5 Hits@10 0.8 MRR 0.46600456950456953 rank 1 total_num 29 205\n",
      "checkcorrect (2002, 116, 2342) (2002, 116, 2342) real score 0.0 Hits@1 0.2903225806451613 Hits@3 0.4838709677419355 Hits@10 0.7741935483870968 MRR 0.45237468858085966 rank 22 total_num 30 205\n",
      "checkcorrect (1852, 10, 2325) (1852, 10, 2325) real score 0.9172418355941773 Hits@1 0.28125 Hits@3 0.46875 Hits@10 0.78125 MRR 0.4444879795627078 rank 4 total_num 31 205\n",
      "checkcorrect (1675, 212, 1617) (1675, 212, 1617) real score 0.9015332013368607 Hits@1 0.30303030303030304 Hits@3 0.48484848484848486 Hits@10 0.7878787878787878 MRR 0.4613216771517167 rank 0 total_num 32 205\n",
      "checkcorrect (2073, 212, 1947) (2073, 212, 1947) real score 0.7914443135261535 Hits@1 0.29411764705882354 Hits@3 0.47058823529411764 Hits@10 0.7941176470588235 MRR 0.4536357454707838 rank 4 total_num 33 205\n",
      "checkcorrect (2486, 212, 1617) (2486, 212, 1617) real score 0.025714330701157452 Hits@1 0.2857142857142857 Hits@3 0.45714285714285713 Hits@10 0.7714285714285715 MRR 0.44210329560019 rank 19 total_num 34 205\n",
      "checkcorrect (2393, 196, 1886) (2393, 196, 1886) real score 0.9125030815601349 Hits@1 0.2777777777777778 Hits@3 0.4444444444444444 Hits@10 0.7777777777777778 MRR 0.43379090246843865 rank 6 total_num 35 205\n",
      "checkcorrect (1841, 346, 1787) (1841, 346, 1787) real score -0.09514892846345901 Hits@1 0.2702702702702703 Hits@3 0.43243243243243246 Hits@10 0.7567567567567568 MRR 0.4227260198044927 rank 40 total_num 36 205\n",
      "checkcorrect (2118, 342, 2638) (2118, 342, 2638) real score 0.40140891820192337 Hits@1 0.2894736842105263 Hits@3 0.4473684210526316 Hits@10 0.7631578947368421 MRR 0.4379174403359534 rank 0 total_num 37 205\n",
      "checkcorrect (2436, 122, 1756) (2436, 122, 1756) real score 0.7859449729323387 Hits@1 0.28205128205128205 Hits@3 0.46153846153846156 Hits@10 0.7692307692307693 MRR 0.43523579656665545 rank 2 total_num 38 205\n",
      "checkcorrect (1724, 68, 1723) (1724, 68, 1723) real score 0.9014177739620208 Hits@1 0.3 Hits@3 0.475 Hits@10 0.775 MRR 0.44935490165248904 rank 0 total_num 39 205\n",
      "checkcorrect (2261, 262, 2472) (2261, 262, 2472) real score 0.8865074574947358 Hits@1 0.2926829268292683 Hits@3 0.4878048780487805 Hits@10 0.7804878048780488 MRR 0.4505901479536479 rank 1 total_num 40 205\n",
      "checkcorrect (1944, 2, 1654) (1944, 2, 1654) real score 0.9357173159718513 Hits@1 0.2857142857142857 Hits@3 0.5 Hits@10 0.7857142857142857 MRR 0.45176657300237055 rank 1 total_num 41 205\n",
      "checkcorrect (2207, 262, 1738) (2207, 262, 1738) real score 0.8595003455877304 Hits@1 0.27906976744186046 Hits@3 0.4883720930232558 Hits@10 0.7906976744186046 MRR 0.4451363426224705 rank 5 total_num 42 205\n",
      "checkcorrect (1894, 210, 2659) (1894, 210, 2659) real score 0.9488568514585495 Hits@1 0.29545454545454547 Hits@3 0.5 Hits@10 0.7954545454545454 MRR 0.4577468802901416 rank 0 total_num 43 205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkcorrect (2438, 20, 1756) (2438, 20, 1756) real score 0.8196619391441345 Hits@1 0.28888888888888886 Hits@3 0.5111111111111111 Hits@10 0.8 MRR 0.4586858385059162 rank 1 total_num 44 205\n",
      "checkcorrect (2292, 58, 1764) (2292, 58, 1764) real score 0.8870183318853377 Hits@1 0.2826086956521739 Hits@3 0.5217391304347826 Hits@10 0.8043478260869565 MRR 0.4595839724514398 rank 1 total_num 45 205\n",
      "checkcorrect (1748, 74, 1617) (1748, 74, 1617) real score 0.8909080445766449 Hits@1 0.2765957446808511 Hits@3 0.5319148936170213 Hits@10 0.8085106382978723 MRR 0.4604438879311964 rank 1 total_num 46 205\n",
      "checkcorrect (2325, 106, 1693) (2325, 106, 1693) real score 0.8959174066781997 Hits@1 0.2708333333333333 Hits@3 0.5208333333333334 Hits@10 0.8125 MRR 0.45605964026596313 rank 3 total_num 47 205\n",
      "checkcorrect (2143, 56, 1698) (2143, 56, 1698) real score 0.7193954840302468 Hits@1 0.2653061224489796 Hits@3 0.5102040816326531 Hits@10 0.8163265306122449 MRR 0.4490198743648437 rank 8 total_num 48 205\n",
      "checkcorrect (2315, 308, 1717) (2315, 308, 1717) real score 0.4201738931238651 Hits@1 0.26 Hits@3 0.52 Hits@10 0.82 MRR 0.4500394768775468 rank 1 total_num 49 205\n",
      "checkcorrect (2614, 234, 1834) (2614, 234, 1834) real score 0.8439758062362671 Hits@1 0.27450980392156865 Hits@3 0.5294117647058824 Hits@10 0.8235294117647058 MRR 0.4608230165466145 rank 0 total_num 50 205\n",
      "checkcorrect (1718, 204, 2045) (1718, 204, 2045) real score 0.2812406521290541 Hits@1 0.2692307692307692 Hits@3 0.5192307692307693 Hits@10 0.8076923076923077 MRR 0.4532430867412309 rank 14 total_num 51 205\n",
      "checkcorrect (2262, 2, 1654) (2262, 2, 1654) real score 0.6126421429216862 Hits@1 0.2641509433962264 Hits@3 0.5094339622641509 Hits@10 0.8113207547169812 MRR 0.44846491529328314 rank 4 total_num 52 205\n",
      "checkcorrect (1924, 56, 1698) (1924, 56, 1698) real score 0.8903033167123795 Hits@1 0.25925925925925924 Hits@3 0.5 Hits@10 0.8148148148148148 MRR 0.4432464292076051 rank 5 total_num 53 205\n",
      "checkcorrect (1933, 10, 1818) (1933, 10, 1818) real score 0.9663817137479782 Hits@1 0.2727272727272727 Hits@3 0.509090909090909 Hits@10 0.8181818181818182 MRR 0.45336922140383046 rank 0 total_num 54 205\n",
      "checkcorrect (1753, 10, 2109) (1753, 10, 2109) real score 0.8990839928388595 Hits@1 0.26785714285714285 Hits@3 0.5 Hits@10 0.8214285714285714 MRR 0.4497376281644763 rank 3 total_num 55 205\n",
      "checkcorrect (1680, 34, 1670) (1680, 34, 1670) real score 0.9927631825208664 Hits@1 0.2807017543859649 Hits@3 0.5087719298245614 Hits@10 0.8245614035087719 MRR 0.45939135398615216 rank 0 total_num 56 205\n",
      "checkcorrect (2578, 204, 1942) (2578, 204, 1942) real score 0.5608677223324776 Hits@1 0.27586206896551724 Hits@3 0.5 Hits@10 0.8275862068965517 MRR 0.4536259858139772 rank 7 total_num 57 205\n",
      "checkcorrect (1623, 56, 1824) (1623, 56, 1824) real score 0.8514641553163529 Hits@1 0.2711864406779661 Hits@3 0.4915254237288136 Hits@10 0.8305084745762712 MRR 0.44876226854029394 rank 5 total_num 58 205\n",
      "checkcorrect (1883, 156, 2518) (1883, 156, 2518) real score 0.7397428393363952 Hits@1 0.26666666666666666 Hits@3 0.5 Hits@10 0.8333333333333334 MRR 0.44961623073128903 rank 1 total_num 59 205\n",
      "checkcorrect (2247, 54, 1630) (2247, 54, 1630) real score 0.9033192247152328 Hits@1 0.26229508196721313 Hits@3 0.5081967213114754 Hits@10 0.8360655737704918 MRR 0.44770995372476513 rank 2 total_num 60 205\n",
      "checkcorrect (1853, 20, 1744) (1853, 20, 1744) real score 0.8328034400939941 Hits@1 0.25806451612903225 Hits@3 0.5 Hits@10 0.8387096774193549 MRR 0.44371463189049476 rank 4 total_num 61 205\n",
      "checkcorrect (1700, 8, 1609) (1700, 8, 1609) real score 0.9041296929121018 Hits@1 0.25396825396825395 Hits@3 0.5079365079365079 Hits@10 0.8412698412698413 MRR 0.4419625477864128 rank 2 total_num 62 205\n",
      "checkcorrect (1612, 88, 2326) (1612, 88, 2326) real score 0.9318938821554184 Hits@1 0.265625 Hits@3 0.515625 Hits@10 0.84375 MRR 0.4506818829772501 rank 0 total_num 63 205\n",
      "checkcorrect (1902, 94, 1901) (1902, 94, 1901) real score 0.8476027369499206 Hits@1 0.26153846153846155 Hits@3 0.5076923076923077 Hits@10 0.8461538461538461 MRR 0.4463124181109334 rank 5 total_num 64 205\n",
      "checkcorrect (2343, 46, 2065) (2343, 46, 2065) real score 0.4530011773109436 Hits@1 0.25757575757575757 Hits@3 0.5 Hits@10 0.8333333333333334 MRR 0.4409275192139358 rank 10 total_num 65 205\n",
      "checkcorrect (2659, 210, 1621) (2659, 210, 1621) real score 0.9523305177688599 Hits@1 0.26865671641791045 Hits@3 0.5074626865671642 Hits@10 0.835820895522388 MRR 0.44927188459880246 rank 0 total_num 66 205\n",
      "checkcorrect (2576, 346, 1605) (2576, 346, 1605) real score -0.10045930799096822 Hits@1 0.2647058823529412 Hits@3 0.5 Hits@10 0.8235294117647058 MRR 0.4430325921782318 rank 39 total_num 67 205\n",
      "checkcorrect (1673, 234, 1995) (1673, 234, 1995) real score 0.6713835164904594 Hits@1 0.2753623188405797 Hits@3 0.5072463768115942 Hits@10 0.8260869565217391 MRR 0.4511045835959386 rank 0 total_num 68 205\n",
      "checkcorrect (2594, 346, 1787) (2594, 346, 1787) real score -0.10057074837386609 Hits@1 0.2714285714285714 Hits@3 0.5 Hits@10 0.8142857142857143 MRR 0.44505705779853627 rank 35 total_num 69 205\n",
      "checkcorrect (2365, 2, 1670) (2365, 2, 1670) real score 0.9846769332885742 Hits@1 0.2676056338028169 Hits@3 0.5070422535211268 Hits@10 0.8169014084507042 MRR 0.44583090205489495 rank 1 total_num 70 205\n",
      "checkcorrect (1963, 342, 2176) (1963, 342, 2176) real score 0.35172501504421233 Hits@1 0.2638888888888889 Hits@3 0.5 Hits@10 0.8194444444444444 MRR 0.44311102841524364 rank 3 total_num 71 205\n",
      "checkcorrect (1803, 292, 2291) (1803, 292, 2291) real score 0.7169881820678712 Hits@1 0.273972602739726 Hits@3 0.5068493150684932 Hits@10 0.821917808219178 MRR 0.4507396444643499 rank 0 total_num 72 205\n",
      "checkcorrect (1607, 46, 2472) (1607, 46, 2472) real score 0.9173172920942306 Hits@1 0.28378378378378377 Hits@3 0.5135135135135135 Hits@10 0.8243243243243243 MRR 0.4581620817013181 rank 0 total_num 73 205\n",
      "checkcorrect (1753, 10, 2129) (1753, 10, 2129) real score 0.943435361981392 Hits@1 0.29333333333333333 Hits@3 0.52 Hits@10 0.8266666666666667 MRR 0.46538658727863386 rank 0 total_num 74 205\n",
      "checkcorrect (2151, 106, 2413) (2151, 106, 2413) real score 0.9236642420291901 Hits@1 0.3026315789473684 Hits@3 0.5263157894736842 Hits@10 0.8289473684210527 MRR 0.4724209742881255 rank 0 total_num 75 205\n",
      "checkcorrect (2217, 46, 2154) (2217, 46, 2154) real score 0.9414986670017242 Hits@1 0.2987012987012987 Hits@3 0.5324675324675324 Hits@10 0.8311688311688312 MRR 0.4727791434532148 rank 1 total_num 76 205\n",
      "checkcorrect (2511, 56, 1824) (2511, 56, 1824) real score 0.6703498795628547 Hits@1 0.2948717948717949 Hits@3 0.5256410256410257 Hits@10 0.8205128205128205 MRR 0.4677862484516779 rank 11 total_num 77 205\n",
      "checkcorrect (1619, 344, 2273) (1619, 344, 2273) real score 0.07852485328912734 Hits@1 0.2911392405063291 Hits@3 0.5189873417721519 Hits@10 0.8227848101265823 MRR 0.46327137329546825 rank 8 total_num 78 205\n",
      "checkcorrect (2140, 188, 2613) (2140, 188, 2613) real score 0.8292390793561936 Hits@1 0.2875 Hits@3 0.5125 Hits@10 0.825 MRR 0.4606054811292749 rank 3 total_num 79 205\n",
      "checkcorrect (1642, 56, 1824) (1642, 56, 1824) real score 0.8047315090894699 Hits@1 0.2839506172839506 Hits@3 0.5061728395061729 Hits@10 0.8271604938271605 MRR 0.457388129510395 rank 4 total_num 80 205\n",
      "checkcorrect (1773, 310, 1617) (1773, 310, 1617) real score 0.36652861833572387 Hits@1 0.2804878048780488 Hits@3 0.5 Hits@10 0.8292682926829268 MRR 0.4542492498822195 rank 4 total_num 81 205\n",
      "checkcorrect (1958, 18, 1957) (1958, 18, 1957) real score 0.3264723211526871 Hits@1 0.27710843373493976 Hits@3 0.4939759036144578 Hits@10 0.8192771084337349 MRR 0.4493500908188078 rank 20 total_num 82 205\n",
      "checkcorrect (1811, 46, 1961) (1811, 46, 1961) real score 0.9446049064397812 Hits@1 0.2857142857142857 Hits@3 0.5 Hits@10 0.8214285714285714 MRR 0.4559054468804887 rank 0 total_num 83 205\n",
      "checkcorrect (2685, 46, 1608) (2685, 46, 1608) real score 0.0 Hits@1 0.2823529411764706 Hits@3 0.49411764705882355 Hits@10 0.8117647058823529 MRR 0.45078194942619243 rank 48 total_num 84 205\n",
      "checkcorrect (2314, 74, 1617) (2314, 74, 1617) real score 0.7362326756119728 Hits@1 0.27906976744186046 Hits@3 0.4883720930232558 Hits@10 0.813953488372093 MRR 0.4472014284195756 rank 6 total_num 85 205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkcorrect (1662, 110, 1661) (1662, 110, 1661) real score 0.8174346506595611 Hits@1 0.27586206896551724 Hits@3 0.4942528735632184 Hits@10 0.8160919540229885 MRR 0.44589259974042345 rank 2 total_num 86 205\n",
      "checkcorrect (2194, 66, 2195) (2194, 66, 2195) real score 0.0 Hits@1 0.2727272727272727 Hits@3 0.48863636363636365 Hits@10 0.8068181818181818 MRR 0.4411412949453934 rank 35 total_num 87 205\n",
      "checkcorrect (1799, 26, 1613) (1799, 26, 1613) real score 0.9041504949331284 Hits@1 0.2696629213483146 Hits@3 0.4943820224719101 Hits@10 0.8089887640449438 MRR 0.4399299695340219 rank 2 total_num 88 205\n",
      "checkcorrect (1986, 212, 1796) (1986, 212, 1796) real score 0.7754508644342422 Hits@1 0.26666666666666666 Hits@3 0.4888888888888889 Hits@10 0.8111111111111111 MRR 0.4366291603487233 rank 6 total_num 89 205\n",
      "checkcorrect (1692, 18, 2053) (1692, 18, 2053) real score 0.9737893074750901 Hits@1 0.26373626373626374 Hits@3 0.4945054945054945 Hits@10 0.8131868131868132 MRR 0.4354940413705322 rank 2 total_num 90 205\n",
      "checkcorrect (1852, 10, 1732) (1852, 10, 1732) real score 0.9587879210710526 Hits@1 0.2608695652173913 Hits@3 0.5 Hits@10 0.8152173913043478 MRR 0.43438359889186706 rank 2 total_num 91 205\n",
      "checkcorrect (1866, 188, 2354) (1866, 188, 2354) real score 0.7657746702432633 Hits@1 0.25806451612903225 Hits@3 0.4946236559139785 Hits@10 0.8172043010752689 MRR 0.43150492220127346 rank 5 total_num 92 205\n",
      "checkcorrect (1677, 88, 2234) (1677, 88, 2234) real score 0.0 Hits@1 0.2553191489361702 Hits@3 0.48936170212765956 Hits@10 0.8085106382978723 MRR 0.4271739149853284 rank 40 total_num 93 205\n",
      "checkcorrect (1615, 126, 2212) (1615, 126, 2212) real score 0.9238352417945861 Hits@1 0.25263157894736843 Hits@3 0.49473684210526314 Hits@10 0.8105263157894737 MRR 0.42794050535390393 rank 1 total_num 94 205\n",
      "checkcorrect (1924, 50, 1911) (1924, 50, 1911) real score 0.936910080909729 Hits@1 0.25 Hits@3 0.5 Hits@10 0.8125 MRR 0.4269550139786897 rank 2 total_num 95 205\n",
      "checkcorrect (2632, 312, 1624) (2632, 312, 1624) real score 0.0 Hits@1 0.24742268041237114 Hits@3 0.4948453608247423 Hits@10 0.8041237113402062 MRR 0.4228320450410436 rank 36 total_num 96 205\n",
      "checkcorrect (1935, 224, 1936) (1935, 224, 1936) real score 0.0 Hits@1 0.24489795918367346 Hits@3 0.4897959183673469 Hits@10 0.7959183673469388 MRR 0.4189255956018493 rank 24 total_num 97 205\n",
      "checkcorrect (2222, 188, 1676) (2222, 188, 1676) real score 0.8363858520984649 Hits@1 0.24242424242424243 Hits@3 0.494949494949495 Hits@10 0.797979797979798 MRR 0.4197445289796084 rank 1 total_num 98 205\n",
      "checkcorrect (1618, 76, 2198) (1618, 76, 2198) real score 0.5911349646747113 Hits@1 0.24 Hits@3 0.49 Hits@10 0.8 MRR 0.41721375035647895 rank 5 total_num 99 205\n",
      "checkcorrect (2553, 8, 1918) (2553, 8, 1918) real score 0.8860746085643768 Hits@1 0.2376237623762376 Hits@3 0.48514851485148514 Hits@10 0.801980198019802 MRR 0.4147330861615303 rank 5 total_num 100 205\n",
      "checkcorrect (2123, 8, 1609) (2123, 8, 1609) real score 0.9600968331098556 Hits@1 0.24509803921568626 Hits@3 0.49019607843137253 Hits@10 0.803921568627451 MRR 0.4204709970815153 rank 0 total_num 101 205\n",
      "checkcorrect (2078, 234, 1834) (2078, 234, 1834) real score 0.4532040223479271 Hits@1 0.2524271844660194 Hits@3 0.49514563106796117 Hits@10 0.8058252427184466 MRR 0.42609749225548116 rank 0 total_num 102 205\n",
      "checkcorrect (1753, 10, 2325) (1753, 10, 2325) real score 0.9062789142131806 Hits@1 0.25 Hits@3 0.49038461538461536 Hits@10 0.8076923076923077 MRR 0.4236029650863579 rank 5 total_num 103 205\n",
      "checkcorrect (1612, 88, 1676) (1612, 88, 1676) real score 0.796915552020073 Hits@1 0.24761904761904763 Hits@3 0.4857142857142857 Hits@10 0.8095238095238095 MRR 0.4211559527204561 rank 5 total_num 104 205\n",
      "checkcorrect (1736, 310, 2335) (1736, 310, 2335) real score 0.0 Hits@1 0.24528301886792453 Hits@3 0.4811320754716981 Hits@10 0.8018867924528302 MRR 0.4173924269610388 rank 44 total_num 105 205\n",
      "checkcorrect (2163, 18, 1697) (2163, 18, 1697) real score 0.8297324240207673 Hits@1 0.24299065420560748 Hits@3 0.4766355140186916 Hits@10 0.8037383177570093 MRR 0.4150491955564185 rank 5 total_num 106 205\n",
      "checkcorrect (1812, 6, 2261) (1812, 6, 2261) real score 0.9729089885950089 Hits@1 0.25 Hits@3 0.48148148148148145 Hits@10 0.8055555555555556 MRR 0.4204654067086739 rank 0 total_num 107 205\n",
      "checkcorrect (1757, 10, 2312) (1757, 10, 2312) real score 0.963253489136696 Hits@1 0.24770642201834864 Hits@3 0.48623853211009177 Hits@10 0.8073394495412844 MRR 0.4211950818764842 rank 1 total_num 108 205\n",
      "checkcorrect (2111, 30, 1666) (2111, 30, 1666) real score 0.747709047794342 Hits@1 0.24545454545454545 Hits@3 0.4818181818181818 Hits@10 0.8090909090909091 MRR 0.4188811871927586 rank 5 total_num 109 205\n",
      "checkcorrect (1606, 26, 1601) (1606, 26, 1601) real score 0.8816690444946289 Hits@1 0.24324324324324326 Hits@3 0.4864864864864865 Hits@10 0.8108108108108109 MRR 0.4181104858066376 rank 2 total_num 110 205\n",
      "checkcorrect (1612, 88, 1886) (1612, 88, 1886) real score 0.8175112053751945 Hits@1 0.24107142857142858 Hits@3 0.48214285714285715 Hits@10 0.8125 MRR 0.41565286667316004 rank 6 total_num 111 205\n",
      "checkcorrect (2381, 152, 2462) (2381, 152, 2462) real score 0.8409594684839248 Hits@1 0.24778761061946902 Hits@3 0.48672566371681414 Hits@10 0.8141592920353983 MRR 0.4208240802424241 rank 0 total_num 112 205\n",
      "checkcorrect (1864, 2, 1644) (1864, 2, 1644) real score 0.9280066519975663 Hits@1 0.24561403508771928 Hits@3 0.49122807017543857 Hits@10 0.8157894736842105 MRR 0.42151860585433265 rank 1 total_num 113 205\n",
      "checkcorrect (1607, 30, 1753) (1607, 30, 1753) real score 0.8609930366277695 Hits@1 0.24347826086956523 Hits@3 0.48695652173913045 Hits@10 0.8173913043478261 MRR 0.4190954626978354 rank 6 total_num 114 205\n",
      "checkcorrect (1754, 50, 2480) (1754, 50, 2480) real score 0.9458420783281327 Hits@1 0.25 Hits@3 0.49137931034482757 Hits@10 0.8189655172413793 MRR 0.4241032604331989 rank 0 total_num 115 205\n",
      "checkcorrect (2106, 108, 1840) (2106, 108, 1840) real score 0.9582517325878144 Hits@1 0.2564102564102564 Hits@3 0.49572649572649574 Hits@10 0.8205128205128205 MRR 0.4290254547884707 rank 0 total_num 116 205\n",
      "checkcorrect (1606, 170, 1816) (1606, 170, 1816) real score 0.733544185757637 Hits@1 0.2542372881355932 Hits@3 0.5 Hits@10 0.8220338983050848 MRR 0.42962693398517854 rank 1 total_num 117 205\n",
      "checkcorrect (2513, 18, 2477) (2513, 18, 2477) real score 0.4641864985227585 Hits@1 0.25210084033613445 Hits@3 0.4957983193277311 Hits@10 0.8151260504201681 MRR 0.42639859458576146 rank 21 total_num 118 205\n",
      "checkcorrect (2198, 30, 1666) (2198, 30, 1666) real score 0.887397649884224 Hits@1 0.25 Hits@3 0.5 Hits@10 0.8166666666666667 MRR 0.42562305074199125 rank 2 total_num 119 205\n",
      "checkcorrect (1620, 210, 1894) (1620, 210, 1894) real score 0.9549829125404359 Hits@1 0.256198347107438 Hits@3 0.5041322314049587 Hits@10 0.8181818181818182 MRR 0.4303699676780079 rank 0 total_num 120 205\n",
      "checkcorrect (1738, 28, 1923) (1738, 28, 1923) real score 0.9274490356445313 Hits@1 0.2540983606557377 Hits@3 0.5 Hits@10 0.819672131147541 MRR 0.42848168925441765 rank 4 total_num 121 205\n",
      "checkcorrect (2280, 68, 2279) (2280, 68, 2279) real score 0.0 Hits@1 0.25203252032520324 Hits@3 0.4959349593495935 Hits@10 0.8130081300813008 MRR 0.4255788183777848 rank 13 total_num 122 205\n",
      "checkcorrect (2019, 292, 2191) (2019, 292, 2191) real score 0.8379185378551484 Hits@1 0.25806451612903225 Hits@3 0.5 Hits@10 0.8145161290322581 MRR 0.43021124726183485 rank 0 total_num 123 205\n",
      "checkcorrect (1656, 18, 1669) (1656, 18, 1669) real score 0.8541934549808503 Hits@1 0.256 Hits@3 0.496 Hits@10 0.816 MRR 0.4279124144265974 rank 6 total_num 124 205\n",
      "checkcorrect (2005, 74, 1617) (2005, 74, 1617) real score 0.2517856743186712 Hits@1 0.25396825396825395 Hits@3 0.49206349206349204 Hits@10 0.8174603174603174 MRR 0.4255083476454339 rank 7 total_num 125 205\n",
      "checkcorrect (1677, 152, 1756) (1677, 152, 1756) real score 0.5169885143637657 Hits@1 0.25196850393700787 Hits@3 0.4881889763779528 Hits@10 0.8188976377952756 MRR 0.4234702241731601 rank 5 total_num 126 205\n",
      "checkcorrect (1756, 72, 2128) (1756, 72, 2128) real score 0.714191123843193 Hits@1 0.2578125 Hits@3 0.4921875 Hits@10 0.8203125 MRR 0.4279743630468073 rank 0 total_num 127 205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkcorrect (1922, 2, 1644) (1922, 2, 1644) real score 0.7344726487994194 Hits@1 0.26356589147286824 Hits@3 0.49612403100775193 Hits@10 0.8217054263565892 MRR 0.43240867031001035 rank 0 total_num 128 205\n",
      "checkcorrect (2629, 18, 2079) (2629, 18, 2079) real score 0.0 Hits@1 0.26153846153846155 Hits@3 0.49230769230769234 Hits@10 0.8153846153846154 MRR 0.4293783077573298 rank 25 total_num 129 205\n",
      "checkcorrect (1614, 126, 2655) (1614, 126, 2655) real score 0.9137587219476699 Hits@1 0.26717557251908397 Hits@3 0.4961832061068702 Hits@10 0.816793893129771 MRR 0.4337341985378082 rank 0 total_num 130 205\n",
      "checkcorrect (1754, 18, 1876) (1754, 18, 1876) real score 0.663909649476409 Hits@1 0.26515151515151514 Hits@3 0.49242424242424243 Hits@10 0.8106060606060606 MRR 0.4310796465286834 rank 11 total_num 131 205\n",
      "checkcorrect (1809, 56, 2052) (1809, 56, 2052) real score 0.8224323272705079 Hits@1 0.2631578947368421 Hits@3 0.48872180451127817 Hits@10 0.8120300751879699 MRR 0.42877829580290383 rank 7 total_num 132 205\n",
      "checkcorrect (2654, 196, 2267) (2654, 196, 2267) real score 0.6143418684601784 Hits@1 0.26119402985074625 Hits@3 0.48507462686567165 Hits@10 0.8134328358208955 MRR 0.42664455585554745 rank 6 total_num 133 205\n",
      "checkcorrect (1860, 18, 1859) (1860, 18, 1859) real score 0.8010923177003861 Hits@1 0.25925925925925924 Hits@3 0.4888888888888889 Hits@10 0.8148148148148148 MRR 0.4271879295158767 rank 1 total_num 134 205\n",
      "checkcorrect (1720, 126, 2616) (1720, 126, 2616) real score 0.7584815889596939 Hits@1 0.25735294117647056 Hits@3 0.4852941176470588 Hits@10 0.8161764705882353 MRR 0.42496595944590704 rank 7 total_num 135 205\n",
      "checkcorrect (2567, 32, 2449) (2567, 32, 2449) real score 0.0 Hits@1 0.25547445255474455 Hits@3 0.48175182481751827 Hits@10 0.8102189781021898 MRR 0.4220921203258639 rank 31 total_num 136 205\n",
      "checkcorrect (1666, 6, 1889) (1666, 6, 1889) real score 0.9735315650701523 Hits@1 0.2608695652173913 Hits@3 0.4855072463768116 Hits@10 0.8115942028985508 MRR 0.4262798585843722 rank 0 total_num 137 205\n",
      "checkcorrect (2067, 46, 2046) (2067, 46, 2046) real score 0.24999005096033217 Hits@1 0.2589928057553957 Hits@3 0.48201438848920863 Hits@10 0.8057553956834532 MRR 0.423766500442924 rank 12 total_num 138 205\n",
      "checkcorrect (2240, 332, 2322) (2240, 332, 2322) real score 0.08659664695151151 Hits@1 0.2571428571428571 Hits@3 0.4857142857142857 Hits@10 0.8071428571428572 MRR 0.4231205492492841 rank 2 total_num 139 205\n",
      "checkcorrect (2156, 146, 1758) (2156, 146, 1758) real score 0.8735433369874954 Hits@1 0.2553191489361702 Hits@3 0.48936170212765956 Hits@10 0.8085106382978723 MRR 0.4224837604839228 rank 2 total_num 140 205\n",
      "checkcorrect (1932, 50, 2090) (1932, 50, 2090) real score 0.940379160642624 Hits@1 0.2535211267605634 Hits@3 0.49295774647887325 Hits@10 0.8098591549295775 MRR 0.42302964949459937 rank 1 total_num 141 205\n",
      "checkcorrect (1960, 132, 1722) (1960, 132, 1722) real score 0.9034697741270066 Hits@1 0.25874125874125875 Hits@3 0.4965034965034965 Hits@10 0.8111888111888111 MRR 0.4270644071904413 rank 0 total_num 142 205\n",
      "checkcorrect (2193, 212, 2291) (2193, 212, 2291) real score 0.8169501334428787 Hits@1 0.2569444444444444 Hits@3 0.4930555555555556 Hits@10 0.8125 MRR 0.42496673769606325 rank 7 total_num 143 205\n",
      "checkcorrect (1752, 132, 1987) (1752, 132, 1987) real score 0.8434812337160111 Hits@1 0.25517241379310346 Hits@3 0.4896551724137931 Hits@10 0.8137931034482758 MRR 0.4228980015740214 rank 7 total_num 144 205\n",
      "checkcorrect (2493, 74, 2494) (2493, 74, 2494) real score 0.0 Hits@1 0.2534246575342466 Hits@3 0.4863013698630137 Hits@10 0.8082191780821918 MRR 0.42036193018616763 rank 18 total_num 145 205\n",
      "checkcorrect (1772, 76, 1882) (1772, 76, 1882) real score 0.7284893721342087 Hits@1 0.25170068027210885 Hits@3 0.48299319727891155 Hits@10 0.8027210884353742 MRR 0.4180692186429511 rank 11 total_num 146 205\n",
      "checkcorrect (1753, 6, 2261) (1753, 6, 2261) real score 0.9750591129064561 Hits@1 0.25675675675675674 Hits@3 0.4864864864864865 Hits@10 0.8040540540540541 MRR 0.4220011833818501 rank 0 total_num 147 205\n",
      "checkcorrect (2405, 68, 2404) (2405, 68, 2404) real score 0.0 Hits@1 0.2550335570469799 Hits@3 0.48322147651006714 Hits@10 0.7986577181208053 MRR 0.4193786922182135 rank 31 total_num 148 205\n",
      "checkcorrect (2217, 26, 1704) (2217, 26, 1704) real score 0.9368042498826981 Hits@1 0.25333333333333335 Hits@3 0.4866666666666667 Hits@10 0.8 MRR 0.4199161676034254 rank 1 total_num 149 205\n",
      "checkcorrect (2585, 56, 1629) (2585, 56, 1629) real score 0.7803437560796738 Hits@1 0.25165562913907286 Hits@3 0.48344370860927155 Hits@10 0.7947019867549668 MRR 0.41773731279088017 rank 10 total_num 150 205\n",
      "checkcorrect (1601, 30, 1753) (1601, 30, 1753) real score 0.9011423349380493 Hits@1 0.25 Hits@3 0.48026315789473684 Hits@10 0.7960526315789473 MRR 0.4166337778383086 rank 3 total_num 151 205\n",
      "checkcorrect (2454, 18, 1928) (2454, 18, 1928) real score 0.8705432832241058 Hits@1 0.24836601307189543 Hits@3 0.48366013071895425 Hits@10 0.7973856209150327 MRR 0.4160893304886029 rank 2 total_num 152 205\n",
      "checkcorrect (2096, 346, 2330) (2096, 346, 2330) real score -0.10187939144670963 Hits@1 0.24675324675324675 Hits@3 0.4805194805194805 Hits@10 0.7922077922077922 MRR 0.4135969196705998 rank 30 total_num 153 205\n",
      "checkcorrect (1776, 110, 2173) (1776, 110, 2173) real score 0.42712378203868867 Hits@1 0.24516129032258063 Hits@3 0.4774193548387097 Hits@10 0.7870967741935484 MRR 0.4112511330920798 rank 19 total_num 154 205\n",
      "checkcorrect (1744, 160, 2150) (1744, 160, 2150) real score 0.847230215370655 Hits@1 0.25 Hits@3 0.4807692307692308 Hits@10 0.7884615384615384 MRR 0.41502516429020747 rank 0 total_num 155 205\n",
      "checkcorrect (1945, 246, 2311) (1945, 246, 2311) real score 0.2708748564124107 Hits@1 0.2484076433121019 Hits@3 0.47770700636942676 Hits@10 0.7898089171974523 MRR 0.41329161001356374 rank 6 total_num 156 205\n",
      "checkcorrect (2454, 184, 1667) (2454, 184, 1667) real score 0.7057147398591042 Hits@1 0.2468354430379747 Hits@3 0.47468354430379744 Hits@10 0.7911392405063291 MRR 0.4115799994619408 rank 6 total_num 157 205\n",
      "checkcorrect (1816, 20, 1744) (1816, 20, 1744) real score 0.8710708111524582 Hits@1 0.24528301886792453 Hits@3 0.4779874213836478 Hits@10 0.7924528301886793 MRR 0.4110878820649055 rank 2 total_num 158 205\n",
      "checkcorrect (2204, 18, 2513) (2204, 18, 2513) real score 0.8142938628792763 Hits@1 0.24375 Hits@3 0.475 Hits@10 0.79375 MRR 0.40929983280199983 rank 7 total_num 159 205\n",
      "checkcorrect (2634, 56, 1629) (2634, 56, 1629) real score 0.4907039165496826 Hits@1 0.2422360248447205 Hits@3 0.4720496894409938 Hits@10 0.7950310559006211 MRR 0.40737871582807433 rank 9 total_num 160 205\n",
      "checkcorrect (1695, 242, 1829) (1695, 242, 1829) real score 0.6464462831616402 Hits@1 0.24074074074074073 Hits@3 0.4691358024691358 Hits@10 0.7962962962962963 MRR 0.40574586661220435 rank 6 total_num 161 205\n",
      "checkcorrect (1624, 92, 2597) (1624, 92, 2597) real score 0.2778506025671959 Hits@1 0.2392638036809816 Hits@3 0.4662576687116564 Hits@10 0.7975460122699386 MRR 0.4040234993323749 rank 7 total_num 162 205\n",
      "checkcorrect (1737, 262, 2208) (1737, 262, 2208) real score 0.8356633871793747 Hits@1 0.23780487804878048 Hits@3 0.4634146341463415 Hits@10 0.7987804878048781 MRR 0.4024310215489893 rank 6 total_num 163 205\n",
      "checkcorrect (2611, 346, 1605) (2611, 346, 1605) real score -0.11662925332784653 Hits@1 0.23636363636363636 Hits@3 0.46060606060606063 Hits@10 0.793939393939394 MRR 0.40015584582461383 rank 36 total_num 164 205\n",
      "checkcorrect (2088, 272, 1937) (2088, 272, 1937) real score 0.8208083182573318 Hits@1 0.23493975903614459 Hits@3 0.463855421686747 Hits@10 0.7951807228915663 MRR 0.3997533005686422 rank 2 total_num 165 205\n",
      "checkcorrect (2420, 76, 1776) (2420, 76, 1776) real score 0.5173337951302528 Hits@1 0.23353293413173654 Hits@3 0.46107784431137727 Hits@10 0.7964071856287425 MRR 0.3988565742179318 rank 3 total_num 166 205\n",
      "checkcorrect (1631, 6, 2248) (1631, 6, 2248) real score 0.9780790269374848 Hits@1 0.23809523809523808 Hits@3 0.4642857142857143 Hits@10 0.7976190476190477 MRR 0.402434808895206 rank 0 total_num 167 205\n",
      "checkcorrect (1788, 8, 1609) (1788, 8, 1609) real score 0.9400694221258163 Hits@1 0.23668639053254437 Hits@3 0.46745562130177515 Hits@10 0.7988165680473372 MRR 0.40202592442442564 rank 2 total_num 168 205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkcorrect (2248, 130, 1689) (2248, 130, 1689) real score 0.9568297564983368 Hits@1 0.23529411764705882 Hits@3 0.4647058823529412 Hits@10 0.8 MRR 0.40113165428075254 rank 3 total_num 169 205\n",
      "checkcorrect (1921, 144, 2060) (1921, 144, 2060) real score 0.0 Hits@1 0.23391812865497075 Hits@3 0.4619883040935672 Hits@10 0.7953216374269005 MRR 0.39917571868067014 rank 14 total_num 170 205\n",
      "checkcorrect (1692, 18, 1862) (1692, 18, 1862) real score 0.9173686176538467 Hits@1 0.23255813953488372 Hits@3 0.46511627906976744 Hits@10 0.7965116279069767 MRR 0.39879291411469725 rank 2 total_num 171 205\n",
      "checkcorrect (1595, 34, 2436) (1595, 34, 2436) real score 0.9026184022426605 Hits@1 0.23121387283236994 Hits@3 0.4682080924855491 Hits@10 0.7976878612716763 MRR 0.3993779261718377 rank 1 total_num 172 205\n",
      "checkcorrect (1606, 116, 1778) (1606, 116, 1778) real score 0.10797882992774249 Hits@1 0.22988505747126436 Hits@3 0.46551724137931033 Hits@10 0.7988505747126436 MRR 0.3982320760214249 rank 4 total_num 173 205\n",
      "checkcorrect (2622, 50, 2463) (2622, 50, 2463) real score 0.786939799785614 Hits@1 0.22857142857142856 Hits@3 0.46285714285714286 Hits@10 0.8 MRR 0.3967727906890575 rank 6 total_num 174 205\n",
      "checkcorrect (2144, 26, 2053) (2144, 26, 2053) real score 0.9511179447174072 Hits@1 0.22727272727272727 Hits@3 0.4659090909090909 Hits@10 0.8011363636363636 MRR 0.3973593089237788 rank 1 total_num 175 205\n",
      "checkcorrect (1683, 18, 1827) (1683, 18, 1827) real score 0.9731968015432357 Hits@1 0.23163841807909605 Hits@3 0.4689265536723164 Hits@10 0.8022598870056498 MRR 0.4007640585908761 rank 0 total_num 176 205\n",
      "checkcorrect (1948, 126, 2332) (1948, 126, 2332) real score 0.7878810465335846 Hits@1 0.2303370786516854 Hits@3 0.46629213483146065 Hits@10 0.8033707865168539 MRR 0.39921482230665767 rank 7 total_num 177 205\n",
      "checkcorrect (2510, 212, 1796) (2510, 212, 1796) real score 0.8623036950826645 Hits@1 0.22905027932960895 Hits@3 0.4692737430167598 Hits@10 0.8044692737430168 MRR 0.39977786799209536 rank 1 total_num 178 205\n",
      "checkcorrect (1648, 216, 1778) (1648, 216, 1778) real score -0.0010887262411415563 Hits@1 0.22777777777777777 Hits@3 0.4666666666666667 Hits@10 0.8 MRR 0.39806193034163423 rank 10 total_num 179 205\n",
      "checkcorrect (2486, 46, 1707) (2486, 46, 1707) real score 0.10480783991515637 Hits@1 0.2265193370165746 Hits@3 0.46408839779005523 Hits@10 0.7955801104972375 MRR 0.39615347536155543 rank 18 total_num 180 205\n",
      "checkcorrect (1667, 74, 1681) (1667, 74, 1681) real score 0.8568529665470124 Hits@1 0.22527472527472528 Hits@3 0.46153846153846156 Hits@10 0.7967032967032966 MRR 0.39458730852501456 rank 8 total_num 181 205\n",
      "checkcorrect (1766, 126, 2093) (1766, 126, 2093) real score 0.800000113248825 Hits@1 0.22404371584699453 Hits@3 0.45901639344262296 Hits@10 0.7978142076502732 MRR 0.39334184053671756 rank 5 total_num 182 205\n",
      "checkcorrect (2676, 20, 1744) (2676, 20, 1744) real score 0.49161592721939085 Hits@1 0.22282608695652173 Hits@3 0.45652173913043476 Hits@10 0.7989130434782609 MRR 0.3921099102439456 rank 5 total_num 183 205\n",
      "checkcorrect (1639, 170, 2109) (1639, 170, 2109) real score 0.6315498802810908 Hits@1 0.22162162162162163 Hits@3 0.4540540540540541 Hits@10 0.8 MRR 0.39089129811650086 rank 5 total_num 184 205\n",
      "checkcorrect (1708, 26, 1702) (1708, 26, 1702) real score 0.971045857667923 Hits@1 0.22580645161290322 Hits@3 0.45698924731182794 Hits@10 0.8010752688172043 MRR 0.3941660760836165 rank 0 total_num 185 205\n",
      "checkcorrect (2183, 176, 1756) (2183, 176, 1756) real score 0.23707272773608562 Hits@1 0.22459893048128343 Hits@3 0.45454545454545453 Hits@10 0.8021390374331551 MRR 0.39272668530242066 rank 7 total_num 186 205\n",
      "checkcorrect (1734, 50, 1831) (1734, 50, 1831) real score 0.9048897087574005 Hits@1 0.22340425531914893 Hits@3 0.4574468085106383 Hits@10 0.8031914893617021 MRR 0.39241076321747864 rank 2 total_num 187 205\n",
      "checkcorrect (2182, 26, 1622) (2182, 26, 1622) real score 0.9644696861505508 Hits@1 0.2222222222222222 Hits@3 0.4603174603174603 Hits@10 0.8042328042328042 MRR 0.3929800184385502 rank 1 total_num 188 205\n",
      "checkcorrect (1953, 310, 1795) (1953, 310, 1795) real score 0.44578028842806816 Hits@1 0.22105263157894736 Hits@3 0.4631578947368421 Hits@10 0.8052631578947368 MRR 0.39354328149939993 rank 1 total_num 189 205\n",
      "checkcorrect (1747, 94, 2466) (1747, 94, 2466) real score 0.7531783156096935 Hits@1 0.2198952879581152 Hits@3 0.4607329842931937 Hits@10 0.806282722513089 MRR 0.3920064056800313 rank 9 total_num 190 205\n",
      "checkcorrect (1731, 8, 1609) (1731, 8, 1609) real score 0.49754653573036195 Hits@1 0.22395833333333334 Hits@3 0.4635416666666667 Hits@10 0.8072916666666666 MRR 0.39517303898378114 rank 0 total_num 191 205\n",
      "checkcorrect (1649, 8, 1918) (1649, 8, 1918) real score 0.9191915065050125 Hits@1 0.22279792746113988 Hits@3 0.46632124352331605 Hits@10 0.8082901554404145 MRR 0.39485262600113635 rank 2 total_num 192 205\n",
      "checkcorrect (2513, 18, 2204) (2513, 18, 2204) real score 0.8296326518058776 Hits@1 0.22164948453608246 Hits@3 0.4639175257731959 Hits@10 0.8092783505154639 MRR 0.39355368021173426 rank 6 total_num 193 205\n",
      "checkcorrect (1639, 170, 1718) (1639, 170, 1718) real score 0.6609045971184969 Hits@1 0.2205128205128205 Hits@3 0.46153846153846156 Hits@10 0.8102564102564103 MRR 0.3928175074926997 rank 3 total_num 194 205\n",
      "checkcorrect (1703, 18, 1702) (1703, 18, 1702) real score 0.940554428100586 Hits@1 0.2193877551020408 Hits@3 0.4642857142857143 Hits@10 0.8112244897959183 MRR 0.3933643569442676 rank 1 total_num 195 205\n",
      "checkcorrect (1794, 46, 1964) (1794, 46, 1964) real score 0.9059344708919526 Hits@1 0.2182741116751269 Hits@3 0.4619289340101523 Hits@10 0.8121827411167513 MRR 0.39200210132526114 rank 7 total_num 196 205\n",
      "checkcorrect (2514, 310, 1795) (2514, 310, 1795) real score 0.42920891270041467 Hits@1 0.21717171717171718 Hits@3 0.46464646464646464 Hits@10 0.8131313131313131 MRR 0.391705794416211 rank 2 total_num 197 205\n",
      "checkcorrect (1622, 18, 1683) (1622, 18, 1683) real score 0.965640640258789 Hits@1 0.22110552763819097 Hits@3 0.46733668341708545 Hits@10 0.8140703517587939 MRR 0.3947625492181396 rank 0 total_num 198 205\n",
      "checkcorrect (2021, 42, 1672) (2021, 42, 1672) real score 0.9873575508594513 Hits@1 0.225 Hits@3 0.47 Hits@10 0.815 MRR 0.3977887364720489 rank 0 total_num 199 205\n",
      "checkcorrect (1872, 50, 1964) (1872, 50, 1964) real score 0.9508087009191513 Hits@1 0.22388059701492538 Hits@3 0.472636815920398 Hits@10 0.8159203980099502 MRR 0.3982972502209442 rank 1 total_num 200 205\n",
      "checkcorrect (2229, 18, 2121) (2229, 18, 2121) real score 0.896582728624344 Hits@1 0.22277227722772278 Hits@3 0.47029702970297027 Hits@10 0.8168316831683168 MRR 0.3975631054178702 rank 3 total_num 201 205\n",
      "checkcorrect (1772, 76, 2416) (1772, 76, 2416) real score 0.7405510500073433 Hits@1 0.22167487684729065 Hits@3 0.46798029556650245 Hits@10 0.8177339901477833 MRR 0.3961520118498566 rank 8 total_num 202 205\n",
      "checkcorrect (2622, 84, 1670) (2622, 84, 1670) real score 0.8801556080579758 Hits@1 0.22058823529411764 Hits@3 0.47058823529411764 Hits@10 0.8186274509803921 MRR 0.3966610706152985 rank 1 total_num 203 205\n",
      "checkcorrect (2143, 50, 2389) (2143, 50, 2389) real score 0.9333147883415223 Hits@1 0.22439024390243903 Hits@3 0.47317073170731705 Hits@10 0.8195121951219512 MRR 0.3996041873440044 rank 0 total_num 204 205\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "#obtain the Hits@N for entity prediction##############\n",
    "\n",
    "#we select all the triples in the inductive test set\n",
    "selected = list(data_ind_test)\n",
    "\n",
    "###Hit at 1#############################\n",
    "#generate the negative samples by randomly replace relation with all the other relaiton\n",
    "Hits_at_1 = 0\n",
    "Hits_at_3 = 0\n",
    "Hits_at_10 = 0\n",
    "MRR_raw = 0.\n",
    "\n",
    "for i in range(len(selected)):\n",
    "    \n",
    "    triple_list = list()\n",
    "    \n",
    "    #score the true triple\n",
    "    s_pos, r_pos, t_pos = selected[i][0], selected[i][1], selected[i][2]\n",
    "\n",
    "    path_score = path_based_triple_scoring(s_pos, r_pos, t_pos, lower_bound, upper_bound_path, one_hop_ind, id2relation, model)\n",
    "\n",
    "    subg_score = subgraph_triple_scoring(s_pos, r_pos, t_pos, lower_bound, upper_bound_subg, one_hop_ind, id2relation, model_2)\n",
    "    \n",
    "    ave_score = (path_score + subg_score)/float(2)\n",
    "    \n",
    "    triple_list.append([(s_pos, r_pos, t_pos), ave_score])\n",
    "    \n",
    "    #generate the 50 random samples\n",
    "    for sub_i in range(50):\n",
    "        \n",
    "        #decide to replace the head or tail entity\n",
    "        number_0 = random.uniform(0, 1)\n",
    "\n",
    "        if number_0 < 0.5: #replace head entity\n",
    "            \n",
    "            s_neg = random.choice(list(new_ent_set))\n",
    "            \n",
    "            while ((s_neg, r_pos, t_pos) in data_test) or (\n",
    "                   (s_neg, r_pos, t_pos) in data_valid) or (\n",
    "                   (s_neg, r_pos, t_pos) in data) or (\n",
    "                   (s_neg, r_pos, t_pos) in data_ind) or (\n",
    "                   (s_neg, r_pos, t_pos) in data_ind_valid) or (\n",
    "                   (s_neg, r_pos, t_pos) in data_ind_test):\n",
    "\n",
    "                s_neg = random.choice(list(new_ent_set))\n",
    "            \n",
    "            path_score = path_based_triple_scoring(s_neg, r_pos, t_pos, lower_bound, upper_bound_path, one_hop_ind, id2relation, model)\n",
    "\n",
    "            subg_score = subgraph_triple_scoring(s_neg, r_pos, t_pos, lower_bound, upper_bound_subg, one_hop_ind, id2relation, model_2)\n",
    "\n",
    "            ave_score = (path_score + subg_score)/float(2)\n",
    "\n",
    "            triple_list.append([(s_neg, r_pos, t_pos), ave_score])\n",
    "            \n",
    "        else: #replace tail entity\n",
    "\n",
    "            t_neg = random.choice(list(new_ent_set))\n",
    "            \n",
    "            #filter out the existing triples\n",
    "            while ((s_pos, r_pos, t_neg) in data_test) or (\n",
    "                   (s_pos, r_pos, t_neg) in data_valid) or (\n",
    "                   (s_pos, r_pos, t_neg) in data) or (\n",
    "                   (s_pos, r_pos, t_neg) in data_ind) or (\n",
    "                   (s_pos, r_pos, t_neg) in data_ind_valid) or (\n",
    "                   (s_pos, r_pos, t_neg) in data_ind_test):\n",
    "\n",
    "                t_neg = random.choice(list(new_ent_set))\n",
    "            \n",
    "            path_score = path_based_triple_scoring(s_pos, r_pos, t_neg, lower_bound, upper_bound_path, one_hop_ind, id2relation, model)\n",
    "\n",
    "            subg_score = subgraph_triple_scoring(s_pos, r_pos, t_neg, lower_bound, upper_bound_subg, one_hop_ind, id2relation, model_2)\n",
    "\n",
    "            ave_score = (path_score + subg_score)/float(2)\n",
    "\n",
    "            triple_list.append([(s_pos, r_pos, t_neg), ave_score])\n",
    "            \n",
    "    #random shuffle!\n",
    "    random.shuffle(triple_list)\n",
    "    \n",
    "    #sort\n",
    "    sorted_list = sorted(triple_list, key = lambda x: x[-1], reverse=True)\n",
    "    \n",
    "    p = 0\n",
    "    \n",
    "    while p < len(sorted_list) and sorted_list[p][0] != (s_pos, r_pos, t_pos):\n",
    "            \n",
    "        p += 1\n",
    "    \n",
    "    if p == 0:\n",
    "        \n",
    "        Hits_at_1 += 1\n",
    "        \n",
    "    if p < 3:\n",
    "        \n",
    "        Hits_at_3 += 1\n",
    "        \n",
    "    if p < 10:\n",
    "        \n",
    "        Hits_at_10 += 1\n",
    "        \n",
    "    MRR_raw += 1./float(p + 1.) \n",
    "        \n",
    "    print('checkcorrect', (s_pos, r_pos, t_pos), sorted_list[p][0],\n",
    "          'real score', sorted_list[p][-1],\n",
    "          'Hits@1', Hits_at_1/(i+1),\n",
    "          'Hits@3', Hits_at_3/(i+1),\n",
    "          'Hits@10', Hits_at_10/(i+1),\n",
    "          'MRR', MRR_raw/(i+1),\n",
    "          'rank', p,\n",
    "          'total_num', i, len(selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dcec14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50b1482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c23edb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10052e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee5bc67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44b53285",
   "metadata": {},
   "source": [
    "#### Fine tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "864a4f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to build the big batche for path-based training\n",
    "def build_big_batches_path(lower_bd, upper_bd, data, one_hop, s_t_r,\n",
    "                      x_p_list, x_r_list, y_list,\n",
    "                      relation2id, entity2id, id2relation, id2entity):\n",
    "    \n",
    "    #the set of all relation IDs\n",
    "    relation_id_set = set()\n",
    "    \n",
    "    #the set of all initial relations\n",
    "    ini_r_id_set = set()\n",
    "    \n",
    "    for i in range(len(id2relation)):\n",
    "        \n",
    "        if i not in id2relation:\n",
    "            raise ValueError('error when generaing id2relation')\n",
    "        \n",
    "        relation_id_set.add(i)\n",
    "        \n",
    "        if i % 2 == 0: #initial relation id is always an even number\n",
    "            ini_r_id_set.add(i)\n",
    "    \n",
    "    num_r = len(id2relation)\n",
    "    num_ini_r = len(ini_r_id_set)\n",
    "    \n",
    "    if num_ini_r != int(num_r/2):\n",
    "        raise ValueError('error when generating id2relation')\n",
    "    \n",
    "    #in case not all entities in entity2id are in one_hop, \n",
    "    #so we need to find out who are indeed in\n",
    "    existing_ids = set()\n",
    "    \n",
    "    for s_1 in one_hop:\n",
    "        existing_ids.add(s_1)\n",
    "        \n",
    "    existing_ids = list(existing_ids)\n",
    "    random.shuffle(existing_ids)\n",
    "    \n",
    "    count = 0\n",
    "    for s in existing_ids:\n",
    "        \n",
    "        #impliment the path finding algorithm to find paths between s and t\n",
    "        result, length_dict = Class_2.obtain_paths('direct_neighbour', s, 'nb', lower_bd, upper_bd, one_hop)\n",
    "        \n",
    "        for iteration in range(10):\n",
    "\n",
    "            #proceed only if at least three paths are between s and t\n",
    "            for t in result:\n",
    "\n",
    "                if len(s_t_r[(s,t)]) == 0:\n",
    "\n",
    "                    raise ValueError(s,t,id2entity[s], id2entity[t])\n",
    "\n",
    "                #we are only interested in forward link in relation prediciton\n",
    "                ini_r_list = list()\n",
    "\n",
    "                #obtain initial relations between s and t\n",
    "                for r in s_t_r[(s,t)]:\n",
    "                    if r % 2 == 0:#initial relation id is always an even number\n",
    "                        ini_r_list.append(r)\n",
    "\n",
    "                #if there exist more than three paths between s and t, \n",
    "                #and inital connection between s and t exists,\n",
    "                #and not every r in the relation dictionary exists between s and t (although this is rare)\n",
    "                #we then proceed\n",
    "                if len(result[t]) >= 3 and len(ini_r_list) > 0 and len(ini_r_list) < int(num_ini_r):\n",
    "\n",
    "                    #obtain the list form of all the paths from s to t\n",
    "                    temp_path_list = list(result[t])\n",
    "\n",
    "                    temp_pair = random.sample(temp_path_list, 3)\n",
    "\n",
    "                    path_1, path_2, path_3 = temp_pair[0], temp_pair[1], temp_pair[2]\n",
    "\n",
    "                    #####positive#####################\n",
    "                    #append the paths: note that we add the space holder id at the end of the shorter path\n",
    "                    x_p_list['1'].append(list(path_1) + [num_r]*abs(len(path_1)-upper_bd))\n",
    "                    x_p_list['2'].append(list(path_2) + [num_r]*abs(len(path_2)-upper_bd))\n",
    "                    x_p_list['3'].append(list(path_3) + [num_r]*abs(len(path_3)-upper_bd))\n",
    "\n",
    "                    #append relation\n",
    "                    r = random.choice(ini_r_list)\n",
    "                    x_r_list.append([r])\n",
    "                    y_list.append(1.)\n",
    "\n",
    "                    #####negative#####################\n",
    "                    #append the paths: note that we add the space holder id at the end\n",
    "                    #of the shorter path\n",
    "                    x_p_list['1'].append(list(path_1) + [num_r]*abs(len(path_1)-upper_bd))\n",
    "                    x_p_list['2'].append(list(path_2) + [num_r]*abs(len(path_2)-upper_bd))\n",
    "                    x_p_list['3'].append(list(path_3) + [num_r]*abs(len(path_3)-upper_bd))\n",
    "\n",
    "                    #append relation\n",
    "                    neg_r_list = list(ini_r_id_set.difference(set(ini_r_list)))\n",
    "                    r_ran = random.choice(neg_r_list)\n",
    "                    x_r_list.append([r_ran])\n",
    "                    y_list.append(0.)\n",
    "        \n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print('generating big-batches for path-based model', count, len(existing_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc82bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again, it is too slow to run the path-finding algorithm again and again on the complete FB15K-237\n",
    "#Instead, we will find the subgraph for each entity once.\n",
    "#then in the subgraph based training, the subgraphs are stored and used for multiple times\n",
    "def store_subgraph_dicts(lower_bd, upper_bd, data, one_hop, s_t_r,\n",
    "                         relation2id, entity2id, id2relation, id2entity):\n",
    "    \n",
    "    #the set of all relation IDs\n",
    "    relation_id_set = set()\n",
    "    \n",
    "    for i in range(len(id2relation)):\n",
    "        \n",
    "        if i not in id2relation:\n",
    "            raise ValueError('error when generaing id2relation')\n",
    "        \n",
    "        relation_id_set.add(i)\n",
    "    \n",
    "    num_r = len(id2relation)\n",
    "    \n",
    "    #in case not all entities in entity2id are in one_hop, \n",
    "    #so we need to find out who are indeed in\n",
    "    existing_ids = set()\n",
    "    \n",
    "    for s_1 in one_hop:\n",
    "        existing_ids.add(s_1)\n",
    "    \n",
    "    #the ids to start path finding\n",
    "    existing_ids = list(existing_ids)\n",
    "    random.shuffle(existing_ids)\n",
    "    \n",
    "    #Dict stores the subgraph for each entity\n",
    "    Dict_1 = dict()\n",
    "    \n",
    "    count = 0\n",
    "    for s in existing_ids:\n",
    "        \n",
    "        path_set = set()\n",
    "            \n",
    "        result, length_dict = Class_2.obtain_paths('any_target', s, 'any', lower_bd, upper_bd, one_hop)\n",
    "\n",
    "        for t_ in result:\n",
    "            for path in result[t_]:\n",
    "                path_set.add(path)\n",
    "\n",
    "        del(result, length_dict)\n",
    "        \n",
    "        path_list = list(path_set)\n",
    "        \n",
    "        path_select = random.sample(path_list, min(len(path_list), 100))\n",
    "            \n",
    "        Dict_1[s] = deepcopy(path_select)\n",
    "        \n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print('generating and storing paths for the path-based model', count, len(existing_ids))\n",
    "        \n",
    "    return(Dict_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee3bf172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to build the big-batch for one-hope neighbor training\n",
    "def build_big_batches_subgraph(lower_bd, upper_bd, data, one_hop, s_t_r,\n",
    "                      x_s_list, x_t_list, x_r_list, y_list, Dict,\n",
    "                      relation2id, entity2id, id2relation, id2entity):\n",
    "    \n",
    "    #the set of all relation IDs\n",
    "    relation_id_set = set()\n",
    "    \n",
    "    #the set of all initial relations\n",
    "    ini_r_id_set = set()\n",
    "    \n",
    "    for i in range(len(id2relation)):\n",
    "        \n",
    "        if i not in id2relation:\n",
    "            raise ValueError('error when generaing id2relation')\n",
    "        \n",
    "        relation_id_set.add(i)\n",
    "        \n",
    "        if i % 2 == 0: #initial relation id is always an even number\n",
    "            ini_r_id_set.add(i)\n",
    "    \n",
    "    num_r = len(id2relation)\n",
    "    num_ini_r = len(ini_r_id_set)\n",
    "    \n",
    "    if num_ini_r != int(num_r/2):\n",
    "        raise ValueError('error when generating id2relation')\n",
    "        \n",
    "    #if an entity has at least three out-stretching paths, it is a qualified one\n",
    "    qualified = set()\n",
    "    for e in Dict:\n",
    "        if len(Dict[e]) >= 6:\n",
    "            qualified.add(e)\n",
    "    qualified = list(qualified)\n",
    "    \n",
    "    data = list(data)\n",
    "    \n",
    "    for iteration in range(10):\n",
    "\n",
    "        data = shuffle(data)\n",
    "\n",
    "        for i_0 in range(len(data)):\n",
    "\n",
    "            triple = data[i_0]\n",
    "\n",
    "            s, r, t = triple[0], triple[1], triple[2] #obtain entities and relation IDs\n",
    "\n",
    "            if s in qualified and t in qualified:\n",
    "\n",
    "                #obtain the path list for true entities\n",
    "                path_s, path_t = list(Dict[s]), list(Dict[t])\n",
    "\n",
    "                #####positive step###########\n",
    "                #randomly obtain three paths for true entities\n",
    "                temp_s = random.sample(path_s, 6)\n",
    "                temp_t = random.sample(path_t, 6)\n",
    "                s_p_1, s_p_2, s_p_3, s_p_4, s_p_5, s_p_6 = temp_s[0], temp_s[1], temp_s[2], temp_s[3], temp_s[4], temp_s[5]\n",
    "                t_p_1, t_p_2, t_p_3, t_p_4, t_p_5, t_p_6 = temp_t[0], temp_t[1], temp_t[2], temp_t[3], temp_t[4], temp_t[5]\n",
    "\n",
    "                #append the paths: note that we add the space holder id at the end of the shorter path\n",
    "                x_s_list['1'].append(list(s_p_1) + [num_r]*abs(len(s_p_1)-upper_bd))\n",
    "                x_s_list['2'].append(list(s_p_2) + [num_r]*abs(len(s_p_2)-upper_bd))\n",
    "                x_s_list['3'].append(list(s_p_3) + [num_r]*abs(len(s_p_3)-upper_bd))\n",
    "                x_s_list['4'].append(list(s_p_4) + [num_r]*abs(len(s_p_4)-upper_bd))\n",
    "                x_s_list['5'].append(list(s_p_5) + [num_r]*abs(len(s_p_5)-upper_bd))\n",
    "                x_s_list['6'].append(list(s_p_6) + [num_r]*abs(len(s_p_6)-upper_bd))\n",
    "\n",
    "                x_t_list['1'].append(list(t_p_1) + [num_r]*abs(len(t_p_1)-upper_bd))\n",
    "                x_t_list['2'].append(list(t_p_2) + [num_r]*abs(len(t_p_2)-upper_bd))\n",
    "                x_t_list['3'].append(list(t_p_3) + [num_r]*abs(len(t_p_3)-upper_bd))\n",
    "                x_t_list['4'].append(list(t_p_4) + [num_r]*abs(len(t_p_4)-upper_bd))\n",
    "                x_t_list['5'].append(list(t_p_5) + [num_r]*abs(len(t_p_5)-upper_bd))\n",
    "                x_t_list['6'].append(list(t_p_6) + [num_r]*abs(len(t_p_6)-upper_bd))\n",
    "\n",
    "                #append relation\n",
    "                x_r_list.append([r])\n",
    "                y_list.append(1.)\n",
    "\n",
    "                #####negative step for relation###########\n",
    "                #append the paths: note that we add the space holder id at the end of the shorter path\n",
    "                x_s_list['1'].append(list(s_p_1) + [num_r]*abs(len(s_p_1)-upper_bd))\n",
    "                x_s_list['2'].append(list(s_p_2) + [num_r]*abs(len(s_p_2)-upper_bd))\n",
    "                x_s_list['3'].append(list(s_p_3) + [num_r]*abs(len(s_p_3)-upper_bd))\n",
    "                x_s_list['4'].append(list(s_p_4) + [num_r]*abs(len(s_p_4)-upper_bd))\n",
    "                x_s_list['5'].append(list(s_p_5) + [num_r]*abs(len(s_p_5)-upper_bd))\n",
    "                x_s_list['6'].append(list(s_p_6) + [num_r]*abs(len(s_p_6)-upper_bd))\n",
    "\n",
    "                x_t_list['1'].append(list(t_p_1) + [num_r]*abs(len(t_p_1)-upper_bd))\n",
    "                x_t_list['2'].append(list(t_p_2) + [num_r]*abs(len(t_p_2)-upper_bd))\n",
    "                x_t_list['3'].append(list(t_p_3) + [num_r]*abs(len(t_p_3)-upper_bd))\n",
    "                x_t_list['4'].append(list(t_p_4) + [num_r]*abs(len(t_p_4)-upper_bd))\n",
    "                x_t_list['5'].append(list(t_p_5) + [num_r]*abs(len(t_p_5)-upper_bd))\n",
    "                x_t_list['6'].append(list(t_p_6) + [num_r]*abs(len(t_p_6)-upper_bd))\n",
    "\n",
    "                #append relation\n",
    "                neg_r_list = list(ini_r_id_set.difference({r}))\n",
    "                r_ran = random.choice(neg_r_list)\n",
    "                x_r_list.append([r_ran])\n",
    "                y_list.append(0.)\n",
    "                \n",
    "                ##############################################\n",
    "                ##############################################\n",
    "                #randomly choose two negative sampled entities\n",
    "                s_ran = random.choice(qualified)\n",
    "                t_ran = random.choice(qualified)\n",
    "\n",
    "                #obtain the path list for random entities\n",
    "                path_s_ran, path_t_ran = list(Dict[s_ran]), list(Dict[t_ran])\n",
    "                \n",
    "                #####positive step#################\n",
    "                #Again: randomly obtain three paths\n",
    "                temp_s = random.sample(path_s, 6)\n",
    "                temp_t = random.sample(path_t, 6)\n",
    "                s_p_1, s_p_2, s_p_3, s_p_4, s_p_5, s_p_6 = temp_s[0], temp_s[1], temp_s[2], temp_s[3], temp_s[4], temp_s[5]\n",
    "                t_p_1, t_p_2, t_p_3, t_p_4, t_p_5, t_p_6 = temp_t[0], temp_t[1], temp_t[2], temp_t[3], temp_t[4], temp_t[5]\n",
    "\n",
    "                #append the paths: note that we add the space holder id at the end of the shorter path\n",
    "                x_s_list['1'].append(list(s_p_1) + [num_r]*abs(len(s_p_1)-upper_bd))\n",
    "                x_s_list['2'].append(list(s_p_2) + [num_r]*abs(len(s_p_2)-upper_bd))\n",
    "                x_s_list['3'].append(list(s_p_3) + [num_r]*abs(len(s_p_3)-upper_bd))\n",
    "                x_s_list['4'].append(list(s_p_4) + [num_r]*abs(len(s_p_4)-upper_bd))\n",
    "                x_s_list['5'].append(list(s_p_5) + [num_r]*abs(len(s_p_5)-upper_bd))\n",
    "                x_s_list['6'].append(list(s_p_6) + [num_r]*abs(len(s_p_6)-upper_bd))\n",
    "\n",
    "                x_t_list['1'].append(list(t_p_1) + [num_r]*abs(len(t_p_1)-upper_bd))\n",
    "                x_t_list['2'].append(list(t_p_2) + [num_r]*abs(len(t_p_2)-upper_bd))\n",
    "                x_t_list['3'].append(list(t_p_3) + [num_r]*abs(len(t_p_3)-upper_bd))\n",
    "                x_t_list['4'].append(list(t_p_4) + [num_r]*abs(len(t_p_4)-upper_bd))\n",
    "                x_t_list['5'].append(list(t_p_5) + [num_r]*abs(len(t_p_5)-upper_bd))\n",
    "                x_t_list['6'].append(list(t_p_6) + [num_r]*abs(len(t_p_6)-upper_bd))\n",
    "\n",
    "                #append relation\n",
    "                x_r_list.append([r])\n",
    "                y_list.append(1.)\n",
    "\n",
    "                #####negative for source entity###########\n",
    "                #randomly obtain three paths\n",
    "                temp_s = random.sample(path_s_ran, 6)\n",
    "                s_p_1, s_p_2, s_p_3, s_p_4, s_p_5, s_p_6 = temp_s[0], temp_s[1], temp_s[2], temp_s[3], temp_s[4], temp_s[5]\n",
    "\n",
    "                #append the paths: note that we add the space holder id at the end of the shorter path\n",
    "                x_s_list['1'].append(list(s_p_1) + [num_r]*abs(len(s_p_1)-upper_bd))\n",
    "                x_s_list['2'].append(list(s_p_2) + [num_r]*abs(len(s_p_2)-upper_bd))\n",
    "                x_s_list['3'].append(list(s_p_3) + [num_r]*abs(len(s_p_3)-upper_bd))\n",
    "                x_s_list['4'].append(list(s_p_4) + [num_r]*abs(len(s_p_4)-upper_bd))\n",
    "                x_s_list['5'].append(list(s_p_5) + [num_r]*abs(len(s_p_5)-upper_bd))\n",
    "                x_s_list['6'].append(list(s_p_6) + [num_r]*abs(len(s_p_6)-upper_bd))\n",
    "\n",
    "                x_t_list['1'].append(list(t_p_1) + [num_r]*abs(len(t_p_1)-upper_bd))\n",
    "                x_t_list['2'].append(list(t_p_2) + [num_r]*abs(len(t_p_2)-upper_bd))\n",
    "                x_t_list['3'].append(list(t_p_3) + [num_r]*abs(len(t_p_3)-upper_bd))\n",
    "                x_t_list['4'].append(list(t_p_4) + [num_r]*abs(len(t_p_4)-upper_bd))\n",
    "                x_t_list['5'].append(list(t_p_5) + [num_r]*abs(len(t_p_5)-upper_bd))\n",
    "                x_t_list['6'].append(list(t_p_6) + [num_r]*abs(len(t_p_6)-upper_bd))\n",
    "\n",
    "                #append relation\n",
    "                x_r_list.append([r])\n",
    "                y_list.append(0.)\n",
    "\n",
    "                #####positive step###########\n",
    "                #Again: randomly obtain three paths\n",
    "                temp_s = random.sample(path_s, 6)\n",
    "                temp_t = random.sample(path_t, 6)\n",
    "                s_p_1, s_p_2, s_p_3, s_p_4, s_p_5, s_p_6 = temp_s[0], temp_s[1], temp_s[2], temp_s[3], temp_s[4], temp_s[5]\n",
    "                t_p_1, t_p_2, t_p_3, t_p_4, t_p_5, t_p_6 = temp_t[0], temp_t[1], temp_t[2], temp_t[3], temp_t[4], temp_t[5]\n",
    "\n",
    "                #append the paths: note that we add the space holder id at the end of the shorter path\n",
    "                x_s_list['1'].append(list(s_p_1) + [num_r]*abs(len(s_p_1)-upper_bd))\n",
    "                x_s_list['2'].append(list(s_p_2) + [num_r]*abs(len(s_p_2)-upper_bd))\n",
    "                x_s_list['3'].append(list(s_p_3) + [num_r]*abs(len(s_p_3)-upper_bd))\n",
    "                x_s_list['4'].append(list(s_p_4) + [num_r]*abs(len(s_p_4)-upper_bd))\n",
    "                x_s_list['5'].append(list(s_p_5) + [num_r]*abs(len(s_p_5)-upper_bd))\n",
    "                x_s_list['6'].append(list(s_p_6) + [num_r]*abs(len(s_p_6)-upper_bd))\n",
    "\n",
    "                x_t_list['1'].append(list(t_p_1) + [num_r]*abs(len(t_p_1)-upper_bd))\n",
    "                x_t_list['2'].append(list(t_p_2) + [num_r]*abs(len(t_p_2)-upper_bd))\n",
    "                x_t_list['3'].append(list(t_p_3) + [num_r]*abs(len(t_p_3)-upper_bd))\n",
    "                x_t_list['4'].append(list(t_p_4) + [num_r]*abs(len(t_p_4)-upper_bd))\n",
    "                x_t_list['5'].append(list(t_p_5) + [num_r]*abs(len(t_p_5)-upper_bd))\n",
    "                x_t_list['6'].append(list(t_p_6) + [num_r]*abs(len(t_p_6)-upper_bd))\n",
    "\n",
    "                #append relation\n",
    "                x_r_list.append([r])\n",
    "                y_list.append(1.)\n",
    "\n",
    "                #####negative for target entity###########\n",
    "                #randomly obtain three paths\n",
    "                temp_t = random.sample(path_t_ran, 6)\n",
    "                t_p_1, t_p_2, t_p_3, t_p_4, t_p_5, t_p_6 = temp_t[0], temp_t[1], temp_t[2], temp_t[3], temp_t[4], temp_t[5]\n",
    "\n",
    "                #append the paths: note that we add the space holder id at the end of the shorter path\n",
    "                x_s_list['1'].append(list(s_p_1) + [num_r]*abs(len(s_p_1)-upper_bd))\n",
    "                x_s_list['2'].append(list(s_p_2) + [num_r]*abs(len(s_p_2)-upper_bd))\n",
    "                x_s_list['3'].append(list(s_p_3) + [num_r]*abs(len(s_p_3)-upper_bd))\n",
    "                x_s_list['4'].append(list(s_p_4) + [num_r]*abs(len(s_p_4)-upper_bd))\n",
    "                x_s_list['5'].append(list(s_p_5) + [num_r]*abs(len(s_p_5)-upper_bd))\n",
    "                x_s_list['6'].append(list(s_p_6) + [num_r]*abs(len(s_p_6)-upper_bd))\n",
    "\n",
    "                x_t_list['1'].append(list(t_p_1) + [num_r]*abs(len(t_p_1)-upper_bd))\n",
    "                x_t_list['2'].append(list(t_p_2) + [num_r]*abs(len(t_p_2)-upper_bd))\n",
    "                x_t_list['3'].append(list(t_p_3) + [num_r]*abs(len(t_p_3)-upper_bd))\n",
    "                x_t_list['4'].append(list(t_p_4) + [num_r]*abs(len(t_p_4)-upper_bd))\n",
    "                x_t_list['5'].append(list(t_p_5) + [num_r]*abs(len(t_p_5)-upper_bd))\n",
    "                x_t_list['6'].append(list(t_p_6) + [num_r]*abs(len(t_p_6)-upper_bd))\n",
    "\n",
    "                #append relation\n",
    "                x_r_list.append([r])\n",
    "                y_list.append(0.)\n",
    "\n",
    "            if i_0 % 2000 == 0:\n",
    "                print('generating big-batches for subgraph-based model', i_0, len(data), iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45513dda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating big-batches for path-based model 100 225\n",
      "generating big-batches for path-based model 200 225\n",
      "Epoch 1/5\n",
      "313/313 [==============================] - 17s 39ms/step - loss: 0.1153 - binary_accuracy: 0.9695\n",
      "Epoch 2/5\n",
      "313/313 [==============================] - 12s 40ms/step - loss: 0.0843 - binary_accuracy: 0.9766\n",
      "Epoch 3/5\n",
      "313/313 [==============================] - 12s 39ms/step - loss: 0.0826 - binary_accuracy: 0.9770\n",
      "Epoch 4/5\n",
      "313/313 [==============================] - 12s 39ms/step - loss: 0.0809 - binary_accuracy: 0.9769\n",
      "Epoch 5/5\n",
      "313/313 [==============================] - 12s 40ms/step - loss: 0.0810 - binary_accuracy: 0.9766\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9acdf6db20>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###fine tune the path-based model\n",
    "lower_bd = lower_bound\n",
    "upper_bd = upper_bound_path\n",
    "batch_size = 32\n",
    "\n",
    "#define the training lists\n",
    "train_p_list, train_r_list, train_y_list = {'1': [], '2': [], '3': []}, list(), list()\n",
    "\n",
    "#######################################\n",
    "###build the big-batches###############      \n",
    "\n",
    "#fill in the training array list\n",
    "build_big_batches_path(lower_bd, upper_bd, data_ind, one_hop_ind, s_t_r_ind,\n",
    "                      train_p_list, train_r_list, train_y_list,\n",
    "                      relation2id, entity2id, id2relation, id2entity)   \n",
    "\n",
    "#######################################\n",
    "###do the training#####################\n",
    "\n",
    "#generate the input arrays\n",
    "x_train_1 = np.asarray(train_p_list['1'], dtype='int')\n",
    "x_train_2 = np.asarray(train_p_list['2'], dtype='int')\n",
    "x_train_3 = np.asarray(train_p_list['3'], dtype='int')\n",
    "x_train_r = np.asarray(train_r_list, dtype='int')\n",
    "y_train = np.asarray(train_y_list, dtype='int')\n",
    "\n",
    "model.fit([x_train_1, x_train_2, x_train_3, x_train_r], y_train,\n",
    "           batch_size=batch_size, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dcc46d08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating and storing paths for the path-based model 100 225\n",
      "generating and storing paths for the path-based model 200 225\n",
      "generating big-batches for subgraph-based model 0 833 0\n",
      "generating big-batches for subgraph-based model 0 833 1\n",
      "generating big-batches for subgraph-based model 0 833 2\n",
      "generating big-batches for subgraph-based model 0 833 3\n",
      "generating big-batches for subgraph-based model 0 833 4\n",
      "generating big-batches for subgraph-based model 0 833 5\n",
      "generating big-batches for subgraph-based model 0 833 6\n",
      "generating big-batches for subgraph-based model 0 833 7\n",
      "generating big-batches for subgraph-based model 0 833 8\n",
      "generating big-batches for subgraph-based model 0 833 9\n",
      "Epoch 1/5\n",
      "1562/1562 [==============================] - 124s 64ms/step - loss: 0.3678 - binary_accuracy: 0.8374\n",
      "Epoch 2/5\n",
      "1562/1562 [==============================] - 98s 63ms/step - loss: 0.3518 - binary_accuracy: 0.8444\n",
      "Epoch 3/5\n",
      "1562/1562 [==============================] - 101s 65ms/step - loss: 0.3463 - binary_accuracy: 0.8468\n",
      "Epoch 4/5\n",
      "1562/1562 [==============================] - 98s 63ms/step - loss: 0.3416 - binary_accuracy: 0.8485\n",
      "Epoch 5/5\n",
      "1562/1562 [==============================] - 98s 63ms/step - loss: 0.3375 - binary_accuracy: 0.8503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9a299a7550>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###fine tune the subgraph model\n",
    "lower_bd = lower_bound\n",
    "upper_bd = upper_bound_subg\n",
    "batch_size = 32\n",
    "\n",
    "Dict_train_ind = store_subgraph_dicts(lower_bd, upper_bd, data_ind, one_hop_ind, s_t_r_ind,\n",
    "                         relation2id, entity2id, id2relation, id2entity)\n",
    "\n",
    "#define the training lists\n",
    "train_s_list, train_t_list, train_r_list, train_y_list = {'1': [], '2': [], '3': [], '4': [], '5': [], '6': []}, {'1': [], '2': [], '3': [], '4': [], '5': [], '6': []}, list(), list()\n",
    "\n",
    "#######################################\n",
    "###build the big-batches###############      \n",
    "\n",
    "#fill in the training array list\n",
    "build_big_batches_subgraph(lower_bd, upper_bd, data_ind, one_hop_ind, s_t_r_ind,\n",
    "                      train_s_list, train_t_list, train_r_list, train_y_list, Dict_train_ind,\n",
    "                      relation2id, entity2id, id2relation, id2entity)\n",
    "\n",
    "#######################################\n",
    "###do the training#####################\n",
    "\n",
    "#generate the input arrays\n",
    "x_train_s_1 = np.asarray(train_s_list['1'], dtype='int')\n",
    "x_train_s_2 = np.asarray(train_s_list['2'], dtype='int')\n",
    "x_train_s_3 = np.asarray(train_s_list['3'], dtype='int')\n",
    "x_train_s_4 = np.asarray(train_s_list['4'], dtype='int')\n",
    "x_train_s_5 = np.asarray(train_s_list['5'], dtype='int')\n",
    "x_train_s_6 = np.asarray(train_s_list['6'], dtype='int')\n",
    "\n",
    "x_train_t_1 = np.asarray(train_t_list['1'], dtype='int')\n",
    "x_train_t_2 = np.asarray(train_t_list['2'], dtype='int')\n",
    "x_train_t_3 = np.asarray(train_t_list['3'], dtype='int')\n",
    "x_train_t_4 = np.asarray(train_t_list['4'], dtype='int')\n",
    "x_train_t_5 = np.asarray(train_t_list['5'], dtype='int')\n",
    "x_train_t_6 = np.asarray(train_t_list['6'], dtype='int')\n",
    "\n",
    "x_train_r = np.asarray(train_r_list, dtype='int')\n",
    "y_train = np.asarray(train_y_list, dtype='int')\n",
    "\n",
    "model_2.fit([x_train_s_1, x_train_s_2, x_train_s_3, x_train_s_4, x_train_s_5, x_train_s_6,\n",
    "             x_train_t_1, x_train_t_2, x_train_t_3, x_train_t_4, x_train_t_5, x_train_t_6,\n",
    "             x_train_r], y_train,\n",
    "             batch_size=batch_size, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f6be50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "#obtain the Hits@N for relation prediction##############\n",
    "\n",
    "#we select all the triples in the inductive test set\n",
    "selected = list(data_ind_test)\n",
    "\n",
    "###Hit at 1#############################\n",
    "#generate the negative samples by randomly replace relation with all the other relaiton\n",
    "Hits_at_1 = 0\n",
    "Hits_at_3 = 0\n",
    "Hits_at_10 = 0\n",
    "MRR_raw = 0.\n",
    "\n",
    "for i in range(len(selected)):\n",
    "    \n",
    "    s_true, r_true, t_true = selected[i][0], selected[i][1], selected[i][2]\n",
    "    \n",
    "    #run the path-based scoring\n",
    "    score_dict_path = path_based_relation_scoring(s_true, t_true, lower_bound, upper_bound_path, one_hop_ind, id2relation, model)\n",
    "    \n",
    "    #run the one-hop neighbour based scoring\n",
    "    score_dict_subg = subgraph_relation_scoring(s_true, t_true, lower_bound, upper_bound_subg, one_hop_ind, id2relation, model_2)\n",
    "    \n",
    "    #final score dict\n",
    "    score_dict = defaultdict(float)\n",
    "    \n",
    "    for r in score_dict_path:\n",
    "        score_dict[r] += score_dict_path[r]\n",
    "    for r in score_dict_subg:\n",
    "        score_dict[r] += score_dict_subg[r]\n",
    "    \n",
    "    #[... [score, r], ...]\n",
    "    temp_list = list()\n",
    "    \n",
    "    for r in id2relation:\n",
    "        \n",
    "        #again, we only care about initial relation prediciton\n",
    "        if r % 2 == 0:\n",
    "        \n",
    "            if r in score_dict:\n",
    "\n",
    "                temp_list.append([score_dict[r], r])\n",
    "\n",
    "            else:\n",
    "\n",
    "                temp_list.append([0.0, r])\n",
    "        \n",
    "    sorted_list = sorted(temp_list, key = lambda x: x[0], reverse=True)\n",
    "    \n",
    "    p = 0\n",
    "    exist_tri = 0\n",
    "    \n",
    "    while p < len(sorted_list) and sorted_list[p][1] != r_true:\n",
    "        \n",
    "        #moreover, we want to remove existing triples\n",
    "        if ((s_true, sorted_list[p][1], t_true) in data_test) or (\n",
    "            (s_true, sorted_list[p][1], t_true) in data_valid) or (\n",
    "            (s_true, sorted_list[p][1], t_true) in data) or (\n",
    "            (s_true, sorted_list[p][1], t_true) in data_ind) or (\n",
    "            (s_true, sorted_list[p][1], t_true) in data_ind_valid) or (\n",
    "            (s_true, sorted_list[p][1], t_true) in data_ind_test):\n",
    "            \n",
    "            exist_tri += 1\n",
    "            \n",
    "        p += 1\n",
    "    \n",
    "    if p - exist_tri == 0:\n",
    "        \n",
    "        Hits_at_1 += 1\n",
    "        \n",
    "    if p - exist_tri < 3:\n",
    "        \n",
    "        Hits_at_3 += 1\n",
    "        \n",
    "    if p - exist_tri < 10:\n",
    "        \n",
    "        Hits_at_10 += 1\n",
    "        \n",
    "    MRR_raw += 1./float(p - exist_tri + 1.) \n",
    "        \n",
    "    print('checkcorrect', r_true, sorted_list[p][1],\n",
    "          'real score', sorted_list[p][0],\n",
    "          'Hits@1', Hits_at_1/(i+1),\n",
    "          'Hits@3', Hits_at_3/(i+1),\n",
    "          'Hits@10', Hits_at_10/(i+1),\n",
    "          'MRR', MRR_raw/(i+1),\n",
    "          'cur_rank', p - exist_tri,\n",
    "          'abs_cur_rank', p,\n",
    "          'total_num', i, len(selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550a7ab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "##obtain the AUC-PR for the test triples###\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.metrics import auc, plot_precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#we select all the triples in the inductive test set\n",
    "pos_triples = list(data_ind_test)\n",
    "\n",
    "#we build the negative samples by randomly replace head or tail entity in the triple.\n",
    "neg_triples = list()\n",
    "\n",
    "for i in range(len(pos_triples)):\n",
    "    \n",
    "    s_pos, r_pos, t_pos = pos_triples[i][0], pos_triples[i][1], pos_triples[i][2]\n",
    "    \n",
    "    #decide to replace the head or tail entity\n",
    "    number_0 = random.uniform(0, 1)\n",
    "    \n",
    "    if number_0 < 0.5: #replace head entity\n",
    "        s_neg = random.choice(list(new_ent_set))\n",
    "        \n",
    "        #filter out the existing triples\n",
    "        while ((s_neg, r_pos, t_pos) in data_test) or (\n",
    "               (s_neg, r_pos, t_pos) in data_valid) or (\n",
    "               (s_neg, r_pos, t_pos) in data) or (\n",
    "               (s_neg, r_pos, t_pos) in data_ind) or (\n",
    "               (s_neg, r_pos, t_pos) in data_ind_valid) or (\n",
    "               (s_neg, r_pos, t_pos) in data_ind_test):\n",
    "            \n",
    "            s_neg = random.choice(list(new_ent_set))\n",
    "        \n",
    "        neg_triples.append((s_neg, r_pos, t_pos))\n",
    "    \n",
    "    else: #replace tail entity\n",
    "        t_neg = random.choice(list(new_ent_set))\n",
    "        \n",
    "        #filter out the existing triples\n",
    "        while ((s_pos, r_pos, t_neg) in data_test) or (\n",
    "               (s_pos, r_pos, t_neg) in data_valid) or (\n",
    "               (s_pos, r_pos, t_neg) in data) or (\n",
    "               (s_pos, r_pos, t_neg) in data_ind) or (\n",
    "               (s_pos, r_pos, t_neg) in data_ind_valid) or (\n",
    "               (s_pos, r_pos, t_neg) in data_ind_test):\n",
    "            \n",
    "            t_neg = random.choice(list(new_ent_set))\n",
    "        \n",
    "        neg_triples.append((s_pos, r_pos, t_neg))\n",
    "\n",
    "if len(pos_triples) != len(neg_triples):\n",
    "    raise ValueError('error when generating negative triples')\n",
    "        \n",
    "#combine all triples\n",
    "all_triples = pos_triples + neg_triples\n",
    "\n",
    "#obtain the label array\n",
    "arr1 = np.ones((len(pos_triples),))\n",
    "arr2 = np.zeros((len(neg_triples),))\n",
    "y_test = np.concatenate((arr1, arr2))\n",
    "\n",
    "#shuffle positive and negative triples (optional)\n",
    "all_triples, y_test = shuffle(all_triples, y_test)\n",
    "\n",
    "#obtain the score aray\n",
    "y_score = np.zeros((len(y_test),))\n",
    "\n",
    "#implement the scoring\n",
    "for i in range(len(all_triples)):\n",
    "    \n",
    "    s, r, t = all_triples[i][0], all_triples[i][1], all_triples[i][2]\n",
    "    \n",
    "    #path_score = path_based_triple_scoring(s, r, t, lower_bound, upper_bound_path, one_hop_ind, id2relation, model)\n",
    "    \n",
    "    subg_score = subgraph_triple_scoring(s, r, t, lower_bound, upper_bound_subg, one_hop_ind, id2relation, model_2)\n",
    "    \n",
    "    #ave_score = (path_score + subg_score)/float(2)\n",
    "    \n",
    "    #y_score[i] = ave_score\n",
    "    y_score[i] = subg_score\n",
    "    \n",
    "    if i % 20 == 0 and i > 0:\n",
    "        print('evaluating scores', i, len(all_triples))\n",
    "        \n",
    "        # Data to plot precision - recall curve\n",
    "        precision, recall, thresholds = precision_recall_curve(y_test[:i], y_score[:i])\n",
    "        # Use AUC function to calculate the area under the curve of precision recall curve\n",
    "        auc_precision_recall = auc(recall, precision)\n",
    "        print('AUC-PR is:', auc_precision_recall)\n",
    "        \n",
    "        \n",
    "# Data to plot precision - recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_score)\n",
    "# Use AUC function to calculate the area under the curve of precision recall curve\n",
    "auc_precision_recall = auc(recall, precision)\n",
    "print('AUC-PR is:', auc_precision_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9380c1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "#obtain the Hits@N for entity prediction##############\n",
    "\n",
    "#we select all the triples in the inductive test set\n",
    "selected = list(data_ind_test)\n",
    "\n",
    "###Hit at 1#############################\n",
    "#generate the negative samples by randomly replace relation with all the other relaiton\n",
    "Hits_at_1 = 0\n",
    "Hits_at_3 = 0\n",
    "Hits_at_10 = 0\n",
    "MRR_raw = 0.\n",
    "\n",
    "for i in range(len(selected)):\n",
    "    \n",
    "    triple_list = list()\n",
    "    \n",
    "    #score the true triple\n",
    "    s_pos, r_pos, t_pos = selected[i][0], selected[i][1], selected[i][2]\n",
    "\n",
    "    #path_score = path_based_triple_scoring(s_pos, r_pos, t_pos, lower_bound, upper_bound_path, one_hop_ind, id2relation, model)\n",
    "\n",
    "    subg_score = subgraph_triple_scoring(s_pos, r_pos, t_pos, lower_bound, upper_bound_subg, one_hop_ind, id2relation, model_2)\n",
    "    \n",
    "    #ave_score = (path_score + subg_score)/float(2)\n",
    "    \n",
    "    triple_list.append([(s_pos, r_pos, t_pos), subg_score])\n",
    "    \n",
    "    #generate the 50 random samples\n",
    "    for sub_i in range(50):\n",
    "        \n",
    "        #decide to replace the head or tail entity\n",
    "        number_0 = random.uniform(0, 1)\n",
    "\n",
    "        if number_0 < 0.5: #replace head entity\n",
    "            \n",
    "            s_neg = random.choice(list(new_ent_set))\n",
    "            \n",
    "            while ((s_neg, r_pos, t_pos) in data_test) or (\n",
    "                   (s_neg, r_pos, t_pos) in data_valid) or (\n",
    "                   (s_neg, r_pos, t_pos) in data) or (\n",
    "                   (s_neg, r_pos, t_pos) in data_ind) or (\n",
    "                   (s_neg, r_pos, t_pos) in data_ind_valid) or (\n",
    "                   (s_neg, r_pos, t_pos) in data_ind_test):\n",
    "\n",
    "                s_neg = random.choice(list(new_ent_set))\n",
    "            \n",
    "            #path_score = path_based_triple_scoring(s_neg, r_pos, t_pos, lower_bound, upper_bound_path, one_hop_ind, id2relation, model)\n",
    "\n",
    "            subg_score = subgraph_triple_scoring(s_neg, r_pos, t_pos, lower_bound, upper_bound_subg, one_hop_ind, id2relation, model_2)\n",
    "\n",
    "            #ave_score = (path_score + subg_score)/float(2)\n",
    "\n",
    "            triple_list.append([(s_neg, r_pos, t_pos), subg_score])\n",
    "            \n",
    "        else: #replace tail entity\n",
    "\n",
    "            t_neg = random.choice(list(new_ent_set))\n",
    "            \n",
    "            #filter out the existing triples\n",
    "            while ((s_pos, r_pos, t_neg) in data_test) or (\n",
    "                   (s_pos, r_pos, t_neg) in data_valid) or (\n",
    "                   (s_pos, r_pos, t_neg) in data) or (\n",
    "                   (s_pos, r_pos, t_neg) in data_ind) or (\n",
    "                   (s_pos, r_pos, t_neg) in data_ind_valid) or (\n",
    "                   (s_pos, r_pos, t_neg) in data_ind_test):\n",
    "\n",
    "                t_neg = random.choice(list(new_ent_set))\n",
    "            \n",
    "            #path_score = path_based_triple_scoring(s_pos, r_pos, t_neg, lower_bound, upper_bound_path, one_hop_ind, id2relation, model)\n",
    "\n",
    "            subg_score = subgraph_triple_scoring(s_pos, r_pos, t_neg, lower_bound, upper_bound_subg, one_hop_ind, id2relation, model_2)\n",
    "\n",
    "            #ave_score = (path_score + subg_score)/float(2)\n",
    "\n",
    "            triple_list.append([(s_pos, r_pos, t_neg), subg_score])\n",
    "            \n",
    "    #random shuffle!\n",
    "    random.shuffle(triple_list)\n",
    "    \n",
    "    #sort\n",
    "    sorted_list = sorted(triple_list, key = lambda x: x[-1], reverse=True)\n",
    "    \n",
    "    p = 0\n",
    "    \n",
    "    while p < len(sorted_list) and sorted_list[p][0] != (s_pos, r_pos, t_pos):\n",
    "            \n",
    "        p += 1\n",
    "    \n",
    "    if p == 0:\n",
    "        \n",
    "        Hits_at_1 += 1\n",
    "        \n",
    "    if p < 3:\n",
    "        \n",
    "        Hits_at_3 += 1\n",
    "        \n",
    "    if p < 10:\n",
    "        \n",
    "        Hits_at_10 += 1\n",
    "        \n",
    "    MRR_raw += 1./float(p + 1.) \n",
    "        \n",
    "    print('checkcorrect', (s_pos, r_pos, t_pos), sorted_list[p][0],\n",
    "          'real score', sorted_list[p][-1],\n",
    "          'Hits@1', Hits_at_1/(i+1),\n",
    "          'Hits@3', Hits_at_3/(i+1),\n",
    "          'Hits@10', Hits_at_10/(i+1),\n",
    "          'MRR', MRR_raw/(i+1),\n",
    "          'rank', p,\n",
    "          'total_num', i, len(selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefa2d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bb613b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4951254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b6af14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998880eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
