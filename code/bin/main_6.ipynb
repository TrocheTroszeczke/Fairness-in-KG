{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea7a7dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'nell_v4'\n",
    "model_id = 'main_6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54797cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#difine the names for saving\n",
    "model_name = 'Model_' + model_id + '_' + data_name\n",
    "ids_name = 'IDs_' + model_id + '_' + data_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fba371e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import opensmile\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c098e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadKG:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.x = 'Hello'\n",
    "        \n",
    "    def load_train_data(self, data_path, one_hop, data, s_t_r, entity2id, id2entity,\n",
    "                     relation2id, id2relation):\n",
    "        \n",
    "        data_ = set()\n",
    "    \n",
    "        ####load the train, valid and test set##########\n",
    "        with open (data_path, 'r') as f:\n",
    "            \n",
    "            data_ini = f.readlines()\n",
    "                        \n",
    "            for i in range(len(data_ini)):\n",
    "            \n",
    "                x = data_ini[i].split()\n",
    "                \n",
    "                x_ = tuple(x)\n",
    "                \n",
    "                data_.add(x_)\n",
    "        \n",
    "        ####relation dict#################\n",
    "        index = len(relation2id)\n",
    "     \n",
    "        for key in data_:\n",
    "            \n",
    "            if key[1] not in relation2id:\n",
    "                \n",
    "                relation = key[1]\n",
    "                \n",
    "                relation2id[relation] = index\n",
    "                \n",
    "                id2relation[index] = relation\n",
    "                \n",
    "                index += 1\n",
    "                \n",
    "                #the inverse relation\n",
    "                iv_r = '_inverse_' + relation\n",
    "                \n",
    "                relation2id[iv_r] = index\n",
    "                \n",
    "                id2relation[index] = iv_r\n",
    "                \n",
    "                index += 1\n",
    "        \n",
    "        #get the id of the inverse relation, by above definition, initial relation has \n",
    "        #always even id, while inverse relation has always odd id.\n",
    "        def inverse_r(r):\n",
    "            \n",
    "            if r % 2 == 0: #initial relation\n",
    "                \n",
    "                iv_r = r + 1\n",
    "            \n",
    "            else: #inverse relation\n",
    "                \n",
    "                iv_r = r - 1\n",
    "            \n",
    "            return(iv_r)\n",
    "        \n",
    "        ####entity dict###################\n",
    "        index = len(entity2id)\n",
    "        \n",
    "        for key in data_:\n",
    "            \n",
    "            source, target = key[0], key[2]\n",
    "            \n",
    "            if source not in entity2id:\n",
    "                                \n",
    "                entity2id[source] = index\n",
    "                \n",
    "                id2entity[index] = source\n",
    "                \n",
    "                index += 1\n",
    "            \n",
    "            if target not in entity2id:\n",
    "                \n",
    "                entity2id[target] = index\n",
    "                \n",
    "                id2entity[index] = target\n",
    "                \n",
    "                index += 1\n",
    "                \n",
    "        #create the set of triples using id instead of string        \n",
    "        for ele in data_:\n",
    "            \n",
    "            s = entity2id[ele[0]]\n",
    "            \n",
    "            r = relation2id[ele[1]]\n",
    "            \n",
    "            t = entity2id[ele[2]]\n",
    "            \n",
    "            if (s,r,t) not in data:\n",
    "                \n",
    "                data.add((s,r,t))\n",
    "            \n",
    "            s_t_r[(s,t)].add(r)\n",
    "            \n",
    "            if s not in one_hop:\n",
    "                \n",
    "                one_hop[s] = dict()\n",
    "            \n",
    "            if r not in one_hop[s]:\n",
    "                \n",
    "                one_hop[s][r] = set()\n",
    "            \n",
    "            one_hop[s][r].add(t)\n",
    "            \n",
    "            if t not in one_hop:\n",
    "                \n",
    "                one_hop[t] = dict()\n",
    "            \n",
    "            r_inv = inverse_r(r)\n",
    "            \n",
    "            s_t_r[(t,s)].add(r_inv)\n",
    "            \n",
    "            if r_inv not in one_hop[t]:\n",
    "                \n",
    "                one_hop[t][r_inv] = set()\n",
    "            \n",
    "            one_hop[t][r_inv].add(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6cd0f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObtainPathsByDynamicProgramming:\n",
    "\n",
    "    def __init__(self, size_bd=50, threshold=100000):\n",
    "                \n",
    "        self.size_bd = size_bd\n",
    "        \n",
    "        self.threshold = threshold\n",
    "    \n",
    "    '''\n",
    "    Given an entity s, here is the function to find:\n",
    "      1. any else entity t that is directely connected to s\n",
    "      2. most of the paths from s to each t with length L\n",
    "    \n",
    "    One may refer to LeetCode Problem 797 for details:\n",
    "        https://leetcode.com/problems/all-paths-from-source-to-target/\n",
    "    '''\n",
    "    def obtain_paths(self, mode, s, t_input, lower_bd, upper_bd, one_hop):\n",
    "\n",
    "        if type(lower_bd) != type(1) or lower_bd < 1:\n",
    "            \n",
    "            raise TypeError(\"!!! invalid lower bound setting, must >= 1 !!!\")\n",
    "            \n",
    "        if type(upper_bd) != type(1) or upper_bd < 1:\n",
    "            \n",
    "            raise TypeError(\"!!! invalid upper bound setting, must >= 1 !!!\")\n",
    "            \n",
    "        if lower_bd > upper_bd:\n",
    "            \n",
    "            raise TypeError(\"!!! lower bound must not exced upper bound !!!\")\n",
    "            \n",
    "        if s not in one_hop:\n",
    "            \n",
    "            raise ValueError('!!! entity not in one_hop. Please work on active entities for validation')\n",
    "        \n",
    "        #here is the result dict. Its key is each entity t that is directly connected to s\n",
    "        #The value of each t is a set containing the paths from s to t\n",
    "        #These paths can be either the direct connection r, or a multi-hop path\n",
    "        res = defaultdict(set)\n",
    "        \n",
    "        #direct_nb contains all the direct neighbour of s\n",
    "        direct_nb = set()\n",
    "        \n",
    "        if mode == 'direct_neighbour':\n",
    "        \n",
    "            for r in one_hop[s]:\n",
    "            \n",
    "                for t in one_hop[s][r]:\n",
    "                \n",
    "                    direct_nb.add(t)\n",
    "                    \n",
    "        elif mode == 'target_specified':\n",
    "            \n",
    "            direct_nb.add(t_input)\n",
    "            \n",
    "        elif mode == 'any_target':\n",
    "            \n",
    "            for s_any in one_hop:\n",
    "                \n",
    "                direct_nb.add(s_any)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            raise ValueError('not a valid mode')\n",
    "        \n",
    "        '''\n",
    "        We use recursion to find the paths\n",
    "        On current node with the path [r1, ..., rk] and on-path entities {e1, ..., ek-1, node}\n",
    "        from s to this node, we further find the direct neighbor t' of this node. \n",
    "        If t' is not a on-path entity (not among e1,...ek-1), we recursively proceed to t' \n",
    "        '''\n",
    "        def helper(node, path, on_path_en, res, direct_nb, lower_bd, upper_bd, one_hop, length_dict, count_dict):\n",
    "            \n",
    "            #when the current path is within lower_bd and upper_bd and its corresponding\n",
    "            #length still within the size_bd and its tail node is within the note dict, \n",
    "            #we will then intend to add this path\n",
    "            if (len(path) >= lower_bd) and (len(path) <= upper_bd) and (\n",
    "                node in direct_nb) and (length_dict[len(path)] < self.size_bd):\n",
    "                \n",
    "                #if this path already exists between the source entity and the current target node,\n",
    "                #we will not count it.\n",
    "                #here is an interesting situation: this path may exist between s and some other node t,\n",
    "                #however, it does not exist between s and this node t. Then, we still count it: length_dict[len(path)] += 1\n",
    "                #That is, each path may be counted for multiple times.\n",
    "                #We count how many paths we \"actually\" found between entity pairs\n",
    "                #Same type of path between different entity pairs are count separately.\n",
    "                if tuple(path) not in res[node]:\n",
    "                \n",
    "                    res[node].add(tuple(path))\n",
    "                \n",
    "                    length_dict[len(path)] += 1\n",
    "                \n",
    "            #For some rare entities, we may face such a case: so many paths are evaluated,\n",
    "            #but no entities on the paths are direct neighbors of the rare entity.\n",
    "            #In this case, the recursion cannot be bounded and stoped by the size threshold.\n",
    "            #In order to cure this, we count how many times the recursion happens on a specific length, using the count_dict.\n",
    "            #Its key is length, value counts the recursion occurred to that length. \n",
    "            #The recursion is forced to stop for that length (and hence for longer lengths) once reach the threshold.\n",
    "            if (len(path) < upper_bd) and (length_dict[len(path) + 1] < self.size_bd) and (\n",
    "                count_dict[len(path)] <= self.threshold):\n",
    "                \n",
    "                #we randomly shuffle relation r so that the reading in order is not fixed\n",
    "                temp_list = list()\n",
    "                \n",
    "                for r in one_hop[node]:\n",
    "                    \n",
    "                    temp_list.append(r)\n",
    "                \n",
    "                for i_0 in range(len(temp_list)):\n",
    "                    \n",
    "                    if count_dict[len(path)] > self.threshold:\n",
    "                        break\n",
    "                    \n",
    "                    r = random.choice(temp_list)\n",
    "                    \n",
    "                    for i_1 in range(len(one_hop[node][r])):\n",
    "                        \n",
    "                        if count_dict[len(path)] > self.threshold:\n",
    "                            break\n",
    "                        \n",
    "                        t = random.choice(list(one_hop[node][r]))\n",
    "                        \n",
    "                        if t not in on_path_en:\n",
    "                                \n",
    "                            count_dict[len(path)] += 1\n",
    "\n",
    "                            helper(t, path + [r], on_path_en.union({t}), res, direct_nb, \n",
    "                                   lower_bd, upper_bd, one_hop, length_dict, count_dict)\n",
    "        \n",
    "        length_dict = defaultdict(int)\n",
    "        count_dict = defaultdict(int)\n",
    "        \n",
    "        helper(s, [], {s}, res, direct_nb, lower_bd, upper_bd, one_hop, length_dict, count_dict)\n",
    "        \n",
    "        return(res, length_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecaf24ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/' + data_name + '/train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5718867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the classes\n",
    "Class_1 = LoadKG()\n",
    "Class_2 = ObtainPathsByDynamicProgramming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57c472f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the dictionaries and sets for load KG\n",
    "one_hop = dict() \n",
    "data = set()\n",
    "s_t_r = defaultdict(set)\n",
    "entity2id = dict()\n",
    "id2entity = dict()\n",
    "relation2id = dict()\n",
    "id2relation = dict()\n",
    "\n",
    "#fill in the sets and dicts\n",
    "Class_1.load_train_data(train_path, one_hop, data, s_t_r,\n",
    "                        entity2id, id2entity, relation2id, id2relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee8babf",
   "metadata": {},
   "source": [
    "### Build the deep neural network structure\n",
    "\n",
    "We use biLSTM to train on the input path embedding sequence to predict the output embedding or the relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68239c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 19:01:06.788935: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Input layer, using integer to represent each relation type\n",
    "#note that inputs_path is the path inputs, while inputs_out_re is the output relation inputs\n",
    "fst_path = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "scd_path = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "#the relation input layer (for output embedding)\n",
    "id_rela = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "# Embed each integer in a 300-dimensional vector as input,\n",
    "# note that we add another \"space holder\" embedding, \n",
    "# which hold the spaces if the initial length of two paths are not the same\n",
    "in_embd_var = layers.Embedding(len(relation2id)+1, 300)\n",
    "\n",
    "# Obtain the embedding\n",
    "fst_p_embd = in_embd_var(fst_path)\n",
    "scd_p_embd = in_embd_var(scd_path)\n",
    "\n",
    "# Embed each integer in a 300-dimensional vector as output\n",
    "rela_embd = layers.Embedding(len(relation2id)+1, 300)(id_rela)\n",
    "\n",
    "#add 2 layer bi-directional LSTM\n",
    "lstm_layer_1 = layers.Bidirectional(layers.LSTM(150, return_sequences=True))\n",
    "lstm_layer_2 = layers.Bidirectional(layers.LSTM(150, return_sequences=True))\n",
    "\n",
    "#first LSTM layer\n",
    "fst_lstm_mid = lstm_layer_1(fst_p_embd)\n",
    "scd_lstm_mid = lstm_layer_1(scd_p_embd)\n",
    "\n",
    "#second LSTM layer\n",
    "fst_lstm_out = lstm_layer_2(fst_lstm_mid)\n",
    "scd_lstm_out = lstm_layer_2(scd_lstm_mid)\n",
    "\n",
    "###########################################\n",
    "####apply the attention mechanism##########\n",
    "#first expand the dimention at the end to get ready for conv2D: (Batch,time,300,1)\n",
    "fst_exp_dim = tf.expand_dims(fst_lstm_out, axis=-1)\n",
    "scd_exp_dim = tf.expand_dims(scd_lstm_out, axis=-1)\n",
    "\n",
    "#define the attention layer using convolutional 2D: output is\n",
    "att_layer_conv2D = layers.Conv2D(100, (1, 300), padding='valid', activation='relu', \n",
    "                   input_shape=(None, 300), data_format='channels_last')\n",
    "\n",
    "#shape: (Batch,Time,1,100)\n",
    "fst_att_mid = att_layer_conv2D(fst_exp_dim)\n",
    "scd_att_mid = att_layer_conv2D(scd_exp_dim)\n",
    "\n",
    "#squeeze out the dim 1 to become: (Batch, Time, 100)\n",
    "fst_squez = tf.squeeze(fst_att_mid, 2)\n",
    "scd_squez = tf.squeeze(scd_att_mid, 2)\n",
    "\n",
    "#expand the dimention again to become (Batch, Time, 100, 1)\n",
    "fst_exp_dim_2 = tf.expand_dims(fst_squez, axis=-1)\n",
    "scd_exp_dim_2 = tf.expand_dims(scd_squez, axis=-1)\n",
    "\n",
    "#obtain the attention score for each time step by another conv2D layer\n",
    "att_layer_conv2D_2 = layers.Conv2D(1, (1, 100), padding='valid', activation='relu', \n",
    "                     input_shape=(None, 100), data_format='channels_last')\n",
    "\n",
    "#obtain (Batch, Time, 1, 1)\n",
    "fst_mid_score = att_layer_conv2D_2(fst_exp_dim_2)\n",
    "scd_mid_score = att_layer_conv2D_2(scd_exp_dim_2)\n",
    "\n",
    "#squeeze again to obtain (Batch, Time, 1)\n",
    "fst_squez_2 = tf.squeeze(fst_mid_score, -1)\n",
    "scd_squez_2 = tf.squeeze(scd_mid_score, -1)\n",
    "\n",
    "#softmax the attention score\n",
    "softmax_l = layers.Softmax(1) #define softmax\n",
    "\n",
    "fst_att_score = softmax_l(fst_squez_2)\n",
    "scd_att_score = softmax_l(scd_squez_2)\n",
    "\n",
    "#multiply the attention score to lstm output\n",
    "fst_att_befsum = layers.Multiply()([fst_lstm_out, fst_att_score])\n",
    "scd_att_befsum = layers.Multiply()([scd_lstm_out, scd_att_score])\n",
    "\n",
    "#sum over time dimension to complete the attention: (Batch, 300)\n",
    "fst_att_out = tf.reduce_sum(fst_att_befsum, axis=1)\n",
    "scd_att_out = tf.reduce_sum(scd_att_befsum, axis=1)\n",
    "######################################\n",
    "\n",
    "#concatenate the output vector from both siamese tunnel: (Batch, 600)\n",
    "path_concat = layers.concatenate([fst_att_out, scd_att_out], axis=-1)\n",
    "\n",
    "#multiply into output embd size by dense layer: (Batch, 300)\n",
    "path_out_vect = layers.Dense(300, activation='tanh')(path_concat)\n",
    "\n",
    "#remove the time dimension from the output embd since there is only one step\n",
    "rela_out_embd = tf.reduce_sum(rela_embd, axis=1)\n",
    "\n",
    "#concatenate the lstm output and output embd\n",
    "concat = layers.concatenate([path_out_vect, rela_out_embd], axis=-1)\n",
    "\n",
    "#add the dense layer\n",
    "dense_1 = layers.Dense(32, activation='relu')(concat)\n",
    "batch_norm = layers.BatchNormalization()(dense_1)\n",
    "dropout = layers.Dropout(0.25)(batch_norm)\n",
    "\n",
    "#final layer\n",
    "final_out = layers.Dense(2, activation='softmax')(dropout)\n",
    "\n",
    "#put together the model\n",
    "model = keras.Model([fst_path, scd_path, id_rela], final_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5cd4009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config the Adam optimizer \n",
    "opt = keras.optimizers.Adam(learning_rate=0.0005, decay=1e-6)\n",
    "\n",
    "#compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fd204f",
   "metadata": {},
   "source": [
    "### Build the batches\n",
    "We build each big-batch for each path combination with length (i,j). Then, we iteratively train the siamese network on different big-batches. The length of each big-batch is N.\n",
    "\n",
    "To be specific:\n",
    "* If we allow the length difference between two paths in a combination to be d, then the combination with path length i and path length j, denoted as (i,j), will be like (2,2), (2,3), (2,4), (3,3), (3,4), (3,5), ... \n",
    "* We will first build all the big-batches before fitting the NN model. \n",
    "* That is, we will perform the ObtainPathsByDynamicProgramming class function for some randomly chosen source entities. Then, for each target entity, we will further have two for loops:\n",
    "* for path_1 in all the \n",
    "* Do this until all the slots in all big-batchs are filled.\n",
    "* In every epoch, big-batchs will be re-filled.\n",
    "\n",
    "Then, in the training, we will use negative sampling: In each batch (actual batch, not the big-batch), we will include K true output relation embeddings and K random selected output relation embeddings. The true label is [1,0], while the false label is [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bb3ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to build all the big batches\n",
    "def build_big_batches(holder_len, lower_bd, upper_bd, Class_2, one_hop, s_t_r,\n",
    "                      x_p_list, x_r_list, y_list,\n",
    "                      relation2id, entity2id, id2relation, id2entity):\n",
    "    \n",
    "    if holder_len % 10 != 0:\n",
    "        raise ValueError('We would like to take 10X as a big-batch size')\n",
    "    \n",
    "    #the set of all relation IDs\n",
    "    relation_id_set = set()\n",
    "    for i in range(len(id2relation)):\n",
    "        \n",
    "        if i not in id2relation:\n",
    "            raise ValueError('error when generaing id2relation')\n",
    "        \n",
    "        relation_id_set.add(i)\n",
    "    \n",
    "    num_r = len(id2relation)\n",
    "    \n",
    "    #count how many appending has performed\n",
    "    count = 0\n",
    "\n",
    "    #in case not all entities in entity2id are in one_hop, \n",
    "    #so we need to find out who are indeed in\n",
    "    existing_ids = set()\n",
    "    \n",
    "    for s_1 in one_hop:\n",
    "        existing_ids.add(s_1)\n",
    "        \n",
    "    existing_ids = list(existing_ids)\n",
    "    \n",
    "    carry_on = True\n",
    "    \n",
    "    while carry_on:\n",
    "\n",
    "        #obtain paths by dynamic programming\n",
    "        source_id = random.choice(existing_ids)\n",
    "\n",
    "        result, length_dict = Class_2.obtain_paths('direct_neighbour', source_id, \n",
    "                                                   'not_specified', lower_bd, upper_bd, one_hop)\n",
    "        \n",
    "        #We want to increase the diversity of paths and targets.\n",
    "        #So we abandon one sub-graph from a source_id, if we sampled more than K1 path pairs\n",
    "        #Note that we mean \"sampled\", not \"appended\"! \n",
    "        #We do not care whether the pair is actually appended.\n",
    "        threshold_0 = 1000\n",
    "        count_0 = 0\n",
    "        \n",
    "        for target_id in result:\n",
    "\n",
    "            if (not carry_on) or (count_0 > threshold_0):\n",
    "                break\n",
    "            \n",
    "            #we want to make sure s, t are indeed directly connected, \n",
    "            #otherwise there is no relation for positive sample\n",
    "            #also, we want to make sure s and t and not connected by all relations, \n",
    "            #although this situation is rare. \n",
    "            #But in that case, there is no relation for negative samples\n",
    "            #Also, we want at least two different paths here between s and t\n",
    "            if ((source_id, target_id) in s_t_r) and (\n",
    "                len(s_t_r[(source_id, target_id)]) < len(id2relation)) and (\n",
    "                len(result[target_id]) >= 2):\n",
    "                \n",
    "                dir_r = list(s_t_r[(source_id, target_id)])\n",
    "                \n",
    "                non_dir_r = list(relation_id_set.difference(dir_r))\n",
    "                \n",
    "                if len(dir_r) <= 0:\n",
    "                    \n",
    "                    raise ValueError('errors when creating s_t_r !!')\n",
    "                    \n",
    "                temp_path_list = list(result[target_id])\n",
    "                    \n",
    "                #futhermore, we will abandon one targed_id if we sampled more than K2 times\n",
    "                threshold_1 = 50\n",
    "                count_1 = 0\n",
    "                \n",
    "                while count_1 <= threshold_1 and count_0 <= threshold_0:\n",
    "                \n",
    "                    temp_pair = random.sample(temp_path_list,2)\n",
    "                    \n",
    "                    path_1, path_2 = temp_pair[0], temp_pair[1]\n",
    "\n",
    "                    #decide which path is shorter and which is longer\n",
    "                    if len(path_1) <= len(path_2):\n",
    "\n",
    "                        path_s, path_l = path_1, path_2\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        path_s, path_l = path_2, path_1                            \n",
    "\n",
    "                    if (len(path_s) < lower_bd) or (len(path_l) > upper_bd):\n",
    "\n",
    "                        raise ValueError('something wrong with the path finding')\n",
    "\n",
    "                    #proceed when the entire length not yet reached,\n",
    "                    #and whether this path pair is new, and whether the two paths are different\n",
    "                    #But it is optional to require the path to be new. \n",
    "                    #We may remove this requirment, especially for short paths\n",
    "                    '''remember to cancel the comment below when using path_comb'''\n",
    "                    if (carry_on) and (path_s != path_l):\n",
    "\n",
    "                        #we always add one positive and one negative situation together,\n",
    "                        #hence, the length of list should always be even.\n",
    "                        #also we want to make sure the length of lists coincide\n",
    "                        if (len(x_p_list['s']) != len(y_list)) or (\n",
    "                            len(x_p_list['s']) != len(x_p_list['l'])) or (\n",
    "                            len(y_list) != len(x_r_list)) or (\n",
    "                            len(y_list) % 2 != 0):\n",
    "\n",
    "                            raise ValueError('error when building big batches: length error')\n",
    "\n",
    "                        #####positive#####################\n",
    "                        #we randomly choose one direction relation as the target relation\n",
    "                        relation_id = random.choice(dir_r)\n",
    "\n",
    "                        #append the paths: note that we add the space holder id at the end\n",
    "                        #of the shorter path\n",
    "                        x_p_list['s'].append(list(path_s) + [num_r]*abs(len(path_s)-upper_bd))\n",
    "                        x_p_list['l'].append(list(path_l) + [num_r]*abs(len(path_l)-upper_bd))\n",
    "\n",
    "                        #append relation\n",
    "                        x_r_list.append([relation_id])\n",
    "                        y_list.append([1., 0.])\n",
    "\n",
    "                        #####negative#####################\n",
    "                        relation_id = random.choice(non_dir_r)\n",
    "\n",
    "                        #append the paths: note that we add the space holder id at the end\n",
    "                        #of the shorter path\n",
    "                        x_p_list['s'].append(list(path_s) + [num_r]*abs(len(path_s)-upper_bd))\n",
    "                        x_p_list['l'].append(list(path_l) + [num_r]*abs(len(path_l)-upper_bd))\n",
    "\n",
    "                        #append relation\n",
    "                        x_r_list.append([relation_id])\n",
    "                        y_list.append([0., 1.])\n",
    "\n",
    "                        ######add to path combinations#####\n",
    "                        #here is the tricky part: we have to add both (path_s, path_l)\n",
    "                        #and (path_l, path_s). This is because when the length are the same\n",
    "                        #adding only one situation won't guarantee that \n",
    "                        #the same path with different order is also considered.\n",
    "                        #in other words: path combination don't have order, but our dict does.\n",
    "                        #so we have to add both situations.\n",
    "                        '''remember to cancel the comment here when using path_comb'''\n",
    "                        #path_comb[(len(path_s), len(path_l))].add((path_s, path_l))\n",
    "                        #path_comb[(len(path_s), len(path_l))].add((path_l, path_s))\n",
    "\n",
    "                        count += 2\n",
    "\n",
    "                        if count % 20000 == 0:\n",
    "                            print('generating big-batches', count, holder_len)\n",
    "\n",
    "                    if len(y_list) >= holder_len:\n",
    "\n",
    "                        carry_on = False\n",
    "                        \n",
    "                    count_1 += 1\n",
    "                    count_0 += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851e0ac2",
   "metadata": {},
   "source": [
    "### Start Training: load the KG and call classes\n",
    "\n",
    "Here, we use the validation set to see the training efficiency. That is, we use the validation to check whether the true relation between entities can be predicted by paths.\n",
    "\n",
    "The trick is: in validation, we have to use the same relation ID and entity ID as in the training. But we don't want to use the links in training anymore. That is, in validation, we want to use (and update if necessary) entity2id, id2entity, relation2id and id2relation. But we want to use new one_hop, data, data_ and s_t_r for validation set. Then, path-finding will also be based on new one_hop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28be7a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Model_main_6_fb237_v4'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17c02a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IDs_main_6_fb237_v4'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7f57c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, we save the relation and ids\n",
    "Dict = dict()\n",
    "Dict['one_hop'] = one_hop\n",
    "Dict['data'] = data\n",
    "Dict['s_t_r'] = s_t_r\n",
    "Dict['entity2id'] = entity2id\n",
    "Dict['id2entity'] = id2entity\n",
    "Dict['relation2id'] = relation2id\n",
    "Dict['id2relation'] = id2relation\n",
    "\n",
    "with open('../weight_bin/' + ids_name + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(Dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27f51852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating big-batches 20000 300000\n",
      "generating big-batches 40000 300000\n",
      "generating big-batches 60000 300000\n",
      "generating big-batches 80000 300000\n",
      "generating big-batches 100000 300000\n",
      "generating big-batches 120000 300000\n",
      "generating big-batches 140000 300000\n",
      "generating big-batches 160000 300000\n",
      "generating big-batches 180000 300000\n",
      "generating big-batches 200000 300000\n",
      "generating big-batches 220000 300000\n",
      "generating big-batches 240000 300000\n",
      "generating big-batches 260000 300000\n",
      "generating big-batches 280000 300000\n",
      "generating big-batches 300000 300000\n",
      "Epoch 1/30\n",
      "16875/16875 [==============================] - 286s 17ms/step - loss: 0.2599 - categorical_accuracy: 0.8969 - val_loss: 0.4905 - val_categorical_accuracy: 0.8404\n",
      "Epoch 2/30\n",
      "16875/16875 [==============================] - 280s 17ms/step - loss: 0.1354 - categorical_accuracy: 0.9551 - val_loss: 0.4694 - val_categorical_accuracy: 0.8712\n",
      "Epoch 3/30\n",
      "16875/16875 [==============================] - 280s 17ms/step - loss: 0.1145 - categorical_accuracy: 0.9636 - val_loss: 0.5390 - val_categorical_accuracy: 0.8582\n",
      "Epoch 4/30\n",
      "16875/16875 [==============================] - 278s 16ms/step - loss: 0.1011 - categorical_accuracy: 0.9681 - val_loss: 0.4791 - val_categorical_accuracy: 0.8780\n",
      "Epoch 5/30\n",
      "16875/16875 [==============================] - 2189s 130ms/step - loss: 0.0922 - categorical_accuracy: 0.9718 - val_loss: 0.6765 - val_categorical_accuracy: 0.8844\n",
      "Epoch 6/30\n",
      "16875/16875 [==============================] - 291s 17ms/step - loss: 0.0837 - categorical_accuracy: 0.9750 - val_loss: 0.5114 - val_categorical_accuracy: 0.8864\n",
      "Epoch 7/30\n",
      "16875/16875 [==============================] - 292s 17ms/step - loss: 0.0790 - categorical_accuracy: 0.9765 - val_loss: 0.4639 - val_categorical_accuracy: 0.8921\n",
      "Epoch 8/30\n",
      "16875/16875 [==============================] - 299s 18ms/step - loss: 0.0754 - categorical_accuracy: 0.9776 - val_loss: 0.4268 - val_categorical_accuracy: 0.9193\n",
      "Epoch 9/30\n",
      "16875/16875 [==============================] - 291s 17ms/step - loss: 0.0737 - categorical_accuracy: 0.9785 - val_loss: 0.4757 - val_categorical_accuracy: 0.8901\n",
      "Epoch 10/30\n",
      "16875/16875 [==============================] - 294s 17ms/step - loss: 0.0700 - categorical_accuracy: 0.9794 - val_loss: 0.4349 - val_categorical_accuracy: 0.9178\n",
      "Epoch 11/30\n",
      "16875/16875 [==============================] - 299s 18ms/step - loss: 0.0694 - categorical_accuracy: 0.9796 - val_loss: 0.6079 - val_categorical_accuracy: 0.9023\n",
      "Epoch 12/30\n",
      "16875/16875 [==============================] - 298s 18ms/step - loss: 0.0663 - categorical_accuracy: 0.9810 - val_loss: 0.5590 - val_categorical_accuracy: 0.8905\n",
      "Epoch 13/30\n",
      "16875/16875 [==============================] - 280s 17ms/step - loss: 0.0647 - categorical_accuracy: 0.9813 - val_loss: 0.4530 - val_categorical_accuracy: 0.8894\n",
      "Epoch 14/30\n",
      "16875/16875 [==============================] - 280s 17ms/step - loss: 0.0635 - categorical_accuracy: 0.9814 - val_loss: 0.6139 - val_categorical_accuracy: 0.8785\n",
      "Epoch 15/30\n",
      "16875/16875 [==============================] - 2115s 125ms/step - loss: 0.0623 - categorical_accuracy: 0.9818 - val_loss: 0.4646 - val_categorical_accuracy: 0.8951\n",
      "Epoch 16/30\n",
      "16875/16875 [==============================] - 284s 17ms/step - loss: 0.0602 - categorical_accuracy: 0.9825 - val_loss: 0.4589 - val_categorical_accuracy: 0.8807\n",
      "Epoch 17/30\n",
      "16875/16875 [==============================] - 284s 17ms/step - loss: 0.0601 - categorical_accuracy: 0.9826 - val_loss: 0.5568 - val_categorical_accuracy: 0.9028\n",
      "Epoch 18/30\n",
      "16875/16875 [==============================] - 284s 17ms/step - loss: 0.0572 - categorical_accuracy: 0.9835 - val_loss: 0.6605 - val_categorical_accuracy: 0.8817\n",
      "Epoch 19/30\n",
      "16875/16875 [==============================] - 289s 17ms/step - loss: 0.0575 - categorical_accuracy: 0.9833 - val_loss: 0.5283 - val_categorical_accuracy: 0.9099\n",
      "Epoch 20/30\n",
      "16875/16875 [==============================] - 304s 18ms/step - loss: 0.0584 - categorical_accuracy: 0.9829 - val_loss: 0.5436 - val_categorical_accuracy: 0.8967\n",
      "Epoch 21/30\n",
      "16875/16875 [==============================] - 311s 18ms/step - loss: 0.0562 - categorical_accuracy: 0.9838 - val_loss: 0.5539 - val_categorical_accuracy: 0.9035\n",
      "Epoch 22/30\n",
      "16875/16875 [==============================] - 305s 18ms/step - loss: 0.0546 - categorical_accuracy: 0.9842 - val_loss: 0.5580 - val_categorical_accuracy: 0.8912\n",
      "Epoch 23/30\n",
      "16875/16875 [==============================] - 293s 17ms/step - loss: 0.0544 - categorical_accuracy: 0.9841 - val_loss: 0.5249 - val_categorical_accuracy: 0.8948\n",
      "Epoch 24/30\n",
      "16875/16875 [==============================] - 291s 17ms/step - loss: 0.0543 - categorical_accuracy: 0.9842 - val_loss: 0.6292 - val_categorical_accuracy: 0.8719\n",
      "Epoch 25/30\n",
      "16875/16875 [==============================] - 292s 17ms/step - loss: 0.0522 - categorical_accuracy: 0.9849 - val_loss: 0.5997 - val_categorical_accuracy: 0.8943\n",
      "Epoch 26/30\n",
      "16875/16875 [==============================] - 294s 17ms/step - loss: 0.0519 - categorical_accuracy: 0.9849 - val_loss: 0.6510 - val_categorical_accuracy: 0.8817\n",
      "Epoch 27/30\n",
      "16875/16875 [==============================] - 281s 17ms/step - loss: 0.0512 - categorical_accuracy: 0.9852 - val_loss: 0.6097 - val_categorical_accuracy: 0.8815\n",
      "Epoch 28/30\n",
      "16875/16875 [==============================] - 281s 17ms/step - loss: 0.0520 - categorical_accuracy: 0.9851 - val_loss: 0.5703 - val_categorical_accuracy: 0.8848\n",
      "Epoch 29/30\n",
      "16875/16875 [==============================] - 1094s 65ms/step - loss: 0.0508 - categorical_accuracy: 0.9851 - val_loss: 0.5718 - val_categorical_accuracy: 0.8967\n",
      "Epoch 30/30\n",
      "16875/16875 [==============================] - 306s 18ms/step - loss: 0.0498 - categorical_accuracy: 0.9856 - val_loss: 0.5520 - val_categorical_accuracy: 0.8759\n",
      "Save model\n"
     ]
    }
   ],
   "source": [
    "holder_len = 300000\n",
    "lower_bd = 2\n",
    "upper_bd = 8\n",
    "num_epoch = 30\n",
    "batch_size = 16\n",
    "\n",
    "#90% to be train, 10% to be validation\n",
    "train_len = 9*int(holder_len/10)\n",
    "    \n",
    "######################################\n",
    "###pre-define the lists###############\n",
    "\n",
    "#define the lists\n",
    "x_p_list, x_r_list, y_list = {'s': [], 'l': []}, list(), list()\n",
    "\n",
    "#######################################\n",
    "###build the big-batches###############      \n",
    "\n",
    "#fill in the training array list\n",
    "build_big_batches(holder_len, lower_bd, upper_bd, Class_2, one_hop, s_t_r,\n",
    "                      x_p_list, x_r_list, y_list,\n",
    "                      relation2id, entity2id, id2relation, id2entity)\n",
    "\n",
    "#######################################\n",
    "###do the training#####################\n",
    "\n",
    "#generate the input arrays\n",
    "x_train_s = np.asarray(x_p_list['s'][:train_len], dtype='int')\n",
    "x_train_l = np.asarray(x_p_list['l'][:train_len], dtype='int')\n",
    "x_train_r = np.asarray(x_r_list[:train_len], dtype='int')\n",
    "y_train = np.asarray(y_list[:train_len], dtype='int')\n",
    "\n",
    "x_valid_s = np.asarray(x_p_list['s'][train_len:], dtype='int')\n",
    "x_valid_l = np.asarray(x_p_list['l'][train_len:], dtype='int')\n",
    "x_valid_r = np.asarray(x_r_list[train_len:], dtype='int')\n",
    "y_valid = np.asarray(y_list[train_len:], dtype='int')\n",
    "\n",
    "model.fit([x_train_s, x_train_l, x_train_r], y_train, \n",
    "          validation_data=([x_valid_s, x_valid_l, x_valid_r], y_valid),\n",
    "          batch_size=batch_size, epochs=num_epoch)\n",
    "\n",
    "# Save model and weights\n",
    "add_h5 = model_name + '.h5'\n",
    "save_dir = os.path.join(os.getcwd(), '../weight_bin')\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, add_h5)\n",
    "model.save(model_path)\n",
    "print('Save model')\n",
    "del(model)\n",
    "\n",
    "del(x_train_s, x_train_l, x_train_r, y_train)\n",
    "del(x_valid_s, x_valid_l, x_valid_r, y_valid)\n",
    "\n",
    "del(x_p_list, x_r_list, y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aa12c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd83344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1146ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a15e8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77fb8a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04108cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2178e81c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f9152c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cbcb96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4df421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867e4d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f86bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f1d82b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe787997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da50338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14b6bf00",
   "metadata": {},
   "source": [
    "### Result on the testset for inductive link prediction\n",
    "\n",
    "We use the testset for inductive link prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f959af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import opensmile\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37c81c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadKG:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.x = 'Hello'\n",
    "        \n",
    "    def load_train_data(self, data_path, one_hop, data, s_t_r, entity2id, id2entity,\n",
    "                     relation2id, id2relation):\n",
    "        \n",
    "        data_ = set()\n",
    "    \n",
    "        ####load the train, valid and test set##########\n",
    "        with open (data_path, 'r') as f:\n",
    "            \n",
    "            data_ini = f.readlines()\n",
    "                        \n",
    "            for i in range(len(data_ini)):\n",
    "            \n",
    "                x = data_ini[i].split()\n",
    "                \n",
    "                x_ = tuple(x)\n",
    "                \n",
    "                data_.add(x_)\n",
    "        \n",
    "        ####relation dict#################\n",
    "        index = len(relation2id)\n",
    "     \n",
    "        for key in data_:\n",
    "            \n",
    "            if key[1] not in relation2id:\n",
    "                \n",
    "                relation = key[1]\n",
    "                \n",
    "                relation2id[relation] = index\n",
    "                \n",
    "                id2relation[index] = relation\n",
    "                \n",
    "                index += 1\n",
    "                \n",
    "                #the inverse relation\n",
    "                iv_r = '_inverse_' + relation\n",
    "                \n",
    "                relation2id[iv_r] = index\n",
    "                \n",
    "                id2relation[index] = iv_r\n",
    "                \n",
    "                index += 1\n",
    "        \n",
    "        #get the id of the inverse relation, by above definition, initial relation has \n",
    "        #always even id, while inverse relation has always odd id.\n",
    "        def inverse_r(r):\n",
    "            \n",
    "            if r % 2 == 0: #initial relation\n",
    "                \n",
    "                iv_r = r + 1\n",
    "            \n",
    "            else: #inverse relation\n",
    "                \n",
    "                iv_r = r - 1\n",
    "            \n",
    "            return(iv_r)\n",
    "        \n",
    "        ####entity dict###################\n",
    "        index = len(entity2id)\n",
    "        \n",
    "        for key in data_:\n",
    "            \n",
    "            source, target = key[0], key[2]\n",
    "            \n",
    "            if source not in entity2id:\n",
    "                                \n",
    "                entity2id[source] = index\n",
    "                \n",
    "                id2entity[index] = source\n",
    "                \n",
    "                index += 1\n",
    "            \n",
    "            if target not in entity2id:\n",
    "                \n",
    "                entity2id[target] = index\n",
    "                \n",
    "                id2entity[index] = target\n",
    "                \n",
    "                index += 1\n",
    "                \n",
    "        #create the set of triples using id instead of string        \n",
    "        for ele in data_:\n",
    "            \n",
    "            s = entity2id[ele[0]]\n",
    "            \n",
    "            r = relation2id[ele[1]]\n",
    "            \n",
    "            t = entity2id[ele[2]]\n",
    "            \n",
    "            if (s,r,t) not in data:\n",
    "                \n",
    "                data.add((s,r,t))\n",
    "            \n",
    "            s_t_r[(s,t)].add(r)\n",
    "            \n",
    "            if s not in one_hop:\n",
    "                \n",
    "                one_hop[s] = dict()\n",
    "            \n",
    "            if r not in one_hop[s]:\n",
    "                \n",
    "                one_hop[s][r] = set()\n",
    "            \n",
    "            one_hop[s][r].add(t)\n",
    "            \n",
    "            if t not in one_hop:\n",
    "                \n",
    "                one_hop[t] = dict()\n",
    "            \n",
    "            r_inv = inverse_r(r)\n",
    "            \n",
    "            s_t_r[(t,s)].add(r_inv)\n",
    "            \n",
    "            if r_inv not in one_hop[t]:\n",
    "                \n",
    "                one_hop[t][r_inv] = set()\n",
    "            \n",
    "            one_hop[t][r_inv].add(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a714e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObtainPathsByDynamicProgramming:\n",
    "\n",
    "    def __init__(self, size_bd=50, threshold=100000):\n",
    "                \n",
    "        self.size_bd = size_bd\n",
    "        \n",
    "        self.threshold = threshold\n",
    "    \n",
    "    '''\n",
    "    Given an entity s, here is the function to find:\n",
    "      1. any else entity t that is directely connected to s\n",
    "      2. most of the paths from s to each t with length L\n",
    "    \n",
    "    One may refer to LeetCode Problem 797 for details:\n",
    "        https://leetcode.com/problems/all-paths-from-source-to-target/\n",
    "    '''\n",
    "    def obtain_paths(self, mode, s, t_input, lower_bd, upper_bd, one_hop):\n",
    "\n",
    "        if type(lower_bd) != type(1) or lower_bd < 1:\n",
    "            \n",
    "            raise TypeError(\"!!! invalid lower bound setting, must >= 1 !!!\")\n",
    "            \n",
    "        if type(upper_bd) != type(1) or upper_bd < 1:\n",
    "            \n",
    "            raise TypeError(\"!!! invalid upper bound setting, must >= 1 !!!\")\n",
    "            \n",
    "        if lower_bd > upper_bd:\n",
    "            \n",
    "            raise TypeError(\"!!! lower bound must not exced upper bound !!!\")\n",
    "            \n",
    "        if s not in one_hop:\n",
    "            \n",
    "            raise ValueError('!!! entity not in one_hop. Please work on active entities for validation')\n",
    "        \n",
    "        #here is the result dict. Its key is each entity t that is directly connected to s\n",
    "        #The value of each t is a set containing the paths from s to t\n",
    "        #These paths can be either the direct connection r, or a multi-hop path\n",
    "        res = defaultdict(set)\n",
    "        \n",
    "        #direct_nb contains all the direct neighbour of s\n",
    "        direct_nb = set()\n",
    "        \n",
    "        if mode == 'direct_neighbour':\n",
    "        \n",
    "            for r in one_hop[s]:\n",
    "            \n",
    "                for t in one_hop[s][r]:\n",
    "                \n",
    "                    direct_nb.add(t)\n",
    "                    \n",
    "        elif mode == 'target_specified':\n",
    "            \n",
    "            direct_nb.add(t_input)\n",
    "            \n",
    "        elif mode == 'any_target':\n",
    "            \n",
    "            for s_any in one_hop:\n",
    "                \n",
    "                direct_nb.add(s_any)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            raise ValueError('not a valid mode')\n",
    "        \n",
    "        '''\n",
    "        We use recursion to find the paths\n",
    "        On current node with the path [r1, ..., rk] and on-path entities {e1, ..., ek-1, node}\n",
    "        from s to this node, we further find the direct neighbor t' of this node. \n",
    "        If t' is not a on-path entity (not among e1,...ek-1), we recursively proceed to t' \n",
    "        '''\n",
    "        def helper(node, path, on_path_en, res, direct_nb, lower_bd, upper_bd, one_hop, length_dict, count_dict):\n",
    "            \n",
    "            #when the current path is within lower_bd and upper_bd and its corresponding\n",
    "            #length still within the size_bd and its tail node is within the note dict, \n",
    "            #we will then intend to add this path\n",
    "            if (len(path) >= lower_bd) and (len(path) <= upper_bd) and (\n",
    "                node in direct_nb) and (length_dict[len(path)] < self.size_bd):\n",
    "                \n",
    "                #if this path already exists between the source entity and the current target node,\n",
    "                #we will not count it.\n",
    "                #here is an interesting situation: this path may exist between s and some other node t,\n",
    "                #however, it does not exist between s and this node t. Then, we still count it: length_dict[len(path)] += 1\n",
    "                #That is, each path may be counted for multiple times.\n",
    "                #We count how many paths we \"actually\" found between entity pairs\n",
    "                #Same type of path between different entity pairs are count separately.\n",
    "                if tuple(path) not in res[node]:\n",
    "                \n",
    "                    res[node].add(tuple(path))\n",
    "                \n",
    "                    length_dict[len(path)] += 1\n",
    "                \n",
    "            #For some rare entities, we may face such a case: so many paths are evaluated,\n",
    "            #but no entities on the paths are direct neighbors of the rare entity.\n",
    "            #In this case, the recursion cannot be bounded and stoped by the size threshold.\n",
    "            #In order to cure this, we count how many times the recursion happens on a specific length, using the count_dict.\n",
    "            #Its key is length, value counts the recursion occurred to that length. \n",
    "            #The recursion is forced to stop for that length (and hence for longer lengths) once reach the threshold.\n",
    "            if (len(path) < upper_bd) and (length_dict[len(path) + 1] < self.size_bd) and (\n",
    "                count_dict[len(path)] <= self.threshold):\n",
    "                \n",
    "                #we randomly shuffle relation r so that the reading in order is not fixed\n",
    "                temp_list = list()\n",
    "                \n",
    "                for r in one_hop[node]:\n",
    "                    \n",
    "                    temp_list.append(r)\n",
    "                \n",
    "                for i_0 in range(len(temp_list)):\n",
    "                    \n",
    "                    if count_dict[len(path)] > self.threshold:\n",
    "                        break\n",
    "                    \n",
    "                    r = random.choice(temp_list)\n",
    "                    \n",
    "                    for i_1 in range(len(one_hop[node][r])):\n",
    "                        \n",
    "                        if count_dict[len(path)] > self.threshold:\n",
    "                            break\n",
    "                        \n",
    "                        t = random.choice(list(one_hop[node][r]))\n",
    "                        \n",
    "                        if t not in on_path_en:\n",
    "                                \n",
    "                            count_dict[len(path)] += 1\n",
    "\n",
    "                            helper(t, path + [r], on_path_en.union({t}), res, direct_nb, \n",
    "                                   lower_bd, upper_bd, one_hop, length_dict, count_dict)\n",
    "        \n",
    "        length_dict = defaultdict(int)\n",
    "        count_dict = defaultdict(int)\n",
    "        \n",
    "        helper(s, [], {s}, res, direct_nb, lower_bd, upper_bd, one_hop, length_dict, count_dict)\n",
    "        \n",
    "        return(res, length_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0d1f8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the classes\n",
    "Class_1 = LoadKG()\n",
    "Class_2 = ObtainPathsByDynamicProgramming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10f13661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load ids and relation/entity dicts\n",
    "with open('../weight_bin/' + ids_name + '.pickle', 'rb') as handle:\n",
    "    Dict = pickle.load(handle)\n",
    "\n",
    "one_hop = Dict['one_hop']\n",
    "data = Dict['data']\n",
    "s_t_r = Dict['s_t_r']\n",
    "entity2id = Dict['entity2id']\n",
    "id2entity = Dict['id2entity']\n",
    "relation2id = Dict['relation2id']\n",
    "id2relation = Dict['id2relation']\n",
    "\n",
    "#we want to keep the initial entity/relation dicts\n",
    "entity2id_ini = deepcopy(entity2id)\n",
    "id2entity_ini = deepcopy(id2entity)\n",
    "relation2id_ini = deepcopy(relation2id)\n",
    "id2relation_ini = deepcopy(id2relation)\n",
    "\n",
    "num_r = len(id2relation)\n",
    "num_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50c64cf2",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at ../weight_bin/Model_main_6_nell_v4.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1h/rcbfn0354hv_dhq9m90rp5kc0000gn/T/ipykernel_24471/710329891.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../weight_bin/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/python39_tensorfl/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/python39_tensorfl/lib/python3.9/site-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'No file or directory found at {filepath_str}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: No file or directory found at ../weight_bin/Model_main_6_nell_v4.h5"
     ]
    }
   ],
   "source": [
    "#load the model\n",
    "model = keras.models.load_model('../weight_bin/' + model_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2ea521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train_path = '../data/' + data_name + '_ind/train.txt'\n",
    "ind_valid_path = '../data/' + data_name + '_ind/valid.txt'\n",
    "ind_test_path = '../data/' + data_name + '_ind/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d2e6087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the test dataset\n",
    "one_hop_ind = dict() \n",
    "data_ind = set()\n",
    "s_t_r_ind = defaultdict(set)\n",
    "\n",
    "len_0 = len(relation2id)\n",
    "size_0 = len(entity2id)\n",
    "\n",
    "#fill in the sets and dicts\n",
    "Class_1.load_train_data(ind_train_path, \n",
    "                        one_hop_ind, data_ind, s_t_r_ind,\n",
    "                        entity2id, id2entity, relation2id, id2relation)\n",
    "\n",
    "len_1 = len(relation2id)\n",
    "size_1 = len(entity2id)\n",
    "\n",
    "if len_0 != len_1:\n",
    "    raise ValueError('unseen relation!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a6dfe59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4707 7758 11714\n"
     ]
    }
   ],
   "source": [
    "print(size_0, size_1, len(data_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63cec98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the test dataset\n",
    "one_hop_test = dict() \n",
    "data_test = set()\n",
    "s_t_r_test = defaultdict(set)\n",
    "\n",
    "len_0 = len(relation2id)\n",
    "size_0 = len(entity2id)\n",
    "\n",
    "#fill in the sets and dicts\n",
    "Class_1.load_train_data(ind_test_path, \n",
    "                        one_hop_test, data_test, s_t_r_test,\n",
    "                        entity2id, id2entity, relation2id, id2relation)\n",
    "\n",
    "\n",
    "len_1 = len(relation2id)\n",
    "size_1 = len(entity2id)\n",
    "\n",
    "if len_0 != len_1:\n",
    "    raise ValueError('unseen relation!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d18fee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7758 7758 1424\n"
     ]
    }
   ],
   "source": [
    "print(size_0, size_1, len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "757526ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the validation for existing triple removal when ranking\n",
    "one_hop_valid = dict() \n",
    "data_valid = set()\n",
    "s_t_r_valid = defaultdict(set)\n",
    "\n",
    "len_0 = len(relation2id)\n",
    "size_0 = len(entity2id)\n",
    "\n",
    "#fill in the sets and dicts\n",
    "Class_1.load_train_data(ind_valid_path, \n",
    "                        one_hop_valid, data_valid, s_t_r_valid,\n",
    "                        entity2id, id2entity, relation2id, id2relation)\n",
    "\n",
    "len_1 = len(relation2id)\n",
    "size_1 = len(entity2id)\n",
    "\n",
    "if len_0 != len_1:\n",
    "    raise ValueError('unseen relation!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2980a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7758 7758 1416\n"
     ]
    }
   ],
   "source": [
    "print(size_0, size_1, len(data_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f17ff08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7758 4707\n"
     ]
    }
   ],
   "source": [
    "print(len(entity2id), len(entity2id_ini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "915ad29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we want to check whether there are overlapping \n",
    "#between the entities of train triples and inductive test and valid triples\n",
    "overlapping = 0\n",
    "\n",
    "for ele in data_test:\n",
    "    \n",
    "    s, r, t = ele[0], ele[1], ele[2]\n",
    "    \n",
    "    if s in id2entity_ini or t in id2entity_ini:\n",
    "        \n",
    "        overlapping += 1\n",
    "        \n",
    "overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80eb1e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlapping = 0\n",
    "\n",
    "for ele in data_valid:\n",
    "    \n",
    "    s, r, t = ele[0], ele[1], ele[2]\n",
    "    \n",
    "    if s in id2entity_ini or t in id2entity_ini:\n",
    "        \n",
    "        overlapping += 1\n",
    "        \n",
    "overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a3c9dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we want to check whether there are overlapping \n",
    "#between the entities of train triples and inductive test and valid triples\n",
    "overlapping = 0\n",
    "\n",
    "for ele in data_ind:\n",
    "    \n",
    "    s, r, t = ele[0], ele[1], ele[2]\n",
    "    \n",
    "    if s in id2entity_ini or t in id2entity_ini:\n",
    "        \n",
    "        overlapping += 1\n",
    "        \n",
    "overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83ea5533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relation_ranking(s, t, lower_bd, upper_bd, one_hop, id2relation, model):\n",
    "    \n",
    "    path_holder = set()\n",
    "    \n",
    "    for iteration in range(20):\n",
    "    \n",
    "        result, length_dict = Class_2.obtain_paths('target_specified', \n",
    "                                                   s, t, lower_bd, upper_bd, one_hop)\n",
    "        if t in result:\n",
    "            \n",
    "            for path in result[t]:\n",
    "                \n",
    "                path_holder.add(path)\n",
    "    \n",
    "    path_holder = list(path_holder)\n",
    "    random.shuffle(path_holder)\n",
    "    \n",
    "    score_dict = defaultdict(float)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    if len(path_holder) >= 2:\n",
    "    \n",
    "        #iterate over path_1\n",
    "        while count <= 50:\n",
    "\n",
    "            temp_pair = random.sample(path_holder, 2)\n",
    "\n",
    "            path_1, path_2 = temp_pair[0], temp_pair[1]\n",
    "\n",
    "            #decide which path is shorter and which is longer\n",
    "            if len(path_1) <= len(path_2):\n",
    "\n",
    "                path_s, path_l = path_1, path_2\n",
    "\n",
    "            else:\n",
    "\n",
    "                path_s, path_l = path_2, path_1                            \n",
    "\n",
    "            #whether lengths of the two paths satisfies the requirments\n",
    "            if (len(path_s) < lower_bd) or (len(path_l) > upper_bd):\n",
    "\n",
    "                raise ValueError('something wrong with path finding')\n",
    "\n",
    "            list_s = list()\n",
    "            list_l = list()\n",
    "            list_r = list()\n",
    "\n",
    "            for i in range(len(id2relation)):\n",
    "\n",
    "                if i not in id2relation:\n",
    "\n",
    "                    raise ValueError ('error when generating id2relation')\n",
    "\n",
    "                list_s.append(list(path_s) + [num_r]*abs(len(path_s)-upper_bd))\n",
    "                list_l.append(list(path_l) + [num_r]*abs(len(path_l)-upper_bd))\n",
    "                list_r.append([i])\n",
    "\n",
    "            input_s = np.array(list_s)\n",
    "            input_l = np.array(list_l)\n",
    "            input_r = np.array(list_r)\n",
    "\n",
    "            pred = model.predict([input_s, input_l, input_r], verbose = 0)\n",
    "\n",
    "            for i in range(pred.shape[0]):\n",
    "\n",
    "                score_dict[i] += float(pred[i][0])\n",
    "\n",
    "            count += 1\n",
    "                \n",
    "    print(len(score_dict), len(path_holder))\n",
    "\n",
    "    return(score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cd527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "#obtain the precision-recall area under curve (AUC-PR)##\n",
    "\n",
    "#randomly select 10% of the triples\n",
    "selected = random.sample(list(data_test), min(len(data_test), 500))\n",
    "\n",
    "random.shuffle(selected)\n",
    "\n",
    "###Hit at 1#############################\n",
    "#generate the negative samples by randomly replace relation with all the other relaiton\n",
    "Hits_at_1 = 0\n",
    "Hits_at_3 = 0\n",
    "Hits_at_10 = 0\n",
    "MRR_raw = 0.\n",
    "\n",
    "for i in range(len(selected)):\n",
    "    \n",
    "    s_true, r_true, t_true = selected[i][0], selected[i][1], selected[i][2]\n",
    "    \n",
    "    score_dict = relation_ranking(s_true, t_true, 2, 10, one_hop_ind, id2relation, model)\n",
    "    \n",
    "    #[... [score, r], ...]\n",
    "    temp_list = list()\n",
    "    \n",
    "    for r in id2relation:\n",
    "        \n",
    "        if r in score_dict:\n",
    "            \n",
    "            temp_list.append([score_dict[r], r])\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            temp_list.append([0.0, r])\n",
    "        \n",
    "    sorted_list = sorted(temp_list, key = lambda x: x[0], reverse=True)\n",
    "    \n",
    "    p = 0\n",
    "    exist_tri = 0\n",
    "    \n",
    "    while p < len(sorted_list) and sorted_list[p][1] != r_true:\n",
    "        \n",
    "        #moreover, we want to remove existing triples\n",
    "        if ((s_true, sorted_list[p][1], t_true) in data_test) or (\n",
    "              (s_true, sorted_list[p][1], t_true) in data_valid) or (\n",
    "              (s_true, sorted_list[p][1], t_true) in data_ind):\n",
    "            \n",
    "            exist_tri += 1\n",
    "            \n",
    "        p += 1\n",
    "    \n",
    "    if p - exist_tri == 0:\n",
    "        \n",
    "        Hits_at_1 += 1\n",
    "        \n",
    "    if p - exist_tri < 3:\n",
    "        \n",
    "        Hits_at_3 += 1\n",
    "        \n",
    "    if p - exist_tri < 10:\n",
    "        \n",
    "        Hits_at_10 += 1\n",
    "        \n",
    "    MRR_raw += 1./float(p - exist_tri + 1.) \n",
    "        \n",
    "    print('Hits@1', Hits_at_1/(i+1),\n",
    "          'Hits@3', Hits_at_3/(i+1),\n",
    "          'Hits@10', Hits_at_10/(i+1),\n",
    "          'MRR', MRR_raw/(i+1),\n",
    "          'cur_rank', p - exist_tri,\n",
    "          'abs_cur_rank', p,\n",
    "          'total_num', i, len(selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9bbad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46118fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8df22e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e4f427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87550dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756670d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2a3789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0974fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f75daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a876bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45095a43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdf4397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68ff3bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c992580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d0ab9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0ceddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb84dd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438 635\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 1.0 MRR 0.25 cur_rank 3 abs_cur_rank 3 total_num 0 500\n",
      "438 3006\n",
      "Hits@1 0.0 Hits@3 0.5 Hits@10 1.0 MRR 0.375 cur_rank 1 abs_cur_rank 1 total_num 1 500\n",
      "438 426\n",
      "Hits@1 0.0 Hits@3 0.3333333333333333 Hits@10 1.0 MRR 0.31666666666666665 cur_rank 4 abs_cur_rank 4 total_num 2 500\n",
      "438 1651\n",
      "Hits@1 0.25 Hits@3 0.5 Hits@10 1.0 MRR 0.4875 cur_rank 0 abs_cur_rank 0 total_num 3 500\n",
      "438 147\n",
      "Hits@1 0.4 Hits@3 0.6 Hits@10 1.0 MRR 0.5900000000000001 cur_rank 0 abs_cur_rank 0 total_num 4 500\n",
      "438 1387\n",
      "Hits@1 0.5 Hits@3 0.6666666666666666 Hits@10 1.0 MRR 0.6583333333333333 cur_rank 0 abs_cur_rank 0 total_num 5 500\n",
      "438 1180\n",
      "Hits@1 0.42857142857142855 Hits@3 0.7142857142857143 Hits@10 1.0 MRR 0.6357142857142858 cur_rank 1 abs_cur_rank 1 total_num 6 500\n",
      "438 769\n",
      "Hits@1 0.375 Hits@3 0.75 Hits@10 1.0 MRR 0.61875 cur_rank 1 abs_cur_rank 1 total_num 7 500\n",
      "438 2942\n",
      "Hits@1 0.4444444444444444 Hits@3 0.7777777777777778 Hits@10 1.0 MRR 0.6611111111111111 cur_rank 0 abs_cur_rank 0 total_num 8 500\n",
      "438 632\n",
      "Hits@1 0.4 Hits@3 0.7 Hits@10 0.9 MRR 0.6012500000000001 cur_rank 15 abs_cur_rank 15 total_num 9 500\n",
      "438 543\n",
      "Hits@1 0.36363636363636365 Hits@3 0.6363636363636364 Hits@10 0.8181818181818182 MRR 0.5469844549390004 cur_rank 230 abs_cur_rank 230 total_num 10 500\n",
      "438 179\n",
      "Hits@1 0.3333333333333333 Hits@3 0.5833333333333334 Hits@10 0.75 MRR 0.5050256054332142 cur_rank 22 abs_cur_rank 22 total_num 11 500\n",
      "438 3530\n",
      "Hits@1 0.3076923076923077 Hits@3 0.6153846153846154 Hits@10 0.7692307692307693 MRR 0.4918185075793771 cur_rank 2 abs_cur_rank 3 total_num 12 500\n",
      "438 453\n",
      "Hits@1 0.2857142857142857 Hits@3 0.6428571428571429 Hits@10 0.7857142857142857 MRR 0.480498137990374 cur_rank 2 abs_cur_rank 2 total_num 13 500\n",
      "438 5184\n",
      "Hits@1 0.26666666666666666 Hits@3 0.6666666666666666 Hits@10 0.8 MRR 0.48179826212434906 cur_rank 1 abs_cur_rank 1 total_num 14 500\n",
      "438 404\n",
      "Hits@1 0.25 Hits@3 0.625 Hits@10 0.75 MRR 0.45191995313858097 cur_rank 266 abs_cur_rank 266 total_num 15 500\n",
      "438 772\n",
      "Hits@1 0.23529411764705882 Hits@3 0.6470588235294118 Hits@10 0.7647058823529411 MRR 0.4547481911892527 cur_rank 1 abs_cur_rank 1 total_num 16 500\n",
      "438 2049\n",
      "Hits@1 0.2777777777777778 Hits@3 0.6666666666666666 Hits@10 0.7777777777777778 MRR 0.48503995834540525 cur_rank 0 abs_cur_rank 0 total_num 17 500\n",
      "438 76\n",
      "Hits@1 0.2631578947368421 Hits@3 0.631578947368421 Hits@10 0.7894736842105263 MRR 0.4726694342219629 cur_rank 3 abs_cur_rank 3 total_num 18 500\n",
      "438 3084\n",
      "Hits@1 0.25 Hits@3 0.6 Hits@10 0.8 MRR 0.4615359625108647 cur_rank 3 abs_cur_rank 3 total_num 19 500\n",
      "438 4403\n",
      "Hits@1 0.23809523809523808 Hits@3 0.5714285714285714 Hits@10 0.8095238095238095 MRR 0.4474945674706648 cur_rank 5 abs_cur_rank 5 total_num 20 500\n",
      "438 12\n",
      "Hits@1 0.22727272727272727 Hits@3 0.5454545454545454 Hits@10 0.8181818181818182 MRR 0.43851754167654367 cur_rank 3 abs_cur_rank 3 total_num 21 500\n",
      "438 245\n",
      "Hits@1 0.21739130434782608 Hits@3 0.5217391304347826 Hits@10 0.8260869565217391 MRR 0.4266979384152446 cur_rank 5 abs_cur_rank 5 total_num 22 500\n",
      "438 3363\n",
      "Hits@1 0.20833333333333334 Hits@3 0.5 Hits@10 0.7916666666666666 MRR 0.4090545796892023 cur_rank 306 abs_cur_rank 306 total_num 23 500\n",
      "438 3442\n",
      "Hits@1 0.2 Hits@3 0.48 Hits@10 0.76 MRR 0.392817788351164 cur_rank 318 abs_cur_rank 318 total_num 24 500\n",
      "438 517\n",
      "Hits@1 0.19230769230769232 Hits@3 0.46153846153846156 Hits@10 0.7307692307692307 MRR 0.37903567182306885 cur_rank 28 abs_cur_rank 28 total_num 25 500\n",
      "438 4336\n",
      "Hits@1 0.2222222222222222 Hits@3 0.48148148148148145 Hits@10 0.7407407407407407 MRR 0.40203435064443666 cur_rank 0 abs_cur_rank 0 total_num 26 500\n",
      "438 973\n",
      "Hits@1 0.21428571428571427 Hits@3 0.5 Hits@10 0.75 MRR 0.4055331238357068 cur_rank 1 abs_cur_rank 1 total_num 27 500\n",
      "438 1412\n",
      "Hits@1 0.20689655172413793 Hits@3 0.4827586206896552 Hits@10 0.7586206896551724 MRR 0.4001699126689583 cur_rank 3 abs_cur_rank 3 total_num 28 500\n",
      "438 1669\n",
      "Hits@1 0.23333333333333334 Hits@3 0.5 Hits@10 0.7666666666666667 MRR 0.4201642489133263 cur_rank 0 abs_cur_rank 0 total_num 29 500\n",
      "438 2111\n",
      "Hits@1 0.25806451612903225 Hits@3 0.5161290322580645 Hits@10 0.7741935483870968 MRR 0.43886862798063836 cur_rank 0 abs_cur_rank 0 total_num 30 500\n",
      "438 2169\n",
      "Hits@1 0.25 Hits@3 0.53125 Hits@10 0.78125 MRR 0.44077898335624344 cur_rank 1 abs_cur_rank 1 total_num 31 500\n",
      "438 2246\n",
      "Hits@1 0.24242424242424243 Hits@3 0.5151515151515151 Hits@10 0.7575757575757576 MRR 0.42757744462206043 cur_rank 194 abs_cur_rank 195 total_num 32 500\n",
      "438 4475\n",
      "Hits@1 0.2647058823529412 Hits@3 0.5294117647058824 Hits@10 0.7647058823529411 MRR 0.4444134021331763 cur_rank 0 abs_cur_rank 0 total_num 33 500\n",
      "438 2605\n",
      "Hits@1 0.2571428571428571 Hits@3 0.5142857142857142 Hits@10 0.7714285714285715 MRR 0.4374301620722284 cur_rank 4 abs_cur_rank 4 total_num 34 500\n",
      "438 364\n",
      "Hits@1 0.25 Hits@3 0.5 Hits@10 0.7777777777777778 MRR 0.42875154645911095 cur_rank 7 abs_cur_rank 7 total_num 35 500\n",
      "438 354\n",
      "Hits@1 0.2702702702702703 Hits@3 0.5135135135135135 Hits@10 0.7837837837837838 MRR 0.44419069385210797 cur_rank 0 abs_cur_rank 0 total_num 36 500\n",
      "438 3342\n",
      "Hits@1 0.2894736842105263 Hits@3 0.5263157894736842 Hits@10 0.7894736842105263 MRR 0.4588172545402104 cur_rank 0 abs_cur_rank 1 total_num 37 500\n",
      "438 2\n",
      "Hits@1 0.28205128205128205 Hits@3 0.5128205128205128 Hits@10 0.7692307692307693 MRR 0.4477101717479236 cur_rank 38 abs_cur_rank 38 total_num 38 500\n",
      "438 1933\n",
      "Hits@1 0.275 Hits@3 0.5 Hits@10 0.775 MRR 0.4427674174542255 cur_rank 3 abs_cur_rank 3 total_num 39 500\n",
      "438 450\n",
      "Hits@1 0.2682926829268293 Hits@3 0.4878048780487805 Hits@10 0.7804878048780488 MRR 0.43806577312607364 cur_rank 3 abs_cur_rank 3 total_num 40 500\n",
      "438 664\n",
      "Hits@1 0.2857142857142857 Hits@3 0.5 Hits@10 0.7857142857142857 MRR 0.45144515948021474 cur_rank 0 abs_cur_rank 1 total_num 41 500\n",
      "438 637\n",
      "Hits@1 0.27906976744186046 Hits@3 0.5116279069767442 Hits@10 0.7906976744186046 MRR 0.44869837282563607 cur_rank 2 abs_cur_rank 2 total_num 42 500\n",
      "438 1242\n",
      "Hits@1 0.2727272727272727 Hits@3 0.5227272727272727 Hits@10 0.7954545454545454 MRR 0.4498643188977807 cur_rank 1 abs_cur_rank 1 total_num 43 500\n",
      "438 854\n",
      "Hits@1 0.28888888888888886 Hits@3 0.5333333333333333 Hits@10 0.8 MRR 0.4620895562556078 cur_rank 0 abs_cur_rank 0 total_num 44 500\n",
      "438 333\n",
      "Hits@1 0.2826086956521739 Hits@3 0.5217391304347826 Hits@10 0.782608695652174 MRR 0.4534934064819352 cur_rank 14 abs_cur_rank 14 total_num 45 500\n",
      "438 1674\n",
      "Hits@1 0.2765957446808511 Hits@3 0.5319148936170213 Hits@10 0.7872340425531915 MRR 0.45448290847168127 cur_rank 1 abs_cur_rank 1 total_num 46 500\n",
      "438 1269\n",
      "Hits@1 0.2916666666666667 Hits@3 0.5416666666666666 Hits@10 0.7916666666666666 MRR 0.46584784787852124 cur_rank 0 abs_cur_rank 0 total_num 47 500\n",
      "438 953\n",
      "Hits@1 0.30612244897959184 Hits@3 0.5510204081632653 Hits@10 0.7959183673469388 MRR 0.476748912207531 cur_rank 0 abs_cur_rank 0 total_num 48 500\n",
      "438 133\n",
      "Hits@1 0.3 Hits@3 0.56 Hits@10 0.8 MRR 0.47721393396338035 cur_rank 1 abs_cur_rank 1 total_num 49 500\n",
      "438 485\n",
      "Hits@1 0.29411764705882354 Hits@3 0.5686274509803921 Hits@10 0.803921568627451 MRR 0.47766071957194156 cur_rank 1 abs_cur_rank 1 total_num 50 500\n",
      "438 2103\n",
      "Hits@1 0.28846153846153844 Hits@3 0.5576923076923077 Hits@10 0.8076923076923077 MRR 0.47232109034940417 cur_rank 4 abs_cur_rank 4 total_num 51 500\n",
      "438 3590\n",
      "Hits@1 0.2830188679245283 Hits@3 0.5471698113207547 Hits@10 0.8113207547169812 MRR 0.46812635279564185 cur_rank 3 abs_cur_rank 3 total_num 52 500\n",
      "438 703\n",
      "Hits@1 0.2962962962962963 Hits@3 0.5555555555555556 Hits@10 0.8148148148148148 MRR 0.47797586478090776 cur_rank 0 abs_cur_rank 1 total_num 53 500\n",
      "438 16\n",
      "Hits@1 0.2909090909090909 Hits@3 0.5636363636363636 Hits@10 0.8181818181818182 MRR 0.47837630360307304 cur_rank 1 abs_cur_rank 1 total_num 54 500\n",
      "438 1432\n",
      "Hits@1 0.30357142857142855 Hits@3 0.5714285714285714 Hits@10 0.8214285714285714 MRR 0.4876910124673039 cur_rank 0 abs_cur_rank 0 total_num 55 500\n",
      "438 1385\n",
      "Hits@1 0.2982456140350877 Hits@3 0.5789473684210527 Hits@10 0.8245614035087719 MRR 0.4879069596170003 cur_rank 1 abs_cur_rank 1 total_num 56 500\n",
      "438 343\n",
      "Hits@1 0.3103448275862069 Hits@3 0.5862068965517241 Hits@10 0.8275862068965517 MRR 0.49673614996843135 cur_rank 0 abs_cur_rank 0 total_num 57 500\n",
      "438 346\n",
      "Hits@1 0.3050847457627119 Hits@3 0.5932203389830508 Hits@10 0.8305084745762712 MRR 0.49679146946049185 cur_rank 1 abs_cur_rank 1 total_num 58 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438 561\n",
      "Hits@1 0.3 Hits@3 0.6 Hits@10 0.8333333333333334 MRR 0.4968449449694836 cur_rank 1 abs_cur_rank 1 total_num 59 500\n",
      "438 148\n",
      "Hits@1 0.29508196721311475 Hits@3 0.6065573770491803 Hits@10 0.8360655737704918 MRR 0.49689666718309866 cur_rank 1 abs_cur_rank 1 total_num 60 500\n",
      "438 148\n",
      "Hits@1 0.2903225806451613 Hits@3 0.6129032258064516 Hits@10 0.8387096774193549 MRR 0.49694672093820996 cur_rank 1 abs_cur_rank 1 total_num 61 500\n",
      "438 634\n",
      "Hits@1 0.30158730158730157 Hits@3 0.6190476190476191 Hits@10 0.8412698412698413 MRR 0.5049316936217304 cur_rank 0 abs_cur_rank 0 total_num 62 500\n",
      "438 4166\n",
      "Hits@1 0.3125 Hits@3 0.625 Hits@10 0.84375 MRR 0.5126671359088909 cur_rank 0 abs_cur_rank 0 total_num 63 500\n",
      "438 2890\n",
      "Hits@1 0.3230769230769231 Hits@3 0.6307692307692307 Hits@10 0.8461538461538461 MRR 0.5201645645872156 cur_rank 0 abs_cur_rank 0 total_num 64 500\n",
      "438 1240\n",
      "Hits@1 0.3333333333333333 Hits@3 0.6363636363636364 Hits@10 0.8484848484848485 MRR 0.5274347984571063 cur_rank 0 abs_cur_rank 0 total_num 65 500\n",
      "438 2416\n",
      "Hits@1 0.3283582089552239 Hits@3 0.6268656716417911 Hits@10 0.835820895522388 MRR 0.5206287353671282 cur_rank 13 abs_cur_rank 15 total_num 66 500\n",
      "438 2512\n",
      "Hits@1 0.3382352941176471 Hits@3 0.6323529411764706 Hits@10 0.8382352941176471 MRR 0.5276783127881998 cur_rank 0 abs_cur_rank 2 total_num 67 500\n",
      "438 697\n",
      "Hits@1 0.3333333333333333 Hits@3 0.6376811594202898 Hits@10 0.8405797101449275 MRR 0.5272771778202548 cur_rank 1 abs_cur_rank 1 total_num 68 500\n",
      "438 2474\n",
      "Hits@1 0.32857142857142857 Hits@3 0.6285714285714286 Hits@10 0.8428571428571429 MRR 0.5233160752799655 cur_rank 3 abs_cur_rank 3 total_num 69 500\n",
      "438 439\n",
      "Hits@1 0.323943661971831 Hits@3 0.6197183098591549 Hits@10 0.8309859154929577 MRR 0.5163160712545249 cur_rank 37 abs_cur_rank 37 total_num 70 500\n",
      "438 203\n",
      "Hits@1 0.3333333333333333 Hits@3 0.625 Hits@10 0.8333333333333334 MRR 0.5230339035982121 cur_rank 0 abs_cur_rank 0 total_num 71 500\n",
      "438 376\n",
      "Hits@1 0.3424657534246575 Hits@3 0.6301369863013698 Hits@10 0.8356164383561644 MRR 0.5295676857407023 cur_rank 0 abs_cur_rank 0 total_num 72 500\n",
      "438 94\n",
      "Hits@1 0.35135135135135137 Hits@3 0.6351351351351351 Hits@10 0.8378378378378378 MRR 0.5359248791766388 cur_rank 0 abs_cur_rank 0 total_num 73 500\n",
      "438 1985\n",
      "Hits@1 0.3466666666666667 Hits@3 0.64 Hits@10 0.84 MRR 0.535445880787617 cur_rank 1 abs_cur_rank 1 total_num 74 500\n",
      "438 1340\n",
      "Hits@1 0.35526315789473684 Hits@3 0.6447368421052632 Hits@10 0.8421052631578947 MRR 0.5415584349877799 cur_rank 0 abs_cur_rank 0 total_num 75 500\n",
      "438 61\n",
      "Hits@1 0.36363636363636365 Hits@3 0.6493506493506493 Hits@10 0.8441558441558441 MRR 0.5475122215463801 cur_rank 0 abs_cur_rank 0 total_num 76 500\n",
      "438 175\n",
      "Hits@1 0.3717948717948718 Hits@3 0.6538461538461539 Hits@10 0.8461538461538461 MRR 0.5533133469111702 cur_rank 0 abs_cur_rank 0 total_num 77 500\n",
      "438 1155\n",
      "Hits@1 0.3670886075949367 Hits@3 0.6455696202531646 Hits@10 0.8354430379746836 MRR 0.5474601284807641 cur_rank 10 abs_cur_rank 10 total_num 78 500\n",
      "438 1316\n",
      "Hits@1 0.375 Hits@3 0.65 Hits@10 0.8375 MRR 0.5531168768747545 cur_rank 0 abs_cur_rank 0 total_num 79 500\n",
      "438 2487\n",
      "Hits@1 0.37037037037037035 Hits@3 0.6419753086419753 Hits@10 0.8395061728395061 MRR 0.548757409259017 cur_rank 4 abs_cur_rank 4 total_num 80 500\n",
      "438 1138\n",
      "Hits@1 0.36585365853658536 Hits@3 0.6463414634146342 Hits@10 0.8414634146341463 MRR 0.5481628067070776 cur_rank 1 abs_cur_rank 1 total_num 81 500\n",
      "438 834\n",
      "Hits@1 0.37349397590361444 Hits@3 0.6506024096385542 Hits@10 0.8433734939759037 MRR 0.5536066283130164 cur_rank 0 abs_cur_rank 0 total_num 82 500\n",
      "438 388\n",
      "Hits@1 0.36904761904761907 Hits@3 0.6428571428571429 Hits@10 0.8333333333333334 MRR 0.5470877886472133 cur_rank 165 abs_cur_rank 165 total_num 83 500\n",
      "438 2429\n",
      "Hits@1 0.36470588235294116 Hits@3 0.6352941176470588 Hits@10 0.8235294117647058 MRR 0.5407320418992283 cur_rank 145 abs_cur_rank 145 total_num 84 500\n",
      "438 353\n",
      "Hits@1 0.37209302325581395 Hits@3 0.6395348837209303 Hits@10 0.8255813953488372 MRR 0.5460723669934233 cur_rank 0 abs_cur_rank 0 total_num 85 500\n",
      "438 455\n",
      "Hits@1 0.367816091954023 Hits@3 0.632183908045977 Hits@10 0.8160919540229885 MRR 0.5406798464179021 cur_rank 12 abs_cur_rank 13 total_num 86 500\n",
      "438 598\n",
      "Hits@1 0.36363636363636365 Hits@3 0.625 Hits@10 0.8181818181818182 MRR 0.5368084845267896 cur_rank 4 abs_cur_rank 4 total_num 87 500\n",
      "438 1250\n",
      "Hits@1 0.3707865168539326 Hits@3 0.6292134831460674 Hits@10 0.8202247191011236 MRR 0.5420128835770505 cur_rank 0 abs_cur_rank 0 total_num 88 500\n",
      "438 1589\n",
      "Hits@1 0.36666666666666664 Hits@3 0.6222222222222222 Hits@10 0.8111111111111111 MRR 0.5360304862295516 cur_rank 277 abs_cur_rank 277 total_num 89 500\n",
      "438 940\n",
      "Hits@1 0.3626373626373626 Hits@3 0.6153846153846154 Hits@10 0.8021978021978022 MRR 0.5301933860394691 cur_rank 205 abs_cur_rank 205 total_num 90 500\n",
      "438 2508\n",
      "Hits@1 0.3695652173913043 Hits@3 0.6195652173913043 Hits@10 0.8043478260869565 MRR 0.5352999796694748 cur_rank 0 abs_cur_rank 0 total_num 91 500\n",
      "438 1630\n",
      "Hits@1 0.3655913978494624 Hits@3 0.6236559139784946 Hits@10 0.8064516129032258 MRR 0.5349204099956095 cur_rank 1 abs_cur_rank 1 total_num 92 500\n",
      "438 1594\n",
      "Hits@1 0.3617021276595745 Hits@3 0.6170212765957447 Hits@10 0.8085106382978723 MRR 0.5318893418041669 cur_rank 3 abs_cur_rank 5 total_num 93 500\n",
      "438 1290\n",
      "Hits@1 0.3684210526315789 Hits@3 0.6210526315789474 Hits@10 0.8105263157894737 MRR 0.5368168224167545 cur_rank 0 abs_cur_rank 0 total_num 94 500\n",
      "438 179\n",
      "Hits@1 0.375 Hits@3 0.625 Hits@10 0.8125 MRR 0.5416416471832467 cur_rank 0 abs_cur_rank 0 total_num 95 500\n",
      "438 797\n",
      "Hits@1 0.3711340206185567 Hits@3 0.6185567010309279 Hits@10 0.8041237113402062 MRR 0.5365263162375901 cur_rank 21 abs_cur_rank 21 total_num 96 500\n",
      "438 713\n",
      "Hits@1 0.37755102040816324 Hits@3 0.6224489795918368 Hits@10 0.8061224489795918 MRR 0.5412556395412881 cur_rank 0 abs_cur_rank 0 total_num 97 500\n",
      "438 245\n",
      "Hits@1 0.3838383838383838 Hits@3 0.6262626262626263 Hits@10 0.8080808080808081 MRR 0.545889420960063 cur_rank 0 abs_cur_rank 0 total_num 98 500\n",
      "438 3601\n",
      "Hits@1 0.39 Hits@3 0.63 Hits@10 0.81 MRR 0.5504305267504623 cur_rank 0 abs_cur_rank 0 total_num 99 500\n",
      "438 1221\n",
      "Hits@1 0.38613861386138615 Hits@3 0.6336633663366337 Hits@10 0.8118811881188119 MRR 0.5499312146044182 cur_rank 1 abs_cur_rank 1 total_num 100 500\n",
      "438 2982\n",
      "Hits@1 0.38235294117647056 Hits@3 0.6372549019607843 Hits@10 0.8137254901960784 MRR 0.5494416928926101 cur_rank 1 abs_cur_rank 1 total_num 101 500\n",
      "438 1358\n",
      "Hits@1 0.3786407766990291 Hits@3 0.6310679611650486 Hits@10 0.8155339805825242 MRR 0.5453208997577305 cur_rank 7 abs_cur_rank 7 total_num 102 500\n",
      "438 236\n",
      "Hits@1 0.38461538461538464 Hits@3 0.6346153846153846 Hits@10 0.8173076923076923 MRR 0.5496928141831369 cur_rank 0 abs_cur_rank 0 total_num 103 500\n",
      "438 1552\n",
      "Hits@1 0.38095238095238093 Hits@3 0.638095238095238 Hits@10 0.819047619047619 MRR 0.5492195492861546 cur_rank 1 abs_cur_rank 1 total_num 104 500\n",
      "438 379\n",
      "Hits@1 0.37735849056603776 Hits@3 0.6415094339622641 Hits@10 0.8207547169811321 MRR 0.5487552139155305 cur_rank 1 abs_cur_rank 1 total_num 105 500\n",
      "438 2639\n",
      "Hits@1 0.37383177570093457 Hits@3 0.6355140186915887 Hits@10 0.822429906542056 MRR 0.5451842929132047 cur_rank 5 abs_cur_rank 5 total_num 106 500\n",
      "438 267\n",
      "Hits@1 0.37962962962962965 Hits@3 0.6388888888888888 Hits@10 0.8240740740740741 MRR 0.5493955494603046 cur_rank 0 abs_cur_rank 0 total_num 107 500\n",
      "438 2029\n",
      "Hits@1 0.3761467889908257 Hits@3 0.6422018348623854 Hits@10 0.8256880733944955 MRR 0.5489423792817697 cur_rank 1 abs_cur_rank 2 total_num 108 500\n",
      "438 1296\n",
      "Hits@1 0.38181818181818183 Hits@3 0.6454545454545455 Hits@10 0.8272727272727273 MRR 0.5530429031064809 cur_rank 0 abs_cur_rank 0 total_num 109 500\n",
      "438 202\n",
      "Hits@1 0.3783783783783784 Hits@3 0.6396396396396397 Hits@10 0.8288288288288288 MRR 0.5490615356110271 cur_rank 8 abs_cur_rank 8 total_num 110 500\n",
      "438 5681\n",
      "Hits@1 0.375 Hits@3 0.6339285714285714 Hits@10 0.8303571428571429 MRR 0.5452752719002144 cur_rank 7 abs_cur_rank 7 total_num 111 500\n",
      "438 6\n",
      "Hits@1 0.37168141592920356 Hits@3 0.6283185840707964 Hits@10 0.831858407079646 MRR 0.541924753269829 cur_rank 5 abs_cur_rank 5 total_num 112 500\n",
      "438 1130\n",
      "Hits@1 0.3684210526315789 Hits@3 0.6228070175438597 Hits@10 0.8333333333333334 MRR 0.5384241601960336 cur_rank 6 abs_cur_rank 6 total_num 113 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438 1330\n",
      "Hits@1 0.3652173913043478 Hits@3 0.6260869565217392 Hits@10 0.8347826086956521 MRR 0.5380900370638941 cur_rank 1 abs_cur_rank 1 total_num 114 500\n",
      "438 837\n",
      "Hits@1 0.3620689655172414 Hits@3 0.6206896551724138 Hits@10 0.8275862068965517 MRR 0.5341697206524237 cur_rank 11 abs_cur_rank 11 total_num 115 500\n",
      "438 3040\n",
      "Hits@1 0.358974358974359 Hits@3 0.6153846153846154 Hits@10 0.8205128205128205 MRR 0.5296335386478461 cur_rank 290 abs_cur_rank 290 total_num 116 500\n",
      "438 268\n",
      "Hits@1 0.3559322033898305 Hits@3 0.6101694915254238 Hits@10 0.8135593220338984 MRR 0.5251994430356631 cur_rank 155 abs_cur_rank 156 total_num 117 500\n",
      "438 3371\n",
      "Hits@1 0.35294117647058826 Hits@3 0.6050420168067226 Hits@10 0.8151260504201681 MRR 0.5221865625619742 cur_rank 5 abs_cur_rank 5 total_num 118 500\n",
      "438 1830\n",
      "Hits@1 0.35 Hits@3 0.6 Hits@10 0.8083333333333333 MRR 0.5178650921098181 cur_rank 276 abs_cur_rank 277 total_num 119 500\n",
      "438 5573\n",
      "Hits@1 0.34710743801652894 Hits@3 0.5950413223140496 Hits@10 0.8016528925619835 MRR 0.5140443521382952 cur_rank 17 abs_cur_rank 18 total_num 120 500\n",
      "438 12\n",
      "Hits@1 0.3524590163934426 Hits@3 0.5983606557377049 Hits@10 0.8032786885245902 MRR 0.5180275951535551 cur_rank 0 abs_cur_rank 0 total_num 121 500\n",
      "438 1223\n",
      "Hits@1 0.34959349593495936 Hits@3 0.5934959349593496 Hits@10 0.8048780487804879 MRR 0.5146289968189733 cur_rank 9 abs_cur_rank 9 total_num 122 500\n",
      "438 1666\n",
      "Hits@1 0.3548387096774194 Hits@3 0.5967741935483871 Hits@10 0.8064516129032258 MRR 0.5185432791026913 cur_rank 0 abs_cur_rank 0 total_num 123 500\n",
      "438 2691\n",
      "Hits@1 0.36 Hits@3 0.6 Hits@10 0.808 MRR 0.5223949328698697 cur_rank 0 abs_cur_rank 0 total_num 124 500\n",
      "438 753\n",
      "Hits@1 0.35714285714285715 Hits@3 0.5952380952380952 Hits@10 0.8095238095238095 MRR 0.5202330683232835 cur_rank 3 abs_cur_rank 3 total_num 125 500\n",
      "438 535\n",
      "Hits@1 0.3543307086614173 Hits@3 0.5905511811023622 Hits@10 0.8110236220472441 MRR 0.5181052488876671 cur_rank 3 abs_cur_rank 3 total_num 126 500\n",
      "438 492\n",
      "Hits@1 0.359375 Hits@3 0.59375 Hits@10 0.8125 MRR 0.5218700516307322 cur_rank 0 abs_cur_rank 0 total_num 127 500\n",
      "438 2102\n",
      "Hits@1 0.3643410852713178 Hits@3 0.5968992248062015 Hits@10 0.813953488372093 MRR 0.5255764853390211 cur_rank 0 abs_cur_rank 0 total_num 128 500\n",
      "438 104\n",
      "Hits@1 0.36153846153846153 Hits@3 0.5923076923076923 Hits@10 0.8153846153846154 MRR 0.5234566662210286 cur_rank 3 abs_cur_rank 4 total_num 129 500\n",
      "438 1182\n",
      "Hits@1 0.35877862595419846 Hits@3 0.5954198473282443 Hits@10 0.816793893129771 MRR 0.5232776077002574 cur_rank 1 abs_cur_rank 1 total_num 130 500\n",
      "438 284\n",
      "Hits@1 0.36363636363636365 Hits@3 0.5984848484848485 Hits@10 0.8181818181818182 MRR 0.5268891409752554 cur_rank 0 abs_cur_rank 0 total_num 131 500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1h/rcbfn0354hv_dhq9m90rp5kc0000gn/T/ipykernel_56937/392813441.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0ms_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mscore_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelation_ranking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hop_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2relation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#[... [score, r], ...]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/1h/rcbfn0354hv_dhq9m90rp5kc0000gn/T/ipykernel_56937/958173035.py\u001b[0m in \u001b[0;36mrelation_ranking\u001b[0;34m(s, t, lower_bd, upper_bd, one_hop, id2relation, model)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         result, length_dict = Class_2.obtain_paths('target_specified', \n\u001b[0m\u001b[1;32m      8\u001b[0m                                                    s, t, lower_bd, upper_bd, one_hop)\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/1h/rcbfn0354hv_dhq9m90rp5kc0000gn/T/ipykernel_56937/3003381867.py\u001b[0m in \u001b[0;36mobtain_paths\u001b[0;34m(self, mode, s, t_input, lower_bd, upper_bd, one_hop)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mcount_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mhelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirect_nb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower_bd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_bd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/1h/rcbfn0354hv_dhq9m90rp5kc0000gn/T/ipykernel_56937/3003381867.py\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(node, path, on_path_en, res, direct_nb, lower_bd, upper_bd, one_hop, length_dict, count_dict)\u001b[0m\n\u001b[1;32m    124\u001b[0m                             \u001b[0mcount_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                             helper(t, path + [r], on_path_en.union({t}), res, direct_nb, \n\u001b[0m\u001b[1;32m    127\u001b[0m                                    lower_bd, upper_bd, one_hop, length_dict, count_dict)\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/1h/rcbfn0354hv_dhq9m90rp5kc0000gn/T/ipykernel_56937/3003381867.py\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(node, path, on_path_en, res, direct_nb, lower_bd, upper_bd, one_hop, length_dict, count_dict)\u001b[0m\n\u001b[1;32m    124\u001b[0m                             \u001b[0mcount_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                             helper(t, path + [r], on_path_en.union({t}), res, direct_nb, \n\u001b[0m\u001b[1;32m    127\u001b[0m                                    lower_bd, upper_bd, one_hop, length_dict, count_dict)\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/1h/rcbfn0354hv_dhq9m90rp5kc0000gn/T/ipykernel_56937/3003381867.py\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(node, path, on_path_en, res, direct_nb, lower_bd, upper_bd, one_hop, length_dict, count_dict)\u001b[0m\n\u001b[1;32m    124\u001b[0m                             \u001b[0mcount_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                             helper(t, path + [r], on_path_en.union({t}), res, direct_nb, \n\u001b[0m\u001b[1;32m    127\u001b[0m                                    lower_bd, upper_bd, one_hop, length_dict, count_dict)\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/1h/rcbfn0354hv_dhq9m90rp5kc0000gn/T/ipykernel_56937/3003381867.py\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(node, path, on_path_en, res, direct_nb, lower_bd, upper_bd, one_hop, length_dict, count_dict)\u001b[0m\n\u001b[1;32m    124\u001b[0m                             \u001b[0mcount_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                             helper(t, path + [r], on_path_en.union({t}), res, direct_nb, \n\u001b[0m\u001b[1;32m    127\u001b[0m                                    lower_bd, upper_bd, one_hop, length_dict, count_dict)\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/1h/rcbfn0354hv_dhq9m90rp5kc0000gn/T/ipykernel_56937/3003381867.py\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(node, path, on_path_en, res, direct_nb, lower_bd, upper_bd, one_hop, length_dict, count_dict)\u001b[0m\n\u001b[1;32m    124\u001b[0m                             \u001b[0mcount_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                             helper(t, path + [r], on_path_en.union({t}), res, direct_nb, \n\u001b[0m\u001b[1;32m    127\u001b[0m                                    lower_bd, upper_bd, one_hop, length_dict, count_dict)\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/1h/rcbfn0354hv_dhq9m90rp5kc0000gn/T/ipykernel_56937/3003381867.py\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(node, path, on_path_en, res, direct_nb, lower_bd, upper_bd, one_hop, length_dict, count_dict)\u001b[0m\n\u001b[1;32m    124\u001b[0m                             \u001b[0mcount_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                             helper(t, path + [r], on_path_en.union({t}), res, direct_nb, \n\u001b[0m\u001b[1;32m    127\u001b[0m                                    lower_bd, upper_bd, one_hop, length_dict, count_dict)\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/1h/rcbfn0354hv_dhq9m90rp5kc0000gn/T/ipykernel_56937/3003381867.py\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(node, path, on_path_en, res, direct_nb, lower_bd, upper_bd, one_hop, length_dict, count_dict)\u001b[0m\n\u001b[1;32m    124\u001b[0m                             \u001b[0mcount_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                             helper(t, path + [r], on_path_en.union({t}), res, direct_nb, \n\u001b[0m\u001b[1;32m    127\u001b[0m                                    lower_bd, upper_bd, one_hop, length_dict, count_dict)\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/1h/rcbfn0354hv_dhq9m90rp5kc0000gn/T/ipykernel_56937/3003381867.py\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(node, path, on_path_en, res, direct_nb, lower_bd, upper_bd, one_hop, length_dict, count_dict)\u001b[0m\n\u001b[1;32m    118\u001b[0m                             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mon_path_en\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/python39_tensorfl/lib/python3.9/random.py\u001b[0m in \u001b[0;36mchoice\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;34m\"\"\"Choose a random element from a non-empty sequence.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;31m# raises IndexError if seq is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "#obtain the precision-recall area under curve (AUC-PR)##\n",
    "\n",
    "#randomly select 10% of the triples\n",
    "selected = random.sample(list(data_test), min(len(data_test), 500))\n",
    "\n",
    "random.shuffle(selected)\n",
    "\n",
    "###Hit at 1#############################\n",
    "#generate the negative samples by randomly replace relation with all the other relaiton\n",
    "Hits_at_1 = 0\n",
    "Hits_at_3 = 0\n",
    "Hits_at_10 = 0\n",
    "MRR_raw = 0.\n",
    "\n",
    "for i in range(len(selected)):\n",
    "    \n",
    "    s_true, r_true, t_true = selected[i][0], selected[i][1], selected[i][2]\n",
    "    \n",
    "    score_dict = relation_ranking(s_true, t_true, 2, 10, one_hop_ind, id2relation, model)\n",
    "    \n",
    "    #[... [score, r], ...]\n",
    "    temp_list = list()\n",
    "    \n",
    "    for r in id2relation:\n",
    "        \n",
    "        if r in score_dict:\n",
    "            \n",
    "            temp_list.append([score_dict[r], r])\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            temp_list.append([0.0, r])\n",
    "        \n",
    "    sorted_list = sorted(temp_list, key = lambda x: x[0], reverse=True)\n",
    "    \n",
    "    p = 0\n",
    "    exist_tri = 0\n",
    "    \n",
    "    while p < len(sorted_list) and sorted_list[p][1] != r_true:\n",
    "        \n",
    "        #moreover, we want to remove existing triples\n",
    "        if ((s_true, sorted_list[p][1], t_true) in data_test) or (\n",
    "              (s_true, sorted_list[p][1], t_true) in data_valid) or (\n",
    "              (s_true, sorted_list[p][1], t_true) in data_ind):\n",
    "            \n",
    "            exist_tri += 1\n",
    "            \n",
    "        p += 1\n",
    "    \n",
    "    if p - exist_tri == 0:\n",
    "        \n",
    "        Hits_at_1 += 1\n",
    "        \n",
    "    if p - exist_tri < 3:\n",
    "        \n",
    "        Hits_at_3 += 1\n",
    "        \n",
    "    if p - exist_tri < 10:\n",
    "        \n",
    "        Hits_at_10 += 1\n",
    "        \n",
    "    MRR_raw += 1./float(p - exist_tri + 1.) \n",
    "        \n",
    "    print('Hits@1', Hits_at_1/(i+1),\n",
    "          'Hits@3', Hits_at_3/(i+1),\n",
    "          'Hits@10', Hits_at_10/(i+1),\n",
    "          'MRR', MRR_raw/(i+1),\n",
    "          'cur_rank', p - exist_tri,\n",
    "          'abs_cur_rank', p,\n",
    "          'total_num', i, len(selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f1c288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ce01b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de9ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
