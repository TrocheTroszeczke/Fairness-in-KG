{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea7a7dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'nell_v4'\n",
    "model_id = 'main_5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54797cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#difine the names for saving\n",
    "model_name = 'Model_' + model_id + '_' + data_name\n",
    "ids_name = 'IDs_' + model_id + '_' + data_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fba371e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import opensmile\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c098e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadKG:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.x = 'Hello'\n",
    "        \n",
    "    def load_train_data(self, data_path, one_hop, data, s_t_r, entity2id, id2entity,\n",
    "                     relation2id, id2relation):\n",
    "        \n",
    "        data_ = set()\n",
    "    \n",
    "        ####load the train, valid and test set##########\n",
    "        with open (data_path, 'r') as f:\n",
    "            \n",
    "            data_ini = f.readlines()\n",
    "                        \n",
    "            for i in range(len(data_ini)):\n",
    "            \n",
    "                x = data_ini[i].split()\n",
    "                \n",
    "                x_ = tuple(x)\n",
    "                \n",
    "                data_.add(x_)\n",
    "        \n",
    "        ####relation dict#################\n",
    "        index = len(relation2id)\n",
    "     \n",
    "        for key in data_:\n",
    "            \n",
    "            if key[1] not in relation2id:\n",
    "                \n",
    "                relation = key[1]\n",
    "                \n",
    "                relation2id[relation] = index\n",
    "                \n",
    "                id2relation[index] = relation\n",
    "                \n",
    "                index += 1\n",
    "                \n",
    "                #the inverse relation\n",
    "                iv_r = '_inverse_' + relation\n",
    "                \n",
    "                relation2id[iv_r] = index\n",
    "                \n",
    "                id2relation[index] = iv_r\n",
    "                \n",
    "                index += 1\n",
    "        \n",
    "        #get the id of the inverse relation, by above definition, initial relation has \n",
    "        #always even id, while inverse relation has always odd id.\n",
    "        def inverse_r(r):\n",
    "            \n",
    "            if r % 2 == 0: #initial relation\n",
    "                \n",
    "                iv_r = r + 1\n",
    "            \n",
    "            else: #inverse relation\n",
    "                \n",
    "                iv_r = r - 1\n",
    "            \n",
    "            return(iv_r)\n",
    "        \n",
    "        ####entity dict###################\n",
    "        index = len(entity2id)\n",
    "        \n",
    "        for key in data_:\n",
    "            \n",
    "            source, target = key[0], key[2]\n",
    "            \n",
    "            if source not in entity2id:\n",
    "                                \n",
    "                entity2id[source] = index\n",
    "                \n",
    "                id2entity[index] = source\n",
    "                \n",
    "                index += 1\n",
    "            \n",
    "            if target not in entity2id:\n",
    "                \n",
    "                entity2id[target] = index\n",
    "                \n",
    "                id2entity[index] = target\n",
    "                \n",
    "                index += 1\n",
    "                \n",
    "        #create the set of triples using id instead of string        \n",
    "        for ele in data_:\n",
    "            \n",
    "            s = entity2id[ele[0]]\n",
    "            \n",
    "            r = relation2id[ele[1]]\n",
    "            \n",
    "            t = entity2id[ele[2]]\n",
    "            \n",
    "            if (s,r,t) not in data:\n",
    "                \n",
    "                data.add((s,r,t))\n",
    "            \n",
    "            s_t_r[(s,t)].add(r)\n",
    "            \n",
    "            if s not in one_hop:\n",
    "                \n",
    "                one_hop[s] = dict()\n",
    "            \n",
    "            if r not in one_hop[s]:\n",
    "                \n",
    "                one_hop[s][r] = set()\n",
    "            \n",
    "            one_hop[s][r].add(t)\n",
    "            \n",
    "            if t not in one_hop:\n",
    "                \n",
    "                one_hop[t] = dict()\n",
    "            \n",
    "            r_inv = inverse_r(r)\n",
    "            \n",
    "            s_t_r[(t,s)].add(r_inv)\n",
    "            \n",
    "            if r_inv not in one_hop[t]:\n",
    "                \n",
    "                one_hop[t][r_inv] = set()\n",
    "            \n",
    "            one_hop[t][r_inv].add(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6cd0f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObtainPathsByDynamicProgramming:\n",
    "\n",
    "    def __init__(self, size_bd=50, threshold=100000):\n",
    "                \n",
    "        self.size_bd = size_bd\n",
    "        \n",
    "        self.threshold = threshold\n",
    "    \n",
    "    '''\n",
    "    Given an entity s, here is the function to find:\n",
    "      1. any else entity t that is directely connected to s\n",
    "      2. most of the paths from s to each t with length L\n",
    "    \n",
    "    One may refer to LeetCode Problem 797 for details:\n",
    "        https://leetcode.com/problems/all-paths-from-source-to-target/\n",
    "    '''\n",
    "    def obtain_paths(self, mode, s, t_input, lower_bd, upper_bd, one_hop):\n",
    "\n",
    "        if type(lower_bd) != type(1) or lower_bd < 1:\n",
    "            \n",
    "            raise TypeError(\"!!! invalid lower bound setting, must >= 1 !!!\")\n",
    "            \n",
    "        if type(upper_bd) != type(1) or upper_bd < 1:\n",
    "            \n",
    "            raise TypeError(\"!!! invalid upper bound setting, must >= 1 !!!\")\n",
    "            \n",
    "        if lower_bd > upper_bd:\n",
    "            \n",
    "            raise TypeError(\"!!! lower bound must not exced upper bound !!!\")\n",
    "            \n",
    "        if s not in one_hop:\n",
    "            \n",
    "            raise ValueError('!!! entity not in one_hop. Please work on active entities for validation')\n",
    "        \n",
    "        #here is the result dict. Its key is each entity t that is directly connected to s\n",
    "        #The value of each t is a set containing the paths from s to t\n",
    "        #These paths can be either the direct connection r, or a multi-hop path\n",
    "        res = defaultdict(set)\n",
    "        \n",
    "        #direct_nb contains all the direct neighbour of s\n",
    "        direct_nb = set()\n",
    "        \n",
    "        if mode == 'direct_neighbour':\n",
    "        \n",
    "            for r in one_hop[s]:\n",
    "            \n",
    "                for t in one_hop[s][r]:\n",
    "                \n",
    "                    direct_nb.add(t)\n",
    "                    \n",
    "        elif mode == 'target_specified':\n",
    "            \n",
    "            direct_nb.add(t_input)\n",
    "            \n",
    "        elif mode == 'any_target':\n",
    "            \n",
    "            for s_any in one_hop:\n",
    "                \n",
    "                direct_nb.add(s_any)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            raise ValueError('not a valid mode')\n",
    "        \n",
    "        '''\n",
    "        We use recursion to find the paths\n",
    "        On current node with the path [r1, ..., rk] and on-path entities {e1, ..., ek-1, node}\n",
    "        from s to this node, we further find the direct neighbor t' of this node. \n",
    "        If t' is not a on-path entity (not among e1,...ek-1), we recursively proceed to t' \n",
    "        '''\n",
    "        def helper(node, path, on_path_en, res, direct_nb, lower_bd, upper_bd, one_hop, length_dict, count_dict):\n",
    "            \n",
    "            #when the current path is within lower_bd and upper_bd and its corresponding\n",
    "            #length still within the size_bd and its tail node is within the note dict, \n",
    "            #we will then intend to add this path\n",
    "            if (len(path) >= lower_bd) and (len(path) <= upper_bd) and (\n",
    "                node in direct_nb) and (length_dict[len(path)] < self.size_bd):\n",
    "                \n",
    "                #if this path already exists between the source entity and the current target node,\n",
    "                #we will not count it.\n",
    "                #here is an interesting situation: this path may exist between s and some other node t,\n",
    "                #however, it does not exist between s and this node t. Then, we still count it: length_dict[len(path)] += 1\n",
    "                #That is, each path may be counted for multiple times.\n",
    "                #We count how many paths we \"actually\" found between entity pairs\n",
    "                #Same type of path between different entity pairs are count separately.\n",
    "                if tuple(path) not in res[node]:\n",
    "                \n",
    "                    res[node].add(tuple(path))\n",
    "                \n",
    "                    length_dict[len(path)] += 1\n",
    "                \n",
    "            #For some rare entities, we may face such a case: so many paths are evaluated,\n",
    "            #but no entities on the paths are direct neighbors of the rare entity.\n",
    "            #In this case, the recursion cannot be bounded and stoped by the size threshold.\n",
    "            #In order to cure this, we count how many times the recursion happens on a specific length, using the count_dict.\n",
    "            #Its key is length, value counts the recursion occurred to that length. \n",
    "            #The recursion is forced to stop for that length (and hence for longer lengths) once reach the threshold.\n",
    "            if (len(path) < upper_bd) and (length_dict[len(path) + 1] < self.size_bd) and (\n",
    "                count_dict[len(path)] <= self.threshold):\n",
    "                \n",
    "                #we randomly shuffle relation r so that the reading in order is not fixed\n",
    "                temp_list = list()\n",
    "                \n",
    "                for r in one_hop[node]:\n",
    "                    \n",
    "                    temp_list.append(r)\n",
    "                \n",
    "                for i_0 in range(len(temp_list)):\n",
    "                    \n",
    "                    if count_dict[len(path)] > self.threshold:\n",
    "                        break\n",
    "                    \n",
    "                    r = random.choice(temp_list)\n",
    "                    \n",
    "                    for i_1 in range(len(one_hop[node][r])):\n",
    "                        \n",
    "                        if count_dict[len(path)] > self.threshold:\n",
    "                            break\n",
    "                        \n",
    "                        t = random.choice(list(one_hop[node][r]))\n",
    "                        \n",
    "                        if t not in on_path_en:\n",
    "                                \n",
    "                            count_dict[len(path)] += 1\n",
    "\n",
    "                            helper(t, path + [r], on_path_en.union({t}), res, direct_nb, \n",
    "                                   lower_bd, upper_bd, one_hop, length_dict, count_dict)\n",
    "        \n",
    "        length_dict = defaultdict(int)\n",
    "        count_dict = defaultdict(int)\n",
    "        \n",
    "        helper(s, [], {s}, res, direct_nb, lower_bd, upper_bd, one_hop, length_dict, count_dict)\n",
    "        \n",
    "        return(res, length_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecaf24ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/' + data_name + '/train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5718867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the classes\n",
    "Class_1 = LoadKG()\n",
    "Class_2 = ObtainPathsByDynamicProgramming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57c472f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the dictionaries and sets for load KG\n",
    "one_hop = dict() \n",
    "data = set()\n",
    "s_t_r = defaultdict(set)\n",
    "entity2id = dict()\n",
    "id2entity = dict()\n",
    "relation2id = dict()\n",
    "id2relation = dict()\n",
    "\n",
    "#fill in the sets and dicts\n",
    "Class_1.load_train_data(train_path, one_hop, data, s_t_r,\n",
    "                        entity2id, id2entity, relation2id, id2relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee8babf",
   "metadata": {},
   "source": [
    "### Build the deep neural network structure\n",
    "\n",
    "We use biLSTM to train on the input path embedding sequence to predict the output embedding or the relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68239c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-18 15:14:42.168015: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Input layer, using integer to represent each relation type\n",
    "#note that inputs_path is the path inputs, while inputs_out_re is the output relation inputs\n",
    "fst_path = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "scd_path = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "#the relation input layer (for output embedding)\n",
    "id_rela = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "# Embed each integer in a 300-dimensional vector as input,\n",
    "# note that we add another \"space holder\" embedding, \n",
    "# which hold the spaces if the initial length of two paths are not the same\n",
    "in_embd_var = layers.Embedding(len(relation2id)+1, 300)\n",
    "\n",
    "# Obtain the embedding\n",
    "fst_p_embd = in_embd_var(fst_path)\n",
    "scd_p_embd = in_embd_var(scd_path)\n",
    "\n",
    "# Embed each integer in a 300-dimensional vector as output\n",
    "rela_embd = layers.Embedding(len(relation2id)+1, 300)(id_rela)\n",
    "\n",
    "#add 2 layer bi-directional LSTM\n",
    "lstm_layer_1 = layers.Bidirectional(layers.LSTM(150, return_sequences=True))\n",
    "lstm_layer_2 = layers.Bidirectional(layers.LSTM(150, return_sequences=True))\n",
    "\n",
    "#first LSTM layer\n",
    "fst_lstm_mid = lstm_layer_1(fst_p_embd)\n",
    "scd_lstm_mid = lstm_layer_1(scd_p_embd)\n",
    "\n",
    "#second LSTM layer\n",
    "fst_lstm_out = lstm_layer_2(fst_lstm_mid)\n",
    "scd_lstm_out = lstm_layer_2(scd_lstm_mid)\n",
    "\n",
    "###########################################\n",
    "####apply the attention mechanism##########\n",
    "#first expand the dimention at the end to get ready for conv2D: (Batch,time,300,1)\n",
    "fst_exp_dim = tf.expand_dims(fst_lstm_out, axis=-1)\n",
    "scd_exp_dim = tf.expand_dims(scd_lstm_out, axis=-1)\n",
    "\n",
    "#define the attention layer using convolutional 2D: output is\n",
    "att_layer_conv2D = layers.Conv2D(100, (1, 300), padding='valid', activation='relu', \n",
    "                   input_shape=(None, 300), data_format='channels_last')\n",
    "\n",
    "#shape: (Batch,Time,1,100)\n",
    "fst_att_mid = att_layer_conv2D(fst_exp_dim)\n",
    "scd_att_mid = att_layer_conv2D(scd_exp_dim)\n",
    "\n",
    "#squeeze out the dim 1 to become: (Batch, Time, 100)\n",
    "fst_squez = tf.squeeze(fst_att_mid, 2)\n",
    "scd_squez = tf.squeeze(scd_att_mid, 2)\n",
    "\n",
    "#expand the dimention again to become (Batch, Time, 100, 1)\n",
    "fst_exp_dim_2 = tf.expand_dims(fst_squez, axis=-1)\n",
    "scd_exp_dim_2 = tf.expand_dims(scd_squez, axis=-1)\n",
    "\n",
    "#obtain the attention score for each time step by another conv2D layer\n",
    "att_layer_conv2D_2 = layers.Conv2D(1, (1, 100), padding='valid', activation='relu', \n",
    "                     input_shape=(None, 100), data_format='channels_last')\n",
    "\n",
    "#obtain (Batch, Time, 1, 1)\n",
    "fst_mid_score = att_layer_conv2D_2(fst_exp_dim_2)\n",
    "scd_mid_score = att_layer_conv2D_2(scd_exp_dim_2)\n",
    "\n",
    "#squeeze again to obtain (Batch, Time, 1)\n",
    "fst_squez_2 = tf.squeeze(fst_mid_score, -1)\n",
    "scd_squez_2 = tf.squeeze(scd_mid_score, -1)\n",
    "\n",
    "#softmax the attention score\n",
    "softmax_l = layers.Softmax(1) #define softmax\n",
    "\n",
    "fst_att_score = softmax_l(fst_squez_2)\n",
    "scd_att_score = softmax_l(scd_squez_2)\n",
    "\n",
    "#multiply the attention score to lstm output\n",
    "fst_att_befsum = layers.Multiply()([fst_lstm_out, fst_att_score])\n",
    "scd_att_befsum = layers.Multiply()([scd_lstm_out, scd_att_score])\n",
    "\n",
    "#sum over time dimension to complete the attention: (Batch, 300)\n",
    "fst_att_out = tf.reduce_sum(fst_att_befsum, axis=1)\n",
    "scd_att_out = tf.reduce_sum(scd_att_befsum, axis=1)\n",
    "######################################\n",
    "\n",
    "#concatenate the output vector from both siamese tunnel: (Batch, 600)\n",
    "path_concat = layers.concatenate([fst_att_out, scd_att_out], axis=-1)\n",
    "\n",
    "#multiply into output embd size by dense layer: (Batch, 300)\n",
    "path_out_vect = layers.Dense(300, activation='tanh')(path_concat)\n",
    "\n",
    "#remove the time dimension from the output embd since there is only one step\n",
    "rela_out_embd = tf.reduce_sum(rela_embd, axis=1)\n",
    "\n",
    "#concatenate the lstm output and output embd\n",
    "concat = layers.concatenate([path_out_vect, rela_out_embd], axis=-1)\n",
    "\n",
    "#add the dense layer\n",
    "dense_1 = layers.Dense(32, activation='relu')(concat)\n",
    "batch_norm = layers.BatchNormalization()(dense_1)\n",
    "dropout = layers.Dropout(0.25)(batch_norm)\n",
    "\n",
    "#final layer\n",
    "final_out = layers.Dense(1, activation='sigmoid')(dropout)\n",
    "\n",
    "#put together the model\n",
    "model = keras.Model([fst_path, scd_path, id_rela], final_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5cd4009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config the Adam optimizer \n",
    "opt = keras.optimizers.Adam(learning_rate=0.0005, decay=1e-6)\n",
    "\n",
    "#compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fd204f",
   "metadata": {},
   "source": [
    "### Build the batches\n",
    "We build each big-batch for each path combination with length (i,j). Then, we iteratively train the siamese network on different big-batches. The length of each big-batch is N.\n",
    "\n",
    "To be specific:\n",
    "* If we allow the length difference between two paths in a combination to be d, then the combination with path length i and path length j, denoted as (i,j), will be like (2,2), (2,3), (2,4), (3,3), (3,4), (3,5), ... \n",
    "* We will first build all the big-batches before fitting the NN model. \n",
    "* That is, we will perform the ObtainPathsByDynamicProgramming class function for some randomly chosen source entities. Then, for each target entity, we will further have two for loops:\n",
    "* for path_1 in all the \n",
    "* Do this until all the slots in all big-batchs are filled.\n",
    "* In every epoch, big-batchs will be re-filled.\n",
    "\n",
    "Then, in the training, we will use negative sampling: In each batch (actual batch, not the big-batch), we will include K true output relation embeddings and K random selected output relation embeddings. The true label is [1,0], while the false label is [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bb3ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to build all the big batches\n",
    "def build_big_batches(holder_len, lower_bd, upper_bd, Class_2, one_hop, s_t_r,\n",
    "                      x_p_list, x_r_list, y_list,\n",
    "                      relation2id, entity2id, id2relation, id2entity):\n",
    "    \n",
    "    if holder_len % 10 != 0:\n",
    "        raise ValueError('We would like to take 10X as a big-batch size')\n",
    "    \n",
    "    #the set of all relation IDs\n",
    "    relation_id_set = set()\n",
    "    for i in range(len(id2relation)):\n",
    "        \n",
    "        if i not in id2relation:\n",
    "            raise ValueError('error when generaing id2relation')\n",
    "        \n",
    "        relation_id_set.add(i)\n",
    "    \n",
    "    num_r = len(id2relation)\n",
    "    \n",
    "    #count how many appending has performed\n",
    "    count = 0\n",
    "\n",
    "    #in case not all entities in entity2id are in one_hop, \n",
    "    #so we need to find out who are indeed in\n",
    "    existing_ids = set()\n",
    "    \n",
    "    for s_1 in one_hop:\n",
    "        existing_ids.add(s_1)\n",
    "        \n",
    "    existing_ids = list(existing_ids)\n",
    "    \n",
    "    carry_on = True\n",
    "    \n",
    "    while carry_on:\n",
    "\n",
    "        #obtain paths by dynamic programming\n",
    "        source_id = random.choice(existing_ids)\n",
    "\n",
    "        result, length_dict = Class_2.obtain_paths('direct_neighbour', source_id, \n",
    "                                                   'not_specified', lower_bd, upper_bd, one_hop)\n",
    "        \n",
    "        #We want to increase the diversity of paths and targets.\n",
    "        #So we abandon one sub-graph from a source_id, if we sampled more than K1 path pairs\n",
    "        #Note that we mean \"sampled\", not \"appended\"! \n",
    "        #We do not care whether the pair is actually appended.\n",
    "        threshold_0 = 1000\n",
    "        count_0 = 0\n",
    "        \n",
    "        for target_id in result:\n",
    "\n",
    "            if (not carry_on) or (count_0 > threshold_0):\n",
    "                break\n",
    "            \n",
    "            #we want to make sure s, t are indeed directly connected, \n",
    "            #otherwise there is no relation for positive sample\n",
    "            #also, we want to make sure s and t and not connected by all relations, \n",
    "            #although this situation is rare. \n",
    "            #But in that case, there is no relation for negative samples\n",
    "            #Also, we want at least two different paths here between s and t\n",
    "            if ((source_id, target_id) in s_t_r) and (\n",
    "                len(s_t_r[(source_id, target_id)]) < len(id2relation)) and (\n",
    "                len(result[target_id]) >= 2):\n",
    "                \n",
    "                dir_r = list(s_t_r[(source_id, target_id)])\n",
    "                \n",
    "                non_dir_r = list(relation_id_set.difference(dir_r))\n",
    "                \n",
    "                if len(dir_r) <= 0:\n",
    "                    \n",
    "                    raise ValueError('errors when creating s_t_r !!')\n",
    "                    \n",
    "                temp_path_list = list(result[target_id])\n",
    "                    \n",
    "                #futhermore, we will abandon one targed_id if we sampled more than K2 times\n",
    "                threshold_1 = 50\n",
    "                count_1 = 0\n",
    "                \n",
    "                while count_1 <= threshold_1 and count_0 <= threshold_0:\n",
    "                \n",
    "                    temp_pair = random.sample(temp_path_list,2)\n",
    "                    \n",
    "                    path_1, path_2 = temp_pair[0], temp_pair[1]\n",
    "\n",
    "                    #decide which path is shorter and which is longer\n",
    "                    if len(path_1) <= len(path_2):\n",
    "\n",
    "                        path_s, path_l = path_1, path_2\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        path_s, path_l = path_2, path_1                            \n",
    "\n",
    "                    if (len(path_s) < lower_bd) or (len(path_l) > upper_bd):\n",
    "\n",
    "                        raise ValueError('something wrong with the path finding')\n",
    "\n",
    "                    #proceed when the entire length not yet reached,\n",
    "                    #and whether this path pair is new, and whether the two paths are different\n",
    "                    #But it is optional to require the path to be new. \n",
    "                    #We may remove this requirment, especially for short paths\n",
    "                    '''remember to cancel the comment below when using path_comb'''\n",
    "                    if (carry_on) and (path_s != path_l):\n",
    "\n",
    "                        #we always add one positive and one negative situation together,\n",
    "                        #hence, the length of list should always be even.\n",
    "                        #also we want to make sure the length of lists coincide\n",
    "                        if (len(x_p_list['s']) != len(y_list)) or (\n",
    "                            len(x_p_list['s']) != len(x_p_list['l'])) or (\n",
    "                            len(y_list) != len(x_r_list)) or (\n",
    "                            len(y_list) % 2 != 0):\n",
    "\n",
    "                            raise ValueError('error when building big batches: length error')\n",
    "\n",
    "                        #####positive#####################\n",
    "                        #we randomly choose one direction relation as the target relation\n",
    "                        relation_id = random.choice(dir_r)\n",
    "\n",
    "                        #append the paths: note that we add the space holder id at the end\n",
    "                        #of the shorter path\n",
    "                        x_p_list['s'].append(list(path_s) + [num_r]*abs(len(path_s)-upper_bd))\n",
    "                        x_p_list['l'].append(list(path_l) + [num_r]*abs(len(path_l)-upper_bd))\n",
    "\n",
    "                        #append relation\n",
    "                        x_r_list.append([relation_id])\n",
    "                        y_list.append(1.)\n",
    "\n",
    "                        #####negative#####################\n",
    "                        relation_id = random.choice(non_dir_r)\n",
    "\n",
    "                        #append the paths: note that we add the space holder id at the end\n",
    "                        #of the shorter path\n",
    "                        x_p_list['s'].append(list(path_s) + [num_r]*abs(len(path_s)-upper_bd))\n",
    "                        x_p_list['l'].append(list(path_l) + [num_r]*abs(len(path_l)-upper_bd))\n",
    "\n",
    "                        #append relation\n",
    "                        x_r_list.append([relation_id])\n",
    "                        y_list.append(0.)\n",
    "\n",
    "                        ######add to path combinations#####\n",
    "                        #here is the tricky part: we have to add both (path_s, path_l)\n",
    "                        #and (path_l, path_s). This is because when the length are the same\n",
    "                        #adding only one situation won't guarantee that \n",
    "                        #the same path with different order is also considered.\n",
    "                        #in other words: path combination don't have order, but our dict does.\n",
    "                        #so we have to add both situations.\n",
    "                        '''remember to cancel the comment here when using path_comb'''\n",
    "                        #path_comb[(len(path_s), len(path_l))].add((path_s, path_l))\n",
    "                        #path_comb[(len(path_s), len(path_l))].add((path_l, path_s))\n",
    "\n",
    "                        count += 2\n",
    "\n",
    "                        if count % 20000 == 0:\n",
    "                            print('generating big-batches', count, holder_len)\n",
    "\n",
    "                    if len(y_list) >= holder_len:\n",
    "\n",
    "                        carry_on = False\n",
    "                        \n",
    "                    count_1 += 1\n",
    "                    count_0 += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851e0ac2",
   "metadata": {},
   "source": [
    "### Start Training: load the KG and call classes\n",
    "\n",
    "Here, we use the validation set to see the training efficiency. That is, we use the validation to check whether the true relation between entities can be predicted by paths.\n",
    "\n",
    "The trick is: in validation, we have to use the same relation ID and entity ID as in the training. But we don't want to use the links in training anymore. That is, in validation, we want to use (and update if necessary) entity2id, id2entity, relation2id and id2relation. But we want to use new one_hop, data, data_ and s_t_r for validation set. Then, path-finding will also be based on new one_hop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28be7a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Model_main_5_nell_v4'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17c02a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IDs_main_5_nell_v4'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7f57c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, we save the relation and ids\n",
    "Dict = dict()\n",
    "Dict['one_hop'] = one_hop\n",
    "Dict['data'] = data\n",
    "Dict['s_t_r'] = s_t_r\n",
    "Dict['entity2id'] = entity2id\n",
    "Dict['id2entity'] = id2entity\n",
    "Dict['relation2id'] = relation2id\n",
    "Dict['id2relation'] = id2relation\n",
    "\n",
    "with open('../weight_bin/' + ids_name + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(Dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25a5deac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating big-batches 20000 1000000\n",
      "generating big-batches 40000 1000000\n",
      "generating big-batches 60000 1000000\n",
      "generating big-batches 80000 1000000\n",
      "generating big-batches 100000 1000000\n",
      "generating big-batches 120000 1000000\n",
      "generating big-batches 140000 1000000\n",
      "generating big-batches 160000 1000000\n",
      "generating big-batches 180000 1000000\n",
      "generating big-batches 200000 1000000\n",
      "generating big-batches 220000 1000000\n",
      "generating big-batches 240000 1000000\n",
      "generating big-batches 260000 1000000\n",
      "generating big-batches 280000 1000000\n",
      "generating big-batches 300000 1000000\n",
      "generating big-batches 320000 1000000\n",
      "generating big-batches 340000 1000000\n",
      "generating big-batches 360000 1000000\n",
      "generating big-batches 380000 1000000\n",
      "generating big-batches 400000 1000000\n",
      "generating big-batches 420000 1000000\n",
      "generating big-batches 440000 1000000\n",
      "generating big-batches 460000 1000000\n",
      "generating big-batches 480000 1000000\n",
      "generating big-batches 500000 1000000\n",
      "generating big-batches 520000 1000000\n",
      "generating big-batches 540000 1000000\n",
      "generating big-batches 560000 1000000\n",
      "generating big-batches 580000 1000000\n",
      "generating big-batches 600000 1000000\n",
      "generating big-batches 620000 1000000\n",
      "generating big-batches 640000 1000000\n",
      "generating big-batches 660000 1000000\n",
      "generating big-batches 680000 1000000\n",
      "generating big-batches 700000 1000000\n",
      "generating big-batches 720000 1000000\n",
      "generating big-batches 740000 1000000\n",
      "generating big-batches 760000 1000000\n",
      "generating big-batches 780000 1000000\n",
      "generating big-batches 800000 1000000\n",
      "generating big-batches 820000 1000000\n",
      "generating big-batches 840000 1000000\n",
      "generating big-batches 860000 1000000\n",
      "generating big-batches 880000 1000000\n",
      "generating big-batches 900000 1000000\n",
      "generating big-batches 920000 1000000\n",
      "generating big-batches 940000 1000000\n",
      "generating big-batches 960000 1000000\n",
      "generating big-batches 980000 1000000\n",
      "generating big-batches 1000000 1000000\n",
      "Epoch 1/10\n",
      "225000/225000 [==============================] - 3070s 14ms/step - loss: 0.4354 - binary_accuracy: 0.8176 - val_loss: 0.2602 - val_binary_accuracy: 0.9043\n",
      "Epoch 2/10\n",
      "225000/225000 [==============================] - 4204s 19ms/step - loss: 0.3690 - binary_accuracy: 0.8529 - val_loss: 0.2529 - val_binary_accuracy: 0.9187\n",
      "Epoch 3/10\n",
      "225000/225000 [==============================] - 5401s 24ms/step - loss: 0.3404 - binary_accuracy: 0.8671 - val_loss: 0.2367 - val_binary_accuracy: 0.9233\n",
      "Epoch 4/10\n",
      "225000/225000 [==============================] - 3070s 14ms/step - loss: 0.3248 - binary_accuracy: 0.8735 - val_loss: 0.2423 - val_binary_accuracy: 0.9244\n",
      "Epoch 5/10\n",
      "225000/225000 [==============================] - 2964s 13ms/step - loss: 0.3112 - binary_accuracy: 0.8790 - val_loss: 0.2412 - val_binary_accuracy: 0.9302\n",
      "Epoch 6/10\n",
      "225000/225000 [==============================] - 2894s 13ms/step - loss: 0.3029 - binary_accuracy: 0.8820 - val_loss: 0.1985 - val_binary_accuracy: 0.9359\n",
      "Epoch 7/10\n",
      "225000/225000 [==============================] - 11896s 53ms/step - loss: 0.2916 - binary_accuracy: 0.8870 - val_loss: 0.2131 - val_binary_accuracy: 0.9311\n",
      "Epoch 8/10\n",
      "225000/225000 [==============================] - 30043s 134ms/step - loss: 0.2857 - binary_accuracy: 0.8889 - val_loss: 0.2100 - val_binary_accuracy: 0.9356\n",
      "Epoch 9/10\n",
      "225000/225000 [==============================] - 3133s 14ms/step - loss: 0.2804 - binary_accuracy: 0.8910 - val_loss: 0.2184 - val_binary_accuracy: 0.9352\n",
      "Epoch 10/10\n",
      "225000/225000 [==============================] - 7314s 33ms/step - loss: 0.2749 - binary_accuracy: 0.8930 - val_loss: 0.2173 - val_binary_accuracy: 0.9313\n",
      "Save model\n"
     ]
    }
   ],
   "source": [
    "holder_len = 1000000\n",
    "lower_bd = 2\n",
    "upper_bd = 10\n",
    "num_epoch = 10\n",
    "batch_size = 4\n",
    "\n",
    "#90% to be train, 10% to be validation\n",
    "train_len = 9*int(holder_len/10)\n",
    "    \n",
    "######################################\n",
    "###pre-define the lists###############\n",
    "\n",
    "#define the lists\n",
    "x_p_list, x_r_list, y_list = {'s': [], 'l': []}, list(), list()\n",
    "\n",
    "#######################################\n",
    "###build the big-batches###############      \n",
    "\n",
    "#fill in the training array list\n",
    "build_big_batches(holder_len, lower_bd, upper_bd, Class_2, one_hop, s_t_r,\n",
    "                      x_p_list, x_r_list, y_list,\n",
    "                      relation2id, entity2id, id2relation, id2entity)\n",
    "\n",
    "#######################################\n",
    "###do the training#####################\n",
    "\n",
    "#generate the input arrays\n",
    "x_train_s = np.asarray(x_p_list['s'][:train_len], dtype='int')\n",
    "x_train_l = np.asarray(x_p_list['l'][:train_len], dtype='int')\n",
    "x_train_r = np.asarray(x_r_list[:train_len], dtype='int')\n",
    "y_train = np.asarray(y_list[:train_len], dtype='int')\n",
    "\n",
    "x_valid_s = np.asarray(x_p_list['s'][train_len:], dtype='int')\n",
    "x_valid_l = np.asarray(x_p_list['l'][train_len:], dtype='int')\n",
    "x_valid_r = np.asarray(x_r_list[train_len:], dtype='int')\n",
    "y_valid = np.asarray(y_list[train_len:], dtype='int')\n",
    "\n",
    "model.fit([x_train_s, x_train_l, x_train_r], y_train, \n",
    "          validation_data=([x_valid_s, x_valid_l, x_valid_r], y_valid),\n",
    "          batch_size=batch_size, epochs=num_epoch)\n",
    "\n",
    "# Save model and weights\n",
    "add_h5 = model_name + '.h5'\n",
    "save_dir = os.path.join(os.getcwd(), '../weight_bin')\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, add_h5)\n",
    "model.save(model_path)\n",
    "print('Save model')\n",
    "del(model)\n",
    "\n",
    "del(x_train_s, x_train_l, x_train_r, y_train)\n",
    "del(x_valid_s, x_valid_l, x_valid_r, y_valid)\n",
    "\n",
    "del(x_p_list, x_r_list, y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04108cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2178e81c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f9152c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cbcb96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4df421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867e4d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f86bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f1d82b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe787997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da50338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14b6bf00",
   "metadata": {},
   "source": [
    "### Result on the testset for inductive link prediction\n",
    "\n",
    "We use the testset for inductive link prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f959af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import opensmile\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37c81c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadKG:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.x = 'Hello'\n",
    "        \n",
    "    def load_train_data(self, data_path, one_hop, data, s_t_r, entity2id, id2entity,\n",
    "                     relation2id, id2relation):\n",
    "        \n",
    "        data_ = set()\n",
    "    \n",
    "        ####load the train, valid and test set##########\n",
    "        with open (data_path, 'r') as f:\n",
    "            \n",
    "            data_ini = f.readlines()\n",
    "                        \n",
    "            for i in range(len(data_ini)):\n",
    "            \n",
    "                x = data_ini[i].split()\n",
    "                \n",
    "                x_ = tuple(x)\n",
    "                \n",
    "                data_.add(x_)\n",
    "        \n",
    "        ####relation dict#################\n",
    "        index = len(relation2id)\n",
    "     \n",
    "        for key in data_:\n",
    "            \n",
    "            if key[1] not in relation2id:\n",
    "                \n",
    "                relation = key[1]\n",
    "                \n",
    "                relation2id[relation] = index\n",
    "                \n",
    "                id2relation[index] = relation\n",
    "                \n",
    "                index += 1\n",
    "                \n",
    "                #the inverse relation\n",
    "                iv_r = '_inverse_' + relation\n",
    "                \n",
    "                relation2id[iv_r] = index\n",
    "                \n",
    "                id2relation[index] = iv_r\n",
    "                \n",
    "                index += 1\n",
    "        \n",
    "        #get the id of the inverse relation, by above definition, initial relation has \n",
    "        #always even id, while inverse relation has always odd id.\n",
    "        def inverse_r(r):\n",
    "            \n",
    "            if r % 2 == 0: #initial relation\n",
    "                \n",
    "                iv_r = r + 1\n",
    "            \n",
    "            else: #inverse relation\n",
    "                \n",
    "                iv_r = r - 1\n",
    "            \n",
    "            return(iv_r)\n",
    "        \n",
    "        ####entity dict###################\n",
    "        index = len(entity2id)\n",
    "        \n",
    "        for key in data_:\n",
    "            \n",
    "            source, target = key[0], key[2]\n",
    "            \n",
    "            if source not in entity2id:\n",
    "                                \n",
    "                entity2id[source] = index\n",
    "                \n",
    "                id2entity[index] = source\n",
    "                \n",
    "                index += 1\n",
    "            \n",
    "            if target not in entity2id:\n",
    "                \n",
    "                entity2id[target] = index\n",
    "                \n",
    "                id2entity[index] = target\n",
    "                \n",
    "                index += 1\n",
    "                \n",
    "        #create the set of triples using id instead of string        \n",
    "        for ele in data_:\n",
    "            \n",
    "            s = entity2id[ele[0]]\n",
    "            \n",
    "            r = relation2id[ele[1]]\n",
    "            \n",
    "            t = entity2id[ele[2]]\n",
    "            \n",
    "            if (s,r,t) not in data:\n",
    "                \n",
    "                data.add((s,r,t))\n",
    "            \n",
    "            s_t_r[(s,t)].add(r)\n",
    "            \n",
    "            if s not in one_hop:\n",
    "                \n",
    "                one_hop[s] = dict()\n",
    "            \n",
    "            if r not in one_hop[s]:\n",
    "                \n",
    "                one_hop[s][r] = set()\n",
    "            \n",
    "            one_hop[s][r].add(t)\n",
    "            \n",
    "            if t not in one_hop:\n",
    "                \n",
    "                one_hop[t] = dict()\n",
    "            \n",
    "            r_inv = inverse_r(r)\n",
    "            \n",
    "            s_t_r[(t,s)].add(r_inv)\n",
    "            \n",
    "            if r_inv not in one_hop[t]:\n",
    "                \n",
    "                one_hop[t][r_inv] = set()\n",
    "            \n",
    "            one_hop[t][r_inv].add(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a714e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObtainPathsByDynamicProgramming:\n",
    "\n",
    "    def __init__(self, size_bd=50, threshold=100000):\n",
    "                \n",
    "        self.size_bd = size_bd\n",
    "        \n",
    "        self.threshold = threshold\n",
    "    \n",
    "    '''\n",
    "    Given an entity s, here is the function to find:\n",
    "      1. any else entity t that is directely connected to s\n",
    "      2. most of the paths from s to each t with length L\n",
    "    \n",
    "    One may refer to LeetCode Problem 797 for details:\n",
    "        https://leetcode.com/problems/all-paths-from-source-to-target/\n",
    "    '''\n",
    "    def obtain_paths(self, mode, s, t_input, lower_bd, upper_bd, one_hop):\n",
    "\n",
    "        if type(lower_bd) != type(1) or lower_bd < 1:\n",
    "            \n",
    "            raise TypeError(\"!!! invalid lower bound setting, must >= 1 !!!\")\n",
    "            \n",
    "        if type(upper_bd) != type(1) or upper_bd < 1:\n",
    "            \n",
    "            raise TypeError(\"!!! invalid upper bound setting, must >= 1 !!!\")\n",
    "            \n",
    "        if lower_bd > upper_bd:\n",
    "            \n",
    "            raise TypeError(\"!!! lower bound must not exced upper bound !!!\")\n",
    "            \n",
    "        if s not in one_hop:\n",
    "            \n",
    "            raise ValueError('!!! entity not in one_hop. Please work on active entities for validation')\n",
    "        \n",
    "        #here is the result dict. Its key is each entity t that is directly connected to s\n",
    "        #The value of each t is a set containing the paths from s to t\n",
    "        #These paths can be either the direct connection r, or a multi-hop path\n",
    "        res = defaultdict(set)\n",
    "        \n",
    "        #direct_nb contains all the direct neighbour of s\n",
    "        direct_nb = set()\n",
    "        \n",
    "        if mode == 'direct_neighbour':\n",
    "        \n",
    "            for r in one_hop[s]:\n",
    "            \n",
    "                for t in one_hop[s][r]:\n",
    "                \n",
    "                    direct_nb.add(t)\n",
    "                    \n",
    "        elif mode == 'target_specified':\n",
    "            \n",
    "            direct_nb.add(t_input)\n",
    "            \n",
    "        elif mode == 'any_target':\n",
    "            \n",
    "            for s_any in one_hop:\n",
    "                \n",
    "                direct_nb.add(s_any)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            raise ValueError('not a valid mode')\n",
    "        \n",
    "        '''\n",
    "        We use recursion to find the paths\n",
    "        On current node with the path [r1, ..., rk] and on-path entities {e1, ..., ek-1, node}\n",
    "        from s to this node, we further find the direct neighbor t' of this node. \n",
    "        If t' is not a on-path entity (not among e1,...ek-1), we recursively proceed to t' \n",
    "        '''\n",
    "        def helper(node, path, on_path_en, res, direct_nb, lower_bd, upper_bd, one_hop, length_dict, count_dict):\n",
    "            \n",
    "            #when the current path is within lower_bd and upper_bd and its corresponding\n",
    "            #length still within the size_bd and its tail node is within the note dict, \n",
    "            #we will then intend to add this path\n",
    "            if (len(path) >= lower_bd) and (len(path) <= upper_bd) and (\n",
    "                node in direct_nb) and (length_dict[len(path)] < self.size_bd):\n",
    "                \n",
    "                #if this path already exists between the source entity and the current target node,\n",
    "                #we will not count it.\n",
    "                #here is an interesting situation: this path may exist between s and some other node t,\n",
    "                #however, it does not exist between s and this node t. Then, we still count it: length_dict[len(path)] += 1\n",
    "                #That is, each path may be counted for multiple times.\n",
    "                #We count how many paths we \"actually\" found between entity pairs\n",
    "                #Same type of path between different entity pairs are count separately.\n",
    "                if tuple(path) not in res[node]:\n",
    "                \n",
    "                    res[node].add(tuple(path))\n",
    "                \n",
    "                    length_dict[len(path)] += 1\n",
    "                \n",
    "            #For some rare entities, we may face such a case: so many paths are evaluated,\n",
    "            #but no entities on the paths are direct neighbors of the rare entity.\n",
    "            #In this case, the recursion cannot be bounded and stoped by the size threshold.\n",
    "            #In order to cure this, we count how many times the recursion happens on a specific length, using the count_dict.\n",
    "            #Its key is length, value counts the recursion occurred to that length. \n",
    "            #The recursion is forced to stop for that length (and hence for longer lengths) once reach the threshold.\n",
    "            if (len(path) < upper_bd) and (length_dict[len(path) + 1] < self.size_bd) and (\n",
    "                count_dict[len(path)] <= self.threshold):\n",
    "                \n",
    "                #we randomly shuffle relation r so that the reading in order is not fixed\n",
    "                temp_list = list()\n",
    "                \n",
    "                for r in one_hop[node]:\n",
    "                    \n",
    "                    temp_list.append(r)\n",
    "                \n",
    "                for i_0 in range(len(temp_list)):\n",
    "                    \n",
    "                    if count_dict[len(path)] > self.threshold:\n",
    "                        break\n",
    "                    \n",
    "                    r = random.choice(temp_list)\n",
    "                    \n",
    "                    for i_1 in range(len(one_hop[node][r])):\n",
    "                        \n",
    "                        if count_dict[len(path)] > self.threshold:\n",
    "                            break\n",
    "                        \n",
    "                        t = random.choice(list(one_hop[node][r]))\n",
    "                        \n",
    "                        if t not in on_path_en:\n",
    "                                \n",
    "                            count_dict[len(path)] += 1\n",
    "\n",
    "                            helper(t, path + [r], on_path_en.union({t}), res, direct_nb, \n",
    "                                   lower_bd, upper_bd, one_hop, length_dict, count_dict)\n",
    "        \n",
    "        length_dict = defaultdict(int)\n",
    "        count_dict = defaultdict(int)\n",
    "        \n",
    "        helper(s, [], {s}, res, direct_nb, lower_bd, upper_bd, one_hop, length_dict, count_dict)\n",
    "        \n",
    "        return(res, length_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0d1f8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the classes\n",
    "Class_1 = LoadKG()\n",
    "Class_2 = ObtainPathsByDynamicProgramming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10f13661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load ids and relation/entity dicts\n",
    "with open('../weight_bin/' + ids_name + '.pickle', 'rb') as handle:\n",
    "    Dict = pickle.load(handle)\n",
    "\n",
    "one_hop = Dict['one_hop']\n",
    "data = Dict['data']\n",
    "s_t_r = Dict['s_t_r']\n",
    "entity2id = Dict['entity2id']\n",
    "id2entity = Dict['id2entity']\n",
    "relation2id = Dict['relation2id']\n",
    "id2relation = Dict['id2relation']\n",
    "\n",
    "#we want to keep the initial entity/relation dicts\n",
    "entity2id_ini = deepcopy(entity2id)\n",
    "id2entity_ini = deepcopy(id2entity)\n",
    "relation2id_ini = deepcopy(relation2id)\n",
    "id2relation_ini = deepcopy(id2relation)\n",
    "\n",
    "num_r = len(id2relation)\n",
    "num_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50c64cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 13:28:59.355623: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#load the model\n",
    "model = keras.models.load_model('../weight_bin/' + model_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2ea521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train_path = '../data/' + data_name + '_ind/train.txt'\n",
    "ind_valid_path = '../data/' + data_name + '_ind/valid.txt'\n",
    "ind_test_path = '../data/' + data_name + '_ind/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d2e6087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the test dataset\n",
    "one_hop_ind = dict() \n",
    "data_ind = set()\n",
    "s_t_r_ind = defaultdict(set)\n",
    "\n",
    "len_0 = len(relation2id)\n",
    "size_0 = len(entity2id)\n",
    "\n",
    "#fill in the sets and dicts\n",
    "Class_1.load_train_data(ind_train_path, \n",
    "                        one_hop_ind, data_ind, s_t_r_ind,\n",
    "                        entity2id, id2entity, relation2id, id2relation)\n",
    "\n",
    "len_1 = len(relation2id)\n",
    "size_1 = len(entity2id)\n",
    "\n",
    "if len_0 != len_1:\n",
    "    raise ValueError('unseen relation!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a6dfe59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2092 4886 7073\n"
     ]
    }
   ],
   "source": [
    "print(size_0, size_1, len(data_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63cec98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the test dataset\n",
    "one_hop_test = dict() \n",
    "data_test = set()\n",
    "s_t_r_test = defaultdict(set)\n",
    "\n",
    "len_0 = len(relation2id)\n",
    "size_0 = len(entity2id)\n",
    "\n",
    "#fill in the sets and dicts\n",
    "Class_1.load_train_data(ind_test_path, \n",
    "                        one_hop_test, data_test, s_t_r_test,\n",
    "                        entity2id, id2entity, relation2id, id2relation)\n",
    "\n",
    "\n",
    "len_1 = len(relation2id)\n",
    "size_1 = len(entity2id)\n",
    "\n",
    "if len_0 != len_1:\n",
    "    raise ValueError('unseen relation!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d18fee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4886 4886 731\n"
     ]
    }
   ],
   "source": [
    "print(size_0, size_1, len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "757526ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the validation for existing triple removal when ranking\n",
    "one_hop_valid = dict() \n",
    "data_valid = set()\n",
    "s_t_r_valid = defaultdict(set)\n",
    "\n",
    "len_0 = len(relation2id)\n",
    "size_0 = len(entity2id)\n",
    "\n",
    "#fill in the sets and dicts\n",
    "Class_1.load_train_data(ind_valid_path, \n",
    "                        one_hop_valid, data_valid, s_t_r_valid,\n",
    "                        entity2id, id2entity, relation2id, id2relation)\n",
    "\n",
    "len_1 = len(relation2id)\n",
    "size_1 = len(entity2id)\n",
    "\n",
    "if len_0 != len_1:\n",
    "    raise ValueError('unseen relation!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2980a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4886 4886 716\n"
     ]
    }
   ],
   "source": [
    "print(size_0, size_1, len(data_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f17ff08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4886 2092\n"
     ]
    }
   ],
   "source": [
    "print(len(entity2id), len(entity2id_ini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "915ad29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we want to check whether there are overlapping \n",
    "#between the entities of train triples and inductive test and valid triples\n",
    "overlapping = 0\n",
    "\n",
    "for ele in data_test:\n",
    "    \n",
    "    s, r, t = ele[0], ele[1], ele[2]\n",
    "    \n",
    "    if s in id2entity_ini or t in id2entity_ini:\n",
    "        \n",
    "        overlapping += 1\n",
    "        \n",
    "overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80eb1e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlapping = 0\n",
    "\n",
    "for ele in data_valid:\n",
    "    \n",
    "    s, r, t = ele[0], ele[1], ele[2]\n",
    "    \n",
    "    if s in id2entity_ini or t in id2entity_ini:\n",
    "        \n",
    "        overlapping += 1\n",
    "        \n",
    "overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a3c9dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we want to check whether there are overlapping \n",
    "#between the entities of train triples and inductive test and valid triples\n",
    "overlapping = 0\n",
    "\n",
    "for ele in data_ind:\n",
    "    \n",
    "    s, r, t = ele[0], ele[1], ele[2]\n",
    "    \n",
    "    if s in id2entity_ini or t in id2entity_ini:\n",
    "        \n",
    "        overlapping += 1\n",
    "        \n",
    "overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83ea5533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relation_ranking(s, t, lower_bd, upper_bd, one_hop, id2relation, model):\n",
    "    \n",
    "    path_holder = set()\n",
    "    \n",
    "    for iteration in range(20):\n",
    "    \n",
    "        result, length_dict = Class_2.obtain_paths('target_specified', \n",
    "                                                   s, t, lower_bd, upper_bd, one_hop)\n",
    "        if t in result:\n",
    "            \n",
    "            for path in result[t]:\n",
    "                \n",
    "                path_holder.add(path)\n",
    "    \n",
    "    path_holder = list(path_holder)\n",
    "    random.shuffle(path_holder)\n",
    "    \n",
    "    score_dict = defaultdict(float)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    if len(path_holder) >= 2:\n",
    "    \n",
    "        #iterate over path_1\n",
    "        while count <= 50:\n",
    "\n",
    "            temp_pair = random.sample(path_holder, 2)\n",
    "\n",
    "            path_1, path_2 = temp_pair[0], temp_pair[1]\n",
    "\n",
    "            #decide which path is shorter and which is longer\n",
    "            if len(path_1) <= len(path_2):\n",
    "\n",
    "                path_s, path_l = path_1, path_2\n",
    "\n",
    "            else:\n",
    "\n",
    "                path_s, path_l = path_2, path_1                            \n",
    "\n",
    "            #whether lengths of the two paths satisfies the requirments\n",
    "            if (len(path_s) < lower_bd) or (len(path_l) > upper_bd):\n",
    "\n",
    "                raise ValueError('something wrong with path finding')\n",
    "\n",
    "            list_s = list()\n",
    "            list_l = list()\n",
    "            list_r = list()\n",
    "\n",
    "            for i in range(len(id2relation)):\n",
    "\n",
    "                if i not in id2relation:\n",
    "\n",
    "                    raise ValueError ('error when generating id2relation')\n",
    "\n",
    "                list_s.append(list(path_s) + [num_r]*abs(len(path_s)-upper_bd))\n",
    "                list_l.append(list(path_l) + [num_r]*abs(len(path_l)-upper_bd))\n",
    "                list_r.append([i])\n",
    "\n",
    "            input_s = np.array(list_s)\n",
    "            input_l = np.array(list_l)\n",
    "            input_r = np.array(list_r)\n",
    "\n",
    "            pred = model.predict([input_s, input_l, input_r], verbose = 0)\n",
    "\n",
    "            for i in range(pred.shape[0]):\n",
    "\n",
    "                score_dict[i] += float(pred[i])\n",
    "\n",
    "            count += 1\n",
    "                \n",
    "    print(len(score_dict), len(path_holder))\n",
    "\n",
    "    return(score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb84dd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 1009\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.0 MRR 0.021739130434782608 cur_rank 45 abs_cur_rank 45 total_num 0 500\n",
      "152 1132\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.0 MRR 0.018115942028985508 cur_rank 68 abs_cur_rank 68 total_num 1 500\n",
      "152 331\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.0 MRR 0.014392109500805153 cur_rank 143 abs_cur_rank 143 total_num 2 500\n",
      "152 1532\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.0 MRR 0.015511063257679335 cur_rank 52 abs_cur_rank 52 total_num 3 500\n",
      "0 0\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.0 MRR 0.01675667669309999 cur_rank 45 abs_cur_rank 46 total_num 4 500\n",
      "152 3134\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.0 MRR 0.01758708565004709 cur_rank 45 abs_cur_rank 45 total_num 5 500\n",
      "152 873\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.14285714285714285 MRR 0.03293178770004036 cur_rank 7 abs_cur_rank 7 total_num 6 500\n",
      "0 0\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.125 MRR 0.02984837208877499 cur_rank 120 abs_cur_rank 120 total_num 7 500\n",
      "152 8\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.1111111111111111 MRR 0.028947345238331392 cur_rank 45 abs_cur_rank 45 total_num 8 500\n",
      "152 1331\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.1 MRR 0.027441499603387143 cur_rank 71 abs_cur_rank 71 total_num 9 500\n",
      "0 0\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.09090909090909091 MRR 0.028313821188264407 cur_rank 26 abs_cur_rank 26 total_num 10 500\n",
      "152 1239\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.08333333333333333 MRR 0.027256419422575704 cur_rank 63 abs_cur_rank 63 total_num 11 500\n",
      "152 42\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.07692307692307693 MRR 0.02764116135284904 cur_rank 30 abs_cur_rank 30 total_num 12 500\n",
      "152 1197\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.07142857142857142 MRR 0.027067352908878028 cur_rank 50 abs_cur_rank 50 total_num 13 500\n",
      "0 0\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.06666666666666667 MRR 0.026140055697408966 cur_rank 75 abs_cur_rank 76 total_num 14 500\n",
      "152 713\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.125 MRR 0.03492296888298757 cur_rank 5 abs_cur_rank 5 total_num 15 500\n",
      "0 0\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.11764705882352941 MRR 0.03367447836851691 cur_rank 72 abs_cur_rank 72 total_num 16 500\n",
      "0 0\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.1111111111111111 MRR 0.03256470902232077 cur_rank 72 abs_cur_rank 72 total_num 17 500\n",
      "0 0\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.10526315789473684 MRR 0.032800094707305834 cur_rank 26 abs_cur_rank 26 total_num 18 500\n",
      "152 1138\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.1 MRR 0.031953740765591335 cur_rank 62 abs_cur_rank 62 total_num 19 500\n",
      "0 0\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.09523809523809523 MRR 0.03090832453865841 cur_rank 99 abs_cur_rank 100 total_num 20 500\n",
      "0 0\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.09090909090909091 MRR 0.030126065702218773 cur_rank 72 abs_cur_rank 72 total_num 21 500\n",
      "152 1641\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.08695652173913043 MRR 0.029553156434399385 cur_rank 58 abs_cur_rank 58 total_num 22 500\n",
      "152 3209\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.08333333333333333 MRR 0.02881197099473078 cur_rank 84 abs_cur_rank 84 total_num 23 500\n",
      "152 189\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.12 MRR 0.03765949215494155 cur_rank 3 abs_cur_rank 3 total_num 24 500\n",
      "152 1951\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.11538461538461539 MRR 0.03662461507867622 cur_rank 92 abs_cur_rank 92 total_num 25 500\n",
      "152 1620\n",
      "Hits@1 0.0 Hits@3 0.0 Hits@10 0.14814814814814814 MRR 0.04452740711279932 cur_rank 3 abs_cur_rank 3 total_num 26 500\n",
      "152 1206\n",
      "Hits@1 0.0 Hits@3 0.03571428571428571 Hits@10 0.17857142857142858 MRR 0.05484190447781839 cur_rank 2 abs_cur_rank 2 total_num 27 500\n",
      "152 14\n",
      "Hits@1 0.0 Hits@3 0.034482758620689655 Hits@10 0.1724137931034483 MRR 0.054866513135671396 cur_rank 17 abs_cur_rank 17 total_num 28 500\n",
      "0 0\n",
      "Hits@1 0.0 Hits@3 0.03333333333333333 Hits@10 0.16666666666666666 MRR 0.05347052979738278 cur_rank 76 abs_cur_rank 76 total_num 29 500\n",
      "152 2\n",
      "Hits@1 0.0 Hits@3 0.06451612903225806 Hits@10 0.1935483870967742 MRR 0.06249836216951022 cur_rank 2 abs_cur_rank 2 total_num 30 500\n",
      "0 0\n",
      "Hits@1 0.0 Hits@3 0.0625 Hits@10 0.1875 MRR 0.061134910993222456 cur_rank 52 abs_cur_rank 52 total_num 31 500\n",
      "0 0\n",
      "Hits@1 0.0 Hits@3 0.06060606060606061 Hits@10 0.21212121212121213 MRR 0.06534294399342784 cur_rank 4 abs_cur_rank 4 total_num 32 500\n",
      "152 984\n",
      "Hits@1 0.0 Hits@3 0.058823529411764705 Hits@10 0.20588235294117646 MRR 0.06392819209098421 cur_rank 57 abs_cur_rank 57 total_num 33 500\n",
      "152 718\n",
      "Hits@1 0.0 Hits@3 0.05714285714285714 Hits@10 0.2 MRR 0.06254123275651653 cur_rank 64 abs_cur_rank 64 total_num 34 500\n",
      "152 383\n",
      "Hits@1 0.0 Hits@3 0.08333333333333333 Hits@10 0.2222222222222222 MRR 0.07469286517994662 cur_rank 1 abs_cur_rank 1 total_num 35 500\n",
      "152 2479\n",
      "Hits@1 0.0 Hits@3 0.08108108108108109 Hits@10 0.21621621621621623 MRR 0.07310313952300256 cur_rank 62 abs_cur_rank 62 total_num 36 500\n",
      "152 15\n",
      "Hits@1 0.0 Hits@3 0.07894736842105263 Hits@10 0.21052631578947367 MRR 0.07183726743029195 cur_rank 39 abs_cur_rank 39 total_num 37 500\n",
      "0 0\n",
      "Hits@1 0.0 Hits@3 0.07692307692307693 Hits@10 0.20512820512820512 MRR 0.07023492196778507 cur_rank 106 abs_cur_rank 106 total_num 38 500\n",
      "152 2245\n",
      "Hits@1 0.0 Hits@3 0.075 Hits@10 0.2 MRR 0.06887587431541584 cur_rank 62 abs_cur_rank 62 total_num 39 500\n",
      "152 3\n",
      "Hits@1 0.0 Hits@3 0.07317073170731707 Hits@10 0.1951219512195122 MRR 0.0673714443224622 cur_rank 138 abs_cur_rank 138 total_num 40 500\n",
      "152 1040\n",
      "Hits@1 0.0 Hits@3 0.07142857142857142 Hits@10 0.19047619047619047 MRR 0.06611750237080694 cur_rank 67 abs_cur_rank 68 total_num 41 500\n",
      "152 1491\n",
      "Hits@1 0.0 Hits@3 0.06976744186046512 Hits@10 0.18604651162790697 MRR 0.06498788277262824 cur_rank 56 abs_cur_rank 56 total_num 42 500\n",
      "152 3045\n",
      "Hits@1 0.0 Hits@3 0.06818181818181818 Hits@10 0.18181818181818182 MRR 0.0638774543518427 cur_rank 61 abs_cur_rank 61 total_num 43 500\n",
      "0 0\n",
      "Hits@1 0.0 Hits@3 0.06666666666666667 Hits@10 0.17777777777777778 MRR 0.06294104715368581 cur_rank 45 abs_cur_rank 46 total_num 44 500\n",
      "152 1720\n",
      "Hits@1 0.0 Hits@3 0.06521739130434782 Hits@10 0.17391304347826086 MRR 0.06192914270736548 cur_rank 60 abs_cur_rank 60 total_num 45 500\n",
      "152 508\n",
      "Hits@1 0.0 Hits@3 0.06382978723404255 Hits@10 0.1702127659574468 MRR 0.061297843171381726 cur_rank 30 abs_cur_rank 30 total_num 46 500\n",
      "152 1076\n",
      "Hits@1 0.0 Hits@3 0.0625 Hits@10 0.16666666666666666 MRR 0.060792409710249545 cur_rank 26 abs_cur_rank 26 total_num 47 500\n",
      "152 1234\n",
      "Hits@1 0.0 Hits@3 0.061224489795918366 Hits@10 0.16326530612244897 MRR 0.059922805801506045 cur_rank 54 abs_cur_rank 54 total_num 48 500\n",
      "152 2416\n",
      "Hits@1 0.0 Hits@3 0.06 Hits@10 0.18 MRR 0.06158149254261878 cur_rank 6 abs_cur_rank 6 total_num 49 500\n",
      "152 3921\n",
      "Hits@1 0.0 Hits@3 0.058823529411764705 Hits@10 0.17647058823529413 MRR 0.060680384845704684 cur_rank 63 abs_cur_rank 63 total_num 50 500\n",
      "152 209\n",
      "Hits@1 0.0 Hits@3 0.057692307692307696 Hits@10 0.19230769230769232 MRR 0.06432114667559498 cur_rank 3 abs_cur_rank 3 total_num 51 500\n",
      "152 1562\n",
      "Hits@1 0.0 Hits@3 0.05660377358490566 Hits@10 0.18867924528301888 MRR 0.06337708191352176 cur_rank 69 abs_cur_rank 69 total_num 52 500\n",
      "152 43\n",
      "Hits@1 0.0 Hits@3 0.07407407407407407 Hits@10 0.2037037037037037 MRR 0.0714626915077158 cur_rank 1 abs_cur_rank 1 total_num 53 500\n",
      "152 195\n",
      "Hits@1 0.0 Hits@3 0.07272727272727272 Hits@10 0.2 MRR 0.07095388367793125 cur_rank 22 abs_cur_rank 23 total_num 54 500\n",
      "152 3365\n",
      "Hits@1 0.0 Hits@3 0.07142857142857142 Hits@10 0.19642857142857142 MRR 0.07001753787151317 cur_rank 53 abs_cur_rank 53 total_num 55 500\n",
      "152 2708\n",
      "Hits@1 0.0 Hits@3 0.07017543859649122 Hits@10 0.19298245614035087 MRR 0.0694909144000831 cur_rank 24 abs_cur_rank 24 total_num 56 500\n",
      "152 1698\n",
      "Hits@1 0.0 Hits@3 0.06896551724137931 Hits@10 0.1896551724137931 MRR 0.06930699396925002 cur_rank 16 abs_cur_rank 16 total_num 57 500\n",
      "152 2610\n",
      "Hits@1 0.0 Hits@3 0.06779661016949153 Hits@10 0.1864406779661017 MRR 0.068389104497763 cur_rank 65 abs_cur_rank 65 total_num 58 500\n",
      "0 0\n",
      "Hits@1 0.0 Hits@3 0.06666666666666667 Hits@10 0.18333333333333332 MRR 0.06761160493004666 cur_rank 45 abs_cur_rank 46 total_num 59 500\n",
      "152 1908\n",
      "Hits@1 0.0 Hits@3 0.06557377049180328 Hits@10 0.18032786885245902 MRR 0.06672778567114403 cur_rank 72 abs_cur_rank 72 total_num 60 500\n",
      "152 1659\n",
      "Hits@1 0.0 Hits@3 0.06451612903225806 Hits@10 0.1774193548387097 MRR 0.06593449654175658 cur_rank 56 abs_cur_rank 56 total_num 61 500\n",
      "152 1378\n",
      "Hits@1 0.0 Hits@3 0.06349206349206349 Hits@10 0.1746031746031746 MRR 0.06517136394358812 cur_rank 55 abs_cur_rank 55 total_num 62 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 1495\n",
      "Hits@1 0.0 Hits@3 0.078125 Hits@10 0.1875 MRR 0.07196556138196955 cur_rank 1 abs_cur_rank 1 total_num 63 500\n",
      "152 9\n",
      "Hits@1 0.0 Hits@3 0.07692307692307693 Hits@10 0.18461538461538463 MRR 0.07155769959847072 cur_rank 21 abs_cur_rank 21 total_num 64 500\n",
      "152 1236\n",
      "Hits@1 0.0 Hits@3 0.07575757575757576 Hits@10 0.18181818181818182 MRR 0.07073930808408666 cur_rank 56 abs_cur_rank 56 total_num 65 500\n",
      "152 1089\n",
      "Hits@1 0.0 Hits@3 0.07462686567164178 Hits@10 0.1791044776119403 MRR 0.06995989331445131 cur_rank 53 abs_cur_rank 53 total_num 66 500\n",
      "152 2408\n",
      "Hits@1 0.0 Hits@3 0.07352941176470588 Hits@10 0.19117647058823528 MRR 0.07138205174610154 cur_rank 5 abs_cur_rank 5 total_num 67 500\n",
      "152 2475\n",
      "Hits@1 0.0 Hits@3 0.07246376811594203 Hits@10 0.18840579710144928 MRR 0.0706209774385972 cur_rank 52 abs_cur_rank 52 total_num 68 500\n",
      "152 459\n",
      "Hits@1 0.0 Hits@3 0.07142857142857142 Hits@10 0.18571428571428572 MRR 0.06990365152183589 cur_rank 48 abs_cur_rank 48 total_num 69 500\n",
      "152 1528\n",
      "Hits@1 0.0 Hits@3 0.07042253521126761 Hits@10 0.18309859154929578 MRR 0.06916618966447373 cur_rank 56 abs_cur_rank 56 total_num 70 500\n",
      "152 1415\n",
      "Hits@1 0.0 Hits@3 0.06944444444444445 Hits@10 0.18055555555555555 MRR 0.06844921285870496 cur_rank 56 abs_cur_rank 56 total_num 71 500\n",
      "152 1452\n",
      "Hits@1 0.0 Hits@3 0.0684931506849315 Hits@10 0.1780821917808219 MRR 0.06777498760352776 cur_rank 51 abs_cur_rank 51 total_num 72 500\n",
      "152 449\n",
      "Hits@1 0.0 Hits@3 0.06756756756756757 Hits@10 0.17567567567567569 MRR 0.0679852355187954 cur_rank 11 abs_cur_rank 11 total_num 73 500\n",
      "152 214\n",
      "Hits@1 0.0 Hits@3 0.06666666666666667 Hits@10 0.17333333333333334 MRR 0.06718716679588897 cur_rank 122 abs_cur_rank 122 total_num 74 500\n",
      "152 133\n",
      "Hits@1 0.0 Hits@3 0.06578947368421052 Hits@10 0.17105263157894737 MRR 0.06664938551533364 cur_rank 37 abs_cur_rank 37 total_num 75 500\n",
      "152 1791\n",
      "Hits@1 0.0 Hits@3 0.06493506493506493 Hits@10 0.16883116883116883 MRR 0.06603355932981982 cur_rank 51 abs_cur_rank 51 total_num 76 500\n",
      "152 1514\n",
      "Hits@1 0.0 Hits@3 0.0641025641025641 Hits@10 0.16666666666666666 MRR 0.06545975210436931 cur_rank 46 abs_cur_rank 46 total_num 77 500\n",
      "152 449\n",
      "Hits@1 0.0 Hits@3 0.06329113924050633 Hits@10 0.16455696202531644 MRR 0.06560485748182131 cur_rank 12 abs_cur_rank 12 total_num 78 500\n",
      "152 4478\n",
      "Hits@1 0.0 Hits@3 0.0625 Hits@10 0.1625 MRR 0.06494505317355495 cur_rank 77 abs_cur_rank 77 total_num 79 500\n",
      "152 539\n",
      "Hits@1 0.012345679012345678 Hits@3 0.07407407407407407 Hits@10 0.1728395061728395 MRR 0.0764889414059802 cur_rank 0 abs_cur_rank 0 total_num 80 500\n",
      "0 0\n",
      "Hits@1 0.012195121951219513 Hits@3 0.07317073170731707 Hits@10 0.17073170731707318 MRR 0.07566407087081121 cur_rank 112 abs_cur_rank 112 total_num 81 500\n",
      "152 2044\n",
      "Hits@1 0.012048192771084338 Hits@3 0.07228915662650602 Hits@10 0.1686746987951807 MRR 0.07498415157394324 cur_rank 51 abs_cur_rank 51 total_num 82 500\n",
      "152 2767\n",
      "Hits@1 0.011904761904761904 Hits@3 0.07142857142857142 Hits@10 0.16666666666666666 MRR 0.07425235826370002 cur_rank 73 abs_cur_rank 73 total_num 83 500\n",
      "152 3065\n",
      "Hits@1 0.011764705882352941 Hits@3 0.07058823529411765 Hits@10 0.16470588235294117 MRR 0.07411409522530356 cur_rank 15 abs_cur_rank 15 total_num 84 500\n",
      "152 499\n",
      "Hits@1 0.011627906976744186 Hits@3 0.06976744186046512 Hits@10 0.16279069767441862 MRR 0.07366758581238474 cur_rank 27 abs_cur_rank 27 total_num 85 500\n",
      "152 1570\n",
      "Hits@1 0.011494252873563218 Hits@3 0.06896551724137931 Hits@10 0.16091954022988506 MRR 0.07300926232744871 cur_rank 60 abs_cur_rank 60 total_num 86 500\n",
      "152 550\n",
      "Hits@1 0.011363636363636364 Hits@3 0.06818181818181818 Hits@10 0.1590909090909091 MRR 0.07321266947042192 cur_rank 10 abs_cur_rank 10 total_num 87 500\n",
      "152 1150\n",
      "Hits@1 0.011235955056179775 Hits@3 0.06741573033707865 Hits@10 0.15730337078651685 MRR 0.0729518529595183 cur_rank 19 abs_cur_rank 19 total_num 88 500\n",
      "152 923\n",
      "Hits@1 0.011111111111111112 Hits@3 0.06666666666666667 Hits@10 0.16666666666666666 MRR 0.07436349903774588 cur_rank 4 abs_cur_rank 4 total_num 89 500\n",
      "152 924\n",
      "Hits@1 0.01098901098901099 Hits@3 0.06593406593406594 Hits@10 0.16483516483516483 MRR 0.07379051797383902 cur_rank 44 abs_cur_rank 44 total_num 90 500\n",
      "152 1487\n",
      "Hits@1 0.010869565217391304 Hits@3 0.07608695652173914 Hits@10 0.17391304347826086 MRR 0.07842322973499295 cur_rank 1 abs_cur_rank 1 total_num 91 500\n",
      "152 722\n",
      "Hits@1 0.010752688172043012 Hits@3 0.07526881720430108 Hits@10 0.17204301075268819 MRR 0.07797821691028374 cur_rank 26 abs_cur_rank 26 total_num 92 500\n",
      "152 1757\n",
      "Hits@1 0.010638297872340425 Hits@3 0.07446808510638298 Hits@10 0.1702127659574468 MRR 0.0773420850089171 cur_rank 54 abs_cur_rank 54 total_num 93 500\n",
      "0 0\n",
      "Hits@1 0.010526315789473684 Hits@3 0.07368421052631578 Hits@10 0.16842105263157894 MRR 0.07665632750304659 cur_rank 81 abs_cur_rank 82 total_num 94 500\n",
      "152 1286\n",
      "Hits@1 0.010416666666666666 Hits@3 0.07291666666666667 Hits@10 0.16666666666666666 MRR 0.07605814460437703 cur_rank 51 abs_cur_rank 51 total_num 95 500\n",
      "152 2244\n",
      "Hits@1 0.010309278350515464 Hits@3 0.08247422680412371 Hits@10 0.17525773195876287 MRR 0.08042867919608448 cur_rank 1 abs_cur_rank 1 total_num 96 500\n",
      "152 1343\n",
      "Hits@1 0.01020408163265306 Hits@3 0.08163265306122448 Hits@10 0.17346938775510204 MRR 0.07979019413140141 cur_rank 55 abs_cur_rank 55 total_num 97 500\n",
      "152 1700\n",
      "Hits@1 0.010101010101010102 Hits@3 0.08080808080808081 Hits@10 0.1717171717171717 MRR 0.07907056599418533 cur_rank 116 abs_cur_rank 116 total_num 98 500\n",
      "152 1157\n",
      "Hits@1 0.01 Hits@3 0.08 Hits@10 0.17 MRR 0.07848394196689652 cur_rank 48 abs_cur_rank 48 total_num 99 500\n",
      "152 33\n",
      "Hits@1 0.019801980198019802 Hits@3 0.0891089108910891 Hits@10 0.1782178217821782 MRR 0.08760786333356092 cur_rank 0 abs_cur_rank 0 total_num 100 500\n",
      "152 866\n",
      "Hits@1 0.0196078431372549 Hits@3 0.08823529411764706 Hits@10 0.17647058823529413 MRR 0.08700034531696745 cur_rank 38 abs_cur_rank 38 total_num 101 500\n",
      "152 152\n",
      "Hits@1 0.019417475728155338 Hits@3 0.08737864077669903 Hits@10 0.17475728155339806 MRR 0.08623036436915521 cur_rank 129 abs_cur_rank 129 total_num 102 500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1h/rcbfn0354hv_dhq9m90rp5kc0000gn/T/ipykernel_53994/392813441.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0ms_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mscore_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelation_ranking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hop_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2relation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#[... [score, r], ...]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/1h/rcbfn0354hv_dhq9m90rp5kc0000gn/T/ipykernel_53994/2990326517.py\u001b[0m in \u001b[0;36mrelation_ranking\u001b[0;34m(s, t, lower_bd, upper_bd, one_hop, id2relation, model)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         result, length_dict = Class_2.obtain_paths('target_specified', \n\u001b[0m\u001b[1;32m      8\u001b[0m                                                    s, t, lower_bd, upper_bd, one_hop)\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/1h/rcbfn0354hv_dhq9m90rp5kc0000gn/T/ipykernel_53994/3003381867.py\u001b[0m in \u001b[0;36mobtain_paths\u001b[0;34m(self, mode, s, t_input, lower_bd, upper_bd, one_hop)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mcount_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mhelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirect_nb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower_bd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_bd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/1h/rcbfn0354hv_dhq9m90rp5kc0000gn/T/ipykernel_53994/3003381867.py\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(node, path, on_path_en, res, direct_nb, lower_bd, upper_bd, one_hop, length_dict, count_dict)\u001b[0m\n\u001b[1;32m    124\u001b[0m                             \u001b[0mcount_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                             helper(t, path + [r], on_path_en.union({t}), res, direct_nb, \n\u001b[0m\u001b[1;32m    127\u001b[0m                                    lower_bd, upper_bd, one_hop, length_dict, count_dict)\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/1h/rcbfn0354hv_dhq9m90rp5kc0000gn/T/ipykernel_53994/3003381867.py\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(node, path, on_path_en, res, direct_nb, lower_bd, upper_bd, one_hop, length_dict, count_dict)\u001b[0m\n\u001b[1;32m    124\u001b[0m                             \u001b[0mcount_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                             helper(t, path + [r], on_path_en.union({t}), res, direct_nb, \n\u001b[0m\u001b[1;32m    127\u001b[0m                                    lower_bd, upper_bd, one_hop, length_dict, count_dict)\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/1h/rcbfn0354hv_dhq9m90rp5kc0000gn/T/ipykernel_53994/3003381867.py\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(node, path, on_path_en, res, direct_nb, lower_bd, upper_bd, one_hop, length_dict, count_dict)\u001b[0m\n\u001b[1;32m    124\u001b[0m                             \u001b[0mcount_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                             helper(t, path + [r], on_path_en.union({t}), res, direct_nb, \n\u001b[0m\u001b[1;32m    127\u001b[0m                                    lower_bd, upper_bd, one_hop, length_dict, count_dict)\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/1h/rcbfn0354hv_dhq9m90rp5kc0000gn/T/ipykernel_53994/3003381867.py\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(node, path, on_path_en, res, direct_nb, lower_bd, upper_bd, one_hop, length_dict, count_dict)\u001b[0m\n\u001b[1;32m    124\u001b[0m                             \u001b[0mcount_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                             helper(t, path + [r], on_path_en.union({t}), res, direct_nb, \n\u001b[0m\u001b[1;32m    127\u001b[0m                                    lower_bd, upper_bd, one_hop, length_dict, count_dict)\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/1h/rcbfn0354hv_dhq9m90rp5kc0000gn/T/ipykernel_53994/3003381867.py\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(node, path, on_path_en, res, direct_nb, lower_bd, upper_bd, one_hop, length_dict, count_dict)\u001b[0m\n\u001b[1;32m    124\u001b[0m                             \u001b[0mcount_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                             helper(t, path + [r], on_path_en.union({t}), res, direct_nb, \n\u001b[0m\u001b[1;32m    127\u001b[0m                                    lower_bd, upper_bd, one_hop, length_dict, count_dict)\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/1h/rcbfn0354hv_dhq9m90rp5kc0000gn/T/ipykernel_53994/3003381867.py\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(node, path, on_path_en, res, direct_nb, lower_bd, upper_bd, one_hop, length_dict, count_dict)\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi_1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                         \u001b[0;32mif\u001b[0m \u001b[0mcount_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m                             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "#obtain the precision-recall area under curve (AUC-PR)##\n",
    "\n",
    "#randomly select 10% of the triples\n",
    "selected = random.sample(list(data_test), min(len(data_test), 500))\n",
    "\n",
    "random.shuffle(selected)\n",
    "\n",
    "###Hit at 1#############################\n",
    "#generate the negative samples by randomly replace relation with all the other relaiton\n",
    "Hits_at_1 = 0\n",
    "Hits_at_3 = 0\n",
    "Hits_at_10 = 0\n",
    "MRR_raw = 0.\n",
    "\n",
    "for i in range(len(selected)):\n",
    "    \n",
    "    s_true, r_true, t_true = selected[i][0], selected[i][1], selected[i][2]\n",
    "    \n",
    "    score_dict = relation_ranking(s_true, t_true, 2, 10, one_hop_ind, id2relation, model)\n",
    "    \n",
    "    #[... [score, r], ...]\n",
    "    temp_list = list()\n",
    "    \n",
    "    for r in id2relation:\n",
    "        \n",
    "        if r in score_dict:\n",
    "            \n",
    "            temp_list.append([score_dict[r], r])\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            temp_list.append([0.0, r])\n",
    "        \n",
    "    sorted_list = sorted(temp_list, key = lambda x: x[0], reverse=True)\n",
    "    \n",
    "    p = 0\n",
    "    exist_tri = 0\n",
    "    \n",
    "    while p < len(sorted_list) and sorted_list[p][1] != r_true:\n",
    "        \n",
    "        #moreover, we want to remove existing triples\n",
    "        if ((s_true, sorted_list[p][1], t_true) in data_test) or (\n",
    "              (s_true, sorted_list[p][1], t_true) in data_valid) or (\n",
    "              (s_true, sorted_list[p][1], t_true) in data_ind):\n",
    "            \n",
    "            exist_tri += 1\n",
    "            \n",
    "        p += 1\n",
    "    \n",
    "    if p - exist_tri == 0:\n",
    "        \n",
    "        Hits_at_1 += 1\n",
    "        \n",
    "    if p - exist_tri < 3:\n",
    "        \n",
    "        Hits_at_3 += 1\n",
    "        \n",
    "    if p - exist_tri < 10:\n",
    "        \n",
    "        Hits_at_10 += 1\n",
    "        \n",
    "    MRR_raw += 1./float(p - exist_tri + 1.) \n",
    "        \n",
    "    print('Hits@1', Hits_at_1/(i+1),\n",
    "          'Hits@3', Hits_at_3/(i+1),\n",
    "          'Hits@10', Hits_at_10/(i+1),\n",
    "          'MRR', MRR_raw/(i+1),\n",
    "          'cur_rank', p - exist_tri,\n",
    "          'abs_cur_rank', p,\n",
    "          'total_num', i, len(selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f1c288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ce01b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de9ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
