{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cb2f730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "target_shape = (200, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4010a7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_sent_in = keras.Input(shape=(None,))\n",
    "scd_sent_in = keras.Input(shape=(None,))\n",
    "\n",
    "embd_func_1 = layers.Embedding(len(relation2id), 300)\n",
    "\n",
    "fst_sent_embd = embd_func_1(fst_sent_in)\n",
    "scd_sent_embd = embd_func_1(scd_sent_in)\n",
    "\n",
    "lstm_func_1 = layers.Bidirectional(layers.LSTM(150, return_sequences=True))\n",
    "lstm_func_2 = layers.Bidirectional(layers.LSTM(150, return_sequences=False))\n",
    "\n",
    "fst_lstm_mid = lstm_func_1(fst_sent_embd)\n",
    "scd_lstm_mid = lstm_func_1(scd_sent_embd)\n",
    "\n",
    "fst_lstm_out = lstm_func_2(fst_lstm_mid)\n",
    "scd_lstm_out = lstm_func_2(scd_lstm_mid)\n",
    "\n",
    "#fst_redu_mean = tf.reduce_mean(fst_lstm_out, axis=1)\n",
    "#scd_redu_mean = tf.reduce_mean(scd_lstm_out, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17866815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer, using integer to represent each relation type\n",
    "#note that inputs_path is the path inputs, while inputs_out_re is the output relation inputs\n",
    "inputs_path = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "inputs_out_re = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "# Embed each integer in a 300-dimensional vector as input\n",
    "in_embd = layers.Embedding(len(relation2id), 300)(inputs_path)\n",
    "\n",
    "# Embed each integer in a 300-dimensional vector as output\n",
    "ot_embd = layers.Embedding(len(relation2id), 300)(inputs_out_re)\n",
    "\n",
    "#add 2 layer bi-directional LSTM\n",
    "mid_lstm = layers.Bidirectional(layers.LSTM(150, return_sequences=True))(in_embd)\n",
    "out_lstm = layers.Bidirectional(layers.LSTM(150, return_sequences=True))(mid_lstm)\n",
    "\n",
    "#sum into an embedding\n",
    "sum_lstm = tf.reduce_sum(out_lstm, axis=1)\n",
    "\n",
    "#remove the time dimension from the output embd since there is only one step\n",
    "sum_ot_embd = tf.reduce_sum(ot_embd, axis=1)\n",
    "\n",
    "#concatenate the lstm output and output embd\n",
    "concat = layers.concatenate([sum_lstm, sum_ot_embd], axis=1)\n",
    "\n",
    "#add the dense layer\n",
    "dense_1 = layers.Dense(32, activation='relu')(concat)\n",
    "batch_norm = layers.BatchNormalization()(dense_1)\n",
    "dropout = layers.Dropout(0.5)(batch_norm)\n",
    "\n",
    "#final layer\n",
    "final_out = layers.Dense(1, activation='sigmoid')(dropout)\n",
    "\n",
    "#put together the model\n",
    "model = keras.Model([inputs_path, inputs_out_re], final_out)\n",
    "\n",
    "#compile the model\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=\"Adam\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212c307f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85312de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for filling big-batch lists\n",
    "def fill_in_big_batch_lists(existing_ids, x_p_lists, x_r_lists, y_lists, len_record,\n",
    "                            half_len, path_set, lower_bd, upper_bd, Class_2, one_hop, s_t_r, \n",
    "                            relation2id, entity2id, id2relation, id2entity, epoch):\n",
    "\n",
    "    carry_on = True\n",
    "    total_count = 0\n",
    "    slot_checkpoint = -1\n",
    "    \n",
    "    while carry_on:\n",
    "\n",
    "        source_id = random.choice(existing_ids)\n",
    "        \n",
    "        result, length_dict = Class_2.obtain_paths('direct_neighbour', source_id, \n",
    "                                                   'not_specified', lower_bd, upper_bd, one_hop)\n",
    "\n",
    "        for target_id in result:\n",
    "\n",
    "            if not carry_on:\n",
    "                break\n",
    "\n",
    "            #we want to make sure that the same position in different path-len list stores\n",
    "            #path between the same node pair. Hence, in each result, we want to obtain the\n",
    "            #minimum path amount across all different lengths, so that this amount of paths\n",
    "            #will be appended to lists across different lengths.\n",
    "            minimun_amount = 0\n",
    "\n",
    "            for length in length_dict:\n",
    "\n",
    "            #for positive mode, (source_id, target_id) guaranteed to be in s_t_r\n",
    "            if (source_id, target_id) in s_t_r:\n",
    "                dir_r = list(s_t_r[(source_id, target_id)])\n",
    "\n",
    "            for path in result[target_id]:\n",
    "\n",
    "                if not carry_on:\n",
    "                    break\n",
    "\n",
    "                #update the connection dictionary all the time ^_^\n",
    "#                if mode == 'positive':\n",
    "#                    connect_dict[path].update(s_t_r[(source_id, target_id)])\n",
    "\n",
    "                path_len = len(path)\n",
    "\n",
    "                if (path_len >= lower_bd) and (len_record[path_len] < half_len) and (\n",
    "                    path not in path_set[path_len]):\n",
    "\n",
    "                    slot = deepcopy(len_record[path_len])\n",
    "                    \n",
    "                    if mode == 'positive':\n",
    "                        \n",
    "                        x_p_lists[path_len].append(path)\n",
    "                        \n",
    "                        #under this case, dir must exist\n",
    "                        relation_id = random.choice(dir_r)\n",
    "                        x_r_lists[path_len].append(relation_id)\n",
    "                        \n",
    "                        y_lists[path_len].append(1.)\n",
    "                        \n",
    "                        len_record[path_len] += 1\n",
    "                        path_set[path_len].add(path)\n",
    "                    \n",
    "                    else:\n",
    "                        \n",
    "                        selectable = {i for i in id2relation}\n",
    "                        \n",
    "#                        if path in connect_dict:\n",
    "#                            selectable = selectable.difference(connect_dict[path])\n",
    "                            \n",
    "                        if (source_id, target_id) in s_t_r:\n",
    "                            selectable = selectable.difference(s_t_r[(source_id, target_id)])\n",
    "                        \n",
    "                        if len(selectable) > 0:\n",
    "                            x_p_lists[path_len].append(path)\n",
    "                            \n",
    "                            relation_id = random.choice(list(selectable))\n",
    "                            x_r_lists[path_len].append(relation_id)\n",
    "                            \n",
    "                            y_lists[path_len].append(0.)\n",
    "                            \n",
    "                            len_record[path_len] += 1\n",
    "                            path_set[path_len].add(path)\n",
    "                \n",
    "                #here we use the slot_checkpoint to avoid repatitive printing counting result\n",
    "                if len_record[length] % 500 == 0 and len_record[length] != slot_checkpoint:\n",
    "\n",
    "                    print('generating the', mode, 'half', length, len_record[length], half_len, 'for epoch', epoch)\n",
    "\n",
    "                    slot_checkpoint = len_record[length]\n",
    "\n",
    "                if len_record[length] >= half_len:\n",
    "\n",
    "                    carry_on = False\n",
    "\n",
    "        total_count += 1\n",
    "\n",
    "        if total_count >= 3*len(entity2id):\n",
    "\n",
    "            carry_on = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce09b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to build all the big batches\n",
    "def build_big_batches(holder_len, connect_dict, lower_bd, upper_bd, Class_2, one_hop, s_t_r, \n",
    "                      relation2id, entity2id, id2relation, id2entity, epoch):\n",
    "    \n",
    "    if holder_len % 10 != 0:\n",
    "        raise ValueError('We would like to take 10X as a big-batch size')\n",
    "    \n",
    "    #we first append the positive half, then negative half. After that we shuffle the list\n",
    "    half_len = int(holder_len/2)\n",
    "    \n",
    "    #90% to be train, 10% to be validation\n",
    "    train_len = 9*int(holder_len/10)\n",
    "    valid_len = int(holder_len/10)\n",
    "    \n",
    "    #first create the train holder arrays\n",
    "    x_p_train = {length: np.zeros((train_len, length), dtype=int) for length in range(lower_bd, upper_bd+1)} \n",
    "    x_r_train = {length: np.zeros((train_len, 1), dtype=int) for length in range(lower_bd, upper_bd+1)}\n",
    "    y_train = {length: np.zeros((train_len,)) for length in range(lower_bd, upper_bd+1)}\n",
    "\n",
    "    #first create the validation holder arrays\n",
    "    x_p_valid = {length: np.zeros((valid_len, length), dtype=int) for length in range(lower_bd, upper_bd+1)} \n",
    "    x_r_valid = {length: np.zeros((valid_len, 1), dtype=int) for length in range(lower_bd, upper_bd+1)}\n",
    "    y_valid = {length: np.zeros((valid_len,)) for length in range(lower_bd, upper_bd+1)}\n",
    "    \n",
    "    #when filling in big-batch for the first time, we use list instead of the arrays directly\n",
    "    x_p_lists = {length: [] for length in range(lower_bd, upper_bd+1)} \n",
    "    x_r_lists = {length: [] for length in range(lower_bd, upper_bd+1)}\n",
    "    y_lists = {length: [] for length in range(lower_bd, upper_bd+1)}\n",
    "    \n",
    "    #create a record list recording the current slot for each half batch (pos/neg)\n",
    "    pos_record = {length: 0 for length in range(lower_bd, upper_bd+1)}\n",
    "    neg_record = {length: 0 for length in range(lower_bd, upper_bd+1)}\n",
    "    \n",
    "    #for validation case: not all entities in entity2id are in one_hop, \n",
    "    #so we need to find out who are in\n",
    "    existing_ids = set()\n",
    "    \n",
    "    for s_1 in one_hop:\n",
    "        existing_ids.add(s_1)\n",
    "        \n",
    "    existing_ids = list(existing_ids)\n",
    "\n",
    "    #start filling by reading the ObtainPathsByDynamicProgramming\n",
    "    #here, we start from the longest length and drop the length iteratively\n",
    "    #in this way, lower length will be pre-filled in in previous iterations for higher lengths\n",
    "    #so the previous running will not be wasted and the entire filling speed is much faster\n",
    "    for length in range(upper_bd, lower_bd-1, -1):\n",
    "        \n",
    "        #we want to remove duplicated paths in the big-batch, \n",
    "        #so use path_set to keep unique ones\n",
    "        path_set = defaultdict(set)\n",
    "        \n",
    "        fill_in_big_batch_lists('positive', existing_ids, x_p_lists, x_r_lists, y_lists, pos_record,\n",
    "                                half_len, connect_dict, path_set, lower_bd, length, Class_2, one_hop, s_t_r, \n",
    "                                relation2id, entity2id, id2relation, id2entity, epoch)\n",
    "        \n",
    "        path_set = defaultdict(set)\n",
    "        \n",
    "        fill_in_big_batch_lists('negative', existing_ids, x_p_lists, x_r_lists, y_lists, neg_record,\n",
    "                                half_len, connect_dict, path_set, lower_bd, length, Class_2, one_hop, s_t_r, \n",
    "                                relation2id, entity2id, id2relation, id2entity, epoch)\n",
    "\n",
    "    #shuffle the list and fill-in the arrays\n",
    "    for length in range(upper_bd, lower_bd-1, -1):\n",
    "        \n",
    "        if (len(x_p_lists[length]) != len(x_r_lists[length])) or (\n",
    "            len(x_r_lists[length]) != len(y_lists[length])) or (\n",
    "            len(x_p_lists[length]) != pos_record[length] + neg_record[length]):\n",
    "            \n",
    "            raise ValueError('list of the same path-length does not in the same size')\n",
    "        \n",
    "        id_list = [i for i in range(holder_len)]\n",
    "        \n",
    "        random.shuffle(id_list)\n",
    "                \n",
    "        for i in range(train_len):\n",
    "            \n",
    "            ID = id_list[i]\n",
    "            \n",
    "            path = deepcopy(x_p_lists[length][ID])\n",
    "            relation_id = deepcopy(x_r_lists[length][ID])\n",
    "            y_label = deepcopy(y_lists[length][ID])\n",
    "            \n",
    "            x_p_train[length][i] = np.asarray(path)\n",
    "            x_r_train[length][i] = relation_id\n",
    "            y_train[length][i] = y_label\n",
    "            \n",
    "        for i in range(valid_len):\n",
    "            \n",
    "            ID = id_list[train_len + i]\n",
    "            \n",
    "            path = deepcopy(x_p_lists[length][ID])\n",
    "            relation_id = deepcopy(x_r_lists[length][ID])\n",
    "            y_label = deepcopy(y_lists[length][ID])\n",
    "            \n",
    "            x_p_valid[length][i] = np.asarray(path)\n",
    "            x_r_valid[length][i] = relation_id\n",
    "            y_valid[length][i] = y_label\n",
    "                \n",
    "    return(x_p_train, x_r_train, y_train, x_p_valid, x_r_valid, y_valid, pos_record, neg_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7655b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1928513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7b934a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfd5eac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2fc763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca2363f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a125862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220b7efd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2ea16a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97e7cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08803339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5369d2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7db3caf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 4],\n",
       "       [5, 6]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "test = [[1, 2],[3, 4],[5, 6]]\n",
    "test_ar = np.asarray(test[1:], dtype='int')\n",
    "\n",
    "test_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca891e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = {i for i in {1:2, 2:3, 3:4}}\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a9238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #######################################\n",
    "    ###do the training#####################\n",
    "    for path_len in pos_record:\n",
    "        \n",
    "        if (pos_record[path_len] == int(holder_len/2)) and (\n",
    "            neg_record[path_len] == int(holder_len/2)):\n",
    "            \n",
    "            print('training on length', path_len, upper_bd, 'for epoch', entire_epoch)\n",
    "        \n",
    "            model.fit([x_p_train[path_len], x_r_train[path_len]], y_train[path_len], \n",
    "                      validation_data=([x_p_valid[path_len], x_r_valid[path_len]], y_valid[path_len]),\n",
    "                      batch_size=8, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb2b16b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [[1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dict = {1: []}\n",
    "Dict[1].append([1,2,3] + [4]*10)\n",
    "Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7bd7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in x_p_lists:\n",
    "    print(key, len(x_p_lists[key]['s']), len(x_p_lists[key]['l']), \n",
    "          len(y_list[key]),  len(x_r_list[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9404a53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_p_lists[(2,4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e56b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the test dataset\n",
    "one_hop_test = dict() \n",
    "data_test = set()\n",
    "s_t_r_test = defaultdict(set)\n",
    "\n",
    "len_0 = len(relation2id)\n",
    "size_0 = len(entity2id)\n",
    "\n",
    "#fill in the sets and dicts\n",
    "Class_1.load_train_data('../data/fb237_v4_ind/train.txt', \n",
    "                        one_hop_test, data_test, s_t_r_test,\n",
    "                        entity2id, id2entity, relation2id, id2relation)\n",
    "\n",
    "Class_1.load_train_data('../data/fb237_v4_ind/valid.txt', \n",
    "                        one_hop_test, data_test, s_t_r_test,\n",
    "                        entity2id, id2entity, relation2id, id2relation)\n",
    "\n",
    "Class_1.load_train_data('../data/fb237_v4_ind/test.txt', \n",
    "                        one_hop_test, data_test, s_t_r_test,\n",
    "                        entity2id, id2entity, relation2id, id2relation)\n",
    "\n",
    "len_1 = len(relation2id)\n",
    "size_1 = len(entity2id)\n",
    "\n",
    "if len_0 != len_1:\n",
    "    raise ValueError('unseen relation!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a989d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relation_ranking(s, t, lower_bd, upper_bd, one_hop, id2relation, model):\n",
    "    \n",
    "    path_holder = defaultdict(set)\n",
    "    \n",
    "    for iteration in range(15):\n",
    "    \n",
    "        result, length_dict = Class_2.obtain_paths('target_specified', \n",
    "                                                   s, t, lower_bd, upper_bd, one_hop)\n",
    "        if t in result:\n",
    "            \n",
    "            for path in result[t]:\n",
    "                \n",
    "                path_holder[len(path)].add(path)\n",
    "                \n",
    "    score_dict = defaultdict(float)\n",
    "    \n",
    "    for length in path_holder:\n",
    "        \n",
    "        for Tuple in path_holder[length]:\n",
    "            \n",
    "            list_a = list()\n",
    "            list_b = list()\n",
    "            \n",
    "            for i in range(len(id2relation)):\n",
    "\n",
    "                list_a.append(list(Tuple))\n",
    "                list_b.append([i])\n",
    "                \n",
    "            input_a = np.array(list_a)\n",
    "            input_b = np.array(list_b)\n",
    "                \n",
    "            pred = model.predict([input_a, input_b], verbose = 0)\n",
    "            \n",
    "            for i in range(pred.shape[0]):\n",
    "\n",
    "                score_dict[i] += pred[i]\n",
    "\n",
    "    return(score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5506b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "#obtain the precision-recall area under curve (AUC-PR)##\n",
    "\n",
    "#randomly select 10% of the triples\n",
    "selected = random.sample(list(data_test), int(len(data_test)/10))\n",
    "\n",
    "###Hit at 1#############################\n",
    "#generate the negative samples by randomly replace relation with all the other relaiton\n",
    "Hits_at_1 = 0\n",
    "\n",
    "for i in range(len(selected)):\n",
    "    \n",
    "    s_true, r_true, t_true = selected[i][0], selected[i][1], selected[i][2]\n",
    "    \n",
    "    score_dict = relation_ranking(s_true, t_true, 5, 12, one_hop_test, id2relation, model)\n",
    "    \n",
    "    #[... [score, r], ...]\n",
    "    temp_list = list()\n",
    "    \n",
    "    for r in id2relation:\n",
    "        \n",
    "        if r in score_dict:\n",
    "            \n",
    "            temp_list.append([score_dict[r], r])\n",
    "        \n",
    "    temp_list.sort(key = lambda x: x[0], reverse=True)\n",
    "    \n",
    "    p = 0\n",
    "    inverse_r = 0\n",
    "    exist_tri = 0\n",
    "    \n",
    "    while p < len(temp_list) and temp_list[p][1] != r_true:\n",
    "        \n",
    "        #we want to see how many inverse relaiton are ranked above true relation\n",
    "        #then, we remove them from ranking, otherwise it is not fair for us to compare our\n",
    "        #result with other model who does not consider inverse relations\n",
    "        if temp_list[p][1] % 2 != 0:\n",
    "            \n",
    "            inverse_r += 1\n",
    "        \n",
    "        #moreover, we want to remove existing triples\n",
    "        elif ((s_true, temp_list[p][1], t_true) in data_test) or (\n",
    "              (s_true, temp_list[p][1], t_true) in data):\n",
    "            \n",
    "            exist_tri += 1\n",
    "            \n",
    "        p += 1\n",
    "        \n",
    "    print('Calculating', p - inverse_r - exist_tri, i, len(selected))\n",
    "    \n",
    "#    if r_true == temp_list[0][1]:\n",
    "        \n",
    "#        Hits_at_1 += 1\n",
    "        \n",
    "#    print('Calculating hit at 1', Hits_at_1/(i+1), i, len(selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d68aec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb480e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad517182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61bd0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10b22c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb8b6a7b",
   "metadata": {},
   "source": [
    "**Model before Jan 2, 2023**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1685acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer, using integer to represent each relation type\n",
    "#note that inputs_path is the path inputs, while inputs_out_re is the output relation inputs\n",
    "fst_path = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "scd_path = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "#the relation input layer (for output embedding)\n",
    "id_rela = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "# Embed each integer in a 300-dimensional vector as input,\n",
    "# note that we add another \"space holder\" embedding, \n",
    "# which hold the spaces if the initial length of two paths are not the same\n",
    "in_embd_var = layers.Embedding(len(relation2id)+1, 300)\n",
    "\n",
    "# Obtain the embedding\n",
    "fst_p_embd = in_embd_var(fst_path)\n",
    "scd_p_embd = in_embd_var(scd_path)\n",
    "\n",
    "# Embed each integer in a 300-dimensional vector as output\n",
    "rela_embd = layers.Embedding(len(relation2id)+1, 300)(id_rela)\n",
    "\n",
    "#add 2 layer bi-directional LSTM\n",
    "lstm_layer_1 = layers.Bidirectional(layers.LSTM(150, return_sequences=True))\n",
    "lstm_layer_2 = layers.Bidirectional(layers.LSTM(150, return_sequences=False))\n",
    "\n",
    "#first LSTM layer\n",
    "fst_lstm_mid = lstm_layer_1(fst_p_embd)\n",
    "scd_lstm_mid = lstm_layer_1(scd_p_embd)\n",
    "\n",
    "#second LSTM layer\n",
    "fst_lstm_out = lstm_layer_2(fst_lstm_mid)\n",
    "scd_lstm_out = lstm_layer_2(scd_lstm_mid)\n",
    "\n",
    "#concatenate the output vector from both siamese tunnel\n",
    "path_concat = layers.concatenate([fst_lstm_out, scd_lstm_out], axis=-1)\n",
    "\n",
    "#remove the time dimension from the output embd since there is only one step\n",
    "sum_r_embd = tf.reduce_sum(rela_embd, axis=1)\n",
    "\n",
    "#concatenate the lstm output and output embd\n",
    "concat = layers.concatenate([path_concat, sum_r_embd], axis=-1)\n",
    "\n",
    "#add the dense layer\n",
    "dense_1 = layers.Dense(32, activation='relu')(concat)\n",
    "batch_norm = layers.BatchNormalization()(dense_1)\n",
    "dropout = layers.Dropout(0.25)(batch_norm)\n",
    "\n",
    "#final layer\n",
    "final_out = layers.Dense(2, activation='softmax')(dropout)\n",
    "\n",
    "#put together the model\n",
    "model = keras.Model([fst_path, scd_path, id_rela], final_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e4d56",
   "metadata": {},
   "source": [
    "**Jan 4, 2023**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f826fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "holder_len = 20000\n",
    "lower_bd = 2\n",
    "upper_bd = 10\n",
    "diff = 2\n",
    "entire_epochs = 20\n",
    "each_epoch = 2\n",
    "\n",
    "#90% to be train, 10% to be validation\n",
    "train_len = 9*int(holder_len/10)\n",
    "\n",
    "current = 0\n",
    "\n",
    "for entire_epoch in range(entire_epochs):\n",
    "    \n",
    "    ######################################\n",
    "    ###pre-define the lists###############\n",
    "    \n",
    "    #define the lists\n",
    "    x_p_list, x_p_dict, x_r_dict, y_dict, path_comb = list(), dict(), dict(), dict(), dict()\n",
    "    filled, total_num_need_to_fill = set(), 0\n",
    "\n",
    "    #build the lists first\n",
    "    for i in range(lower_bd, upper_bd+1):\n",
    "\n",
    "        Max = min(upper_bd, i+diff)\n",
    "\n",
    "        for j in range(i, Max+1):\n",
    "            \n",
    "            x_p_list.append((i,j))\n",
    "            x_p_dict[(i,j)] = {'s': [], 'l': []}\n",
    "            x_r_dict[(i,j)] = list()\n",
    "            y_dict[(i,j)] = list()\n",
    "            path_comb[(i,j)] = set()\n",
    "            total_num_need_to_fill += 1\n",
    "    \n",
    "    #######################################\n",
    "    ###build the big-batches###############\n",
    "    \n",
    "    #here is a tricky thing: if the distance between upper_bd and lower_bd is big,\n",
    "    #then when run path_finding_dynamic_programming, \n",
    "    #the chance to obtain paths with length close to lower bd is low.\n",
    "    #For instance, if lower_bd = 2, upper_bd = 10, then most found paths will have length\n",
    "    #from 6 to 10. Paths with length 2 or 3 is relatively rare.\n",
    "    #Hence, we need to first build big-batches for combination between shorter lengths.\n",
    "    #otherwise the while loop may last for really really long time,\n",
    "    #due to difficulty to find shorter paths.\n",
    "    build_big_batches(diff, holder_len, lower_bd, lower_bd, Class_2, one_hop, s_t_r,\n",
    "                          x_p_dict, x_r_dict, y_dict, path_comb, filled, total_num_need_to_fill,\n",
    "                          relation2id, entity2id, id2relation, id2entity, entire_epoch)    \n",
    "    \n",
    "    if upper_bd - lower_bd > 3:\n",
    "        \n",
    "        build_big_batches(diff, holder_len, lower_bd, lower_bd + 3, Class_2, one_hop, s_t_r,\n",
    "                          x_p_dict, x_r_dict, y_dict, path_comb, filled, total_num_need_to_fill,\n",
    "                          relation2id, entity2id, id2relation, id2entity, entire_epoch)        \n",
    "    \n",
    "    #fill in the training array list\n",
    "    build_big_batches(diff, holder_len, lower_bd, upper_bd, Class_2, one_hop, s_t_r,\n",
    "                          x_p_dict, x_r_dict, y_dict, path_comb, filled, total_num_need_to_fill,\n",
    "                          relation2id, entity2id, id2relation, id2entity, entire_epoch)\n",
    "    \n",
    "    #######################################\n",
    "    ###do the training#####################\n",
    "    for key in x_p_list:\n",
    "        \n",
    "        #generate the input arrays\n",
    "        x_train_s = np.asarray(x_p_dict[key]['s'][:train_len], dtype='int')\n",
    "        x_train_l = np.asarray(x_p_dict[key]['l'][:train_len], dtype='int')\n",
    "        x_train_r = np.asarray(x_r_dict[key][:train_len], dtype='int')\n",
    "        y_train = np.asarray(y_dict[key][:train_len], dtype='int')\n",
    "        \n",
    "        x_valid_s = np.asarray(x_p_dict[key]['s'][train_len:], dtype='int')\n",
    "        x_valid_l = np.asarray(x_p_dict[key]['l'][train_len:], dtype='int')\n",
    "        x_valid_r = np.asarray(x_r_dict[key][train_len:], dtype='int')\n",
    "        y_valid = np.asarray(y_dict[key][train_len:], dtype='int')\n",
    "        \n",
    "        print('training on length', key, 'for epoch', entire_epoch)\n",
    "        \n",
    "        #we use a bit dummy method: in order to make sure the model is saved,\n",
    "        #we save it at each length pair in each epoch!!!\n",
    "        #so unless it is the first pair first epoch, we will always read from previous checkpoint\n",
    "        if entire_epoch != 0 or key != x_p_list[0]:\n",
    "            \n",
    "            print('load model')\n",
    "            \n",
    "            model = keras.models.load_model('../weight_bin/' + model_name + '.h5')\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            #compile the model\n",
    "            model.compile(\n",
    "                loss='categorical_crossentropy',\n",
    "                optimizer=opt,\n",
    "                metrics=[\"categorical_accuracy\"],)\n",
    "\n",
    "        model.fit([x_train_s, x_train_l, x_train_r], y_train, \n",
    "                  validation_data=([x_valid_s, x_valid_l, x_valid_r], y_valid),\n",
    "                  batch_size=4, epochs=current+each_epoch, initial_epoch=current)\n",
    "        \n",
    "        current += each_epoch\n",
    "        \n",
    "        # Save model and weights\n",
    "        add_h5 = model_name + '.h5'\n",
    "        save_dir = os.path.join(os.getcwd(), '../weight_bin')\n",
    "\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        model_path = os.path.join(save_dir, add_h5)\n",
    "        model.save(model_path)\n",
    "        print('Save model')\n",
    "        del(model)\n",
    "        \n",
    "        del(x_train_s, x_train_l, x_train_r, y_train)\n",
    "        del(x_valid_s, x_valid_l, x_valid_r, y_valid)\n",
    "        \n",
    "    del(x_p_list, x_p_dict, x_r_dict, y_dict, path_comb)\n",
    "    del(filled, total_num_need_to_fill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e4501b",
   "metadata": {},
   "source": [
    "**Jan 4, 2023**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b632f315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to build all the big batches\n",
    "def build_big_batches(diff, holder_len, lower_bd, upper_bd, Class_2, one_hop, s_t_r,\n",
    "                      x_p_dict, x_r_dict, y_dict, path_comb, filled, total_num_need_to_fill,\n",
    "                      relation2id, entity2id, id2relation, id2entity, epoch):\n",
    "    \n",
    "    if holder_len % 10 != 0:\n",
    "        raise ValueError('We would like to take 10X as a big-batch size')\n",
    "    \n",
    "    #the set of all relation IDs\n",
    "    relation_id_set = set()\n",
    "    for i in range(len(id2relation)):\n",
    "        \n",
    "        if i not in id2relation:\n",
    "            raise ValueError('error when generaing id2relation')\n",
    "        \n",
    "        relation_id_set.add(i)\n",
    "    \n",
    "    num_r = len(id2relation)\n",
    "    \n",
    "    #count how many appending has performed\n",
    "    count = 0\n",
    "    \n",
    "    #we count how many combinations need to fill\n",
    "    need_to_fill = set()\n",
    "    \n",
    "    for i in range(lower_bd, upper_bd+1):\n",
    "\n",
    "        Max = min(upper_bd, i+diff)\n",
    "\n",
    "        for j in range(i, Max+1):\n",
    "            \n",
    "            need_to_fill.add((i,j))\n",
    "\n",
    "    #in case not all entities in entity2id are in one_hop, \n",
    "    #so we need to find out who are indeed in\n",
    "    existing_ids = set()\n",
    "    \n",
    "    for s_1 in one_hop:\n",
    "        existing_ids.add(s_1)\n",
    "        \n",
    "    existing_ids = list(existing_ids)\n",
    "    \n",
    "    carry_on = True\n",
    "    \n",
    "    while carry_on:\n",
    "\n",
    "        #obtain paths by dynamic programming\n",
    "        source_id = random.choice(existing_ids)\n",
    "\n",
    "        result, length_dict = Class_2.obtain_paths('direct_neighbour', source_id, \n",
    "                                                   'not_specified', lower_bd, upper_bd, one_hop)\n",
    "        for target_id in result:\n",
    "\n",
    "            if not carry_on:\n",
    "                break\n",
    "            \n",
    "            #we want to make sure s, t are indeed directly connected, \n",
    "            #otherwise there is no relation for positive sample\n",
    "            #also, we want to make sure s and t and not connected by all relations, \n",
    "            #although this situation is rare. \n",
    "            #But in that case, there is no relation for negative samples \n",
    "            if ((source_id, target_id) in s_t_r) and (\n",
    "                len(s_t_r[(source_id, target_id)]) < len(id2relation)):\n",
    "                \n",
    "                dir_r = list(s_t_r[(source_id, target_id)])\n",
    "                \n",
    "                non_dir_r = list(relation_id_set.difference(dir_r))\n",
    "                \n",
    "                if len(dir_r) <= 0:\n",
    "                    \n",
    "                    raise ValueError('errors when creating s_t_r !!')\n",
    "                \n",
    "                #iterate over path_1\n",
    "                for path_1 in result[target_id]:\n",
    "\n",
    "                    if not carry_on:\n",
    "                        break\n",
    "\n",
    "                    #iterate over path_2\n",
    "                    for path_2 in result[target_id]:\n",
    "\n",
    "                        if not carry_on:\n",
    "                            break\n",
    "\n",
    "                        #decide which path is shorter and which is longer\n",
    "                        if len(path_1) <= len(path_2):\n",
    "\n",
    "                            path_s, path_l = path_1, path_2\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            path_s, path_l = path_2, path_1                            \n",
    "\n",
    "                        #whether lengths of the two paths satisfies the requirments\n",
    "                        if (len(path_s) >= lower_bd) and (len(path_l) <= upper_bd) and (\n",
    "                            abs(len(path_s)-len(path_l)) <= diff):\n",
    "\n",
    "                            #further consider: whether the corresponding length comb is not full,\n",
    "                            #and whether this path pair is new, and whether the two paths are different\n",
    "                            #But it is optional to require the path to be new. \n",
    "                            #We may remove this requirment, especially for short paths\n",
    "                            if (len(y_dict[(len(path_s), len(path_l))]) < holder_len) and (\n",
    "                                (path_s, path_l) not in path_comb[(len(path_s), len(path_l))]) and (\n",
    "                                path_s != path_l):\n",
    "                                \n",
    "                                #we always add one positive and one negative situation together,\n",
    "                                #hence, the length of list should always be even.\n",
    "                                #also we want to make sure the length of lists coincide\n",
    "                                if (len(x_p_dict[(len(path_s), len(path_l))]['s']) != len(\n",
    "                                    y_dict[(len(path_s), len(path_l))])) or (\n",
    "                                    len(x_p_dict[(len(path_s), len(path_l))]['s']) != len(\n",
    "                                        x_p_dict[(len(path_s), len(path_l))]['l'])) or (\n",
    "                                    len(y_dict[(len(path_s), len(path_l))]) != len(\n",
    "                                        x_r_dict[(len(path_s), len(path_l))])) or (\n",
    "                                    len(y_dict[(len(path_s), len(path_l))]) % 2 != 0):\n",
    "                                    \n",
    "                                    raise ValueError('error when building big batches: length error')\n",
    "                                \n",
    "                                #####positive#####################\n",
    "                                relation_id = random.choice(dir_r)\n",
    "                                \n",
    "                                #append the paths: note that we add the space holder id at the end\n",
    "                                #of the shorter path\n",
    "                                x_p_dict[(len(path_s), len(path_l))]['s'].append(\n",
    "                                          list(path_s) + [num_r]*abs(len(path_s)-len(path_l)))\n",
    "                                x_p_dict[(len(path_s), len(path_l))]['l'].append(list(path_l))\n",
    "\n",
    "                                #append relation\n",
    "                                x_r_dict[(len(path_s), len(path_l))].append([relation_id])\n",
    "                                y_dict[(len(path_s), len(path_l))].append([1., 0.])\n",
    "                                \n",
    "                                #####negative#####################\n",
    "                                relation_id = random.choice(non_dir_r)\n",
    "                                \n",
    "                                #append the paths: note that we add the space holder id at the end\n",
    "                                #of the shorter path\n",
    "                                x_p_dict[(len(path_s), len(path_l))]['s'].append(\n",
    "                                          list(path_s) + [num_r]*abs(len(path_s)-len(path_l)))\n",
    "                                x_p_dict[(len(path_s), len(path_l))]['l'].append(list(path_l))\n",
    "\n",
    "                                #append relation\n",
    "                                x_r_dict[(len(path_s), len(path_l))].append([relation_id])\n",
    "                                y_dict[(len(path_s), len(path_l))].append([0., 1.])\n",
    "                                \n",
    "                                ######add to path combinations#####\n",
    "                                #here is the tricky part: we have to add both (path_s, path_l)\n",
    "                                #and (path_l, path_s). This is because when the length are the same\n",
    "                                #adding only one situation won't guarantee that \n",
    "                                #the same path with different order is also considered.\n",
    "                                #in other words: path combination don't have order, but our dict does.\n",
    "                                #so we have to add both situations.\n",
    "                                path_comb[(len(path_s), len(path_l))].add((path_s, path_l))\n",
    "                                path_comb[(len(path_s), len(path_l))].add((path_l, path_s))\n",
    "                                \n",
    "                                count += 1\n",
    "                                \n",
    "                                if count % 10000 == 0:\n",
    "                                    print('generating big-batches', count, \n",
    "                                          int(holder_len*0.5*len(need_to_fill)), 'in epoch', epoch)\n",
    "                                \n",
    "                            if len(y_dict[(len(path_s), len(path_l))]) >= holder_len:\n",
    "                                \n",
    "                                prev_size = len(filled)\n",
    "\n",
    "                                filled.add((len(path_s), len(path_l)))\n",
    "                                \n",
    "                                post_size = len(filled)\n",
    "                                \n",
    "                                #when we indeed find a new filled combo, we will print, which looks like:\n",
    "                                #big-batches 1 ( 2 2 ) in N completed in epoch k\n",
    "                                #big-batches 2 ( 3 5 ) in N completed in epoch k\n",
    "                                #big-batches 3 ( 3 4 ) in N completed in epoch k\n",
    "                                if post_size > prev_size:\n",
    "\n",
    "                                    print('big-batches', len(filled), \n",
    "                                          '(', len(path_s), len(path_l), ')',\n",
    "                                          'in', total_num_need_to_fill, \n",
    "                                          'completed in epoch', epoch)\n",
    "                        \n",
    "                        #check whether to finish\n",
    "                        if len(need_to_fill.difference(filled)) == 0:\n",
    "                            \n",
    "                            carry_on = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
