{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cfe7d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'nell_v4'\n",
    "model_id = 'main_4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5532c26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#difine the names for saving\n",
    "model_name = 'Model_' + model_id + '_' + data_name\n",
    "ids_name = 'IDs_' + model_id + '_' + data_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d03b5f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import opensmile\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f54f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadKG:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.x = 'Hello'\n",
    "        \n",
    "    def load_train_data(self, data_path, one_hop, data, s_t_r, entity2id, id2entity,\n",
    "                     relation2id, id2relation):\n",
    "        \n",
    "        data_ = set()\n",
    "    \n",
    "        ####load the train, valid and test set##########\n",
    "        with open (data_path, 'r') as f:\n",
    "            \n",
    "            data_ini = f.readlines()\n",
    "                        \n",
    "            for i in range(len(data_ini)):\n",
    "            \n",
    "                x = data_ini[i].split()\n",
    "                \n",
    "                x_ = tuple(x)\n",
    "                \n",
    "                data_.add(x_)\n",
    "        \n",
    "        ####relation dict#################\n",
    "        index = len(relation2id)\n",
    "     \n",
    "        for key in data_:\n",
    "            \n",
    "            if key[1] not in relation2id:\n",
    "                \n",
    "                relation = key[1]\n",
    "                \n",
    "                relation2id[relation] = index\n",
    "                \n",
    "                id2relation[index] = relation\n",
    "                \n",
    "                index += 1\n",
    "                \n",
    "                #the inverse relation\n",
    "                iv_r = '_inverse_' + relation\n",
    "                \n",
    "                relation2id[iv_r] = index\n",
    "                \n",
    "                id2relation[index] = iv_r\n",
    "                \n",
    "                index += 1\n",
    "        \n",
    "        #get the id of the inverse relation, by above definition, initial relation has \n",
    "        #always even id, while inverse relation has always odd id.\n",
    "        def inverse_r(r):\n",
    "            \n",
    "            if r % 2 == 0: #initial relation\n",
    "                \n",
    "                iv_r = r + 1\n",
    "            \n",
    "            else: #inverse relation\n",
    "                \n",
    "                iv_r = r - 1\n",
    "            \n",
    "            return(iv_r)\n",
    "        \n",
    "        ####entity dict###################\n",
    "        index = len(entity2id)\n",
    "        \n",
    "        for key in data_:\n",
    "            \n",
    "            source, target = key[0], key[2]\n",
    "            \n",
    "            if source not in entity2id:\n",
    "                                \n",
    "                entity2id[source] = index\n",
    "                \n",
    "                id2entity[index] = source\n",
    "                \n",
    "                index += 1\n",
    "            \n",
    "            if target not in entity2id:\n",
    "                \n",
    "                entity2id[target] = index\n",
    "                \n",
    "                id2entity[index] = target\n",
    "                \n",
    "                index += 1\n",
    "                \n",
    "        #create the set of triples using id instead of string        \n",
    "        for ele in data_:\n",
    "            \n",
    "            s = entity2id[ele[0]]\n",
    "            \n",
    "            r = relation2id[ele[1]]\n",
    "            \n",
    "            t = entity2id[ele[2]]\n",
    "            \n",
    "            if (s,r,t) not in data:\n",
    "                \n",
    "                data.add((s,r,t))\n",
    "            \n",
    "            s_t_r[(s,t)].add(r)\n",
    "            \n",
    "            if s not in one_hop:\n",
    "                \n",
    "                one_hop[s] = dict()\n",
    "            \n",
    "            if r not in one_hop[s]:\n",
    "                \n",
    "                one_hop[s][r] = set()\n",
    "            \n",
    "            one_hop[s][r].add(t)\n",
    "            \n",
    "            if t not in one_hop:\n",
    "                \n",
    "                one_hop[t] = dict()\n",
    "            \n",
    "            r_inv = inverse_r(r)\n",
    "            \n",
    "            s_t_r[(t,s)].add(r_inv)\n",
    "            \n",
    "            if r_inv not in one_hop[t]:\n",
    "                \n",
    "                one_hop[t][r_inv] = set()\n",
    "            \n",
    "            one_hop[t][r_inv].add(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f1b4d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObtainPathsByDynamicProgramming:\n",
    "\n",
    "    def __init__(self, size_bd=50, threshold=100000):\n",
    "                \n",
    "        self.size_bd = size_bd\n",
    "        \n",
    "        self.threshold = threshold\n",
    "    \n",
    "    '''\n",
    "    Given an entity s, here is the function to find:\n",
    "      1. any else entity t that is directely connected to s\n",
    "      2. most of the paths from s to each t with length L\n",
    "    \n",
    "    One may refer to LeetCode Problem 797 for details:\n",
    "        https://leetcode.com/problems/all-paths-from-source-to-target/\n",
    "    '''\n",
    "    def obtain_paths(self, mode, s, t_input, lower_bd, upper_bd, one_hop):\n",
    "\n",
    "        if type(lower_bd) != type(1) or lower_bd < 1:\n",
    "            \n",
    "            raise TypeError(\"!!! invalid lower bound setting, must >= 1 !!!\")\n",
    "            \n",
    "        if type(upper_bd) != type(1) or upper_bd < 1:\n",
    "            \n",
    "            raise TypeError(\"!!! invalid upper bound setting, must >= 1 !!!\")\n",
    "            \n",
    "        if lower_bd > upper_bd:\n",
    "            \n",
    "            raise TypeError(\"!!! lower bound must not exced upper bound !!!\")\n",
    "            \n",
    "        if s not in one_hop:\n",
    "            \n",
    "            raise ValueError('!!! entity not in one_hop. Please work on active entities for validation')\n",
    "        \n",
    "        #here is the result dict. Its key is each entity t that is directly connected to s\n",
    "        #The value of each t is a set containing the paths from s to t\n",
    "        #These paths can be either the direct connection r, or a multi-hop path\n",
    "        res = defaultdict(set)\n",
    "        \n",
    "        #direct_nb contains all the direct neighbour of s\n",
    "        direct_nb = set()\n",
    "        \n",
    "        if mode == 'direct_neighbour':\n",
    "        \n",
    "            for r in one_hop[s]:\n",
    "            \n",
    "                for t in one_hop[s][r]:\n",
    "                \n",
    "                    direct_nb.add(t)\n",
    "                    \n",
    "        elif mode == 'target_specified':\n",
    "            \n",
    "            direct_nb.add(t_input)\n",
    "            \n",
    "        elif mode == 'any_target':\n",
    "            \n",
    "            for s_any in one_hop:\n",
    "                \n",
    "                direct_nb.add(s_any)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            raise ValueError('not a valid mode')\n",
    "        \n",
    "        '''\n",
    "        We use recursion to find the paths\n",
    "        On current node with the path [r1, ..., rk] and on-path entities {e1, ..., ek-1, node}\n",
    "        from s to this node, we further find the direct neighbor t' of this node. \n",
    "        If t' is not a on-path entity (not among e1,...ek-1), we recursively proceed to t' \n",
    "        '''\n",
    "        def helper(node, path, on_path_en, res, direct_nb, lower_bd, upper_bd, one_hop, length_dict, count_dict):\n",
    "            \n",
    "            #when the current path is within lower_bd and upper_bd and its corresponding\n",
    "            #length still within the size_bd and its tail node is within the note dict, \n",
    "            #we will then intend to add this path\n",
    "            if (len(path) >= lower_bd) and (len(path) <= upper_bd) and (\n",
    "                node in direct_nb) and (length_dict[len(path)] < self.size_bd):\n",
    "                \n",
    "                #if this path already exists between the source entity and the current target node,\n",
    "                #we will not count it.\n",
    "                #here is an interesting situation: this path may exist between s and some other node t,\n",
    "                #however, it does not exist between s and this node t. Then, we still count it: length_dict[len(path)] += 1\n",
    "                #That is, each path may be counted for multiple times.\n",
    "                #We count how many paths we \"actually\" found between entity pairs\n",
    "                #Same type of path between different entity pairs are count separately.\n",
    "                if tuple(path) not in res[node]:\n",
    "                \n",
    "                    res[node].add(tuple(path))\n",
    "                \n",
    "                    length_dict[len(path)] += 1\n",
    "                \n",
    "            #For some rare entities, we may face such a case: so many paths are evaluated,\n",
    "            #but no entities on the paths are direct neighbors of the rare entity.\n",
    "            #In this case, the recursion cannot be bounded and stoped by the size threshold.\n",
    "            #In order to cure this, we count how many times the recursion happens on a specific length, using the count_dict.\n",
    "            #Its key is length, value counts the recursion occurred to that length. \n",
    "            #The recursion is forced to stop for that length (and hence for longer lengths) once reach the threshold.\n",
    "            if (len(path) < upper_bd) and (length_dict[len(path) + 1] < self.size_bd) and (\n",
    "                count_dict[len(path)] <= self.threshold):\n",
    "                \n",
    "                #we randomly shuffle relation r so that the reading in order is not fixed\n",
    "                temp_list = list()\n",
    "                \n",
    "                for r in one_hop[node]:\n",
    "                    \n",
    "                    temp_list.append(r)\n",
    "                \n",
    "                for i_0 in range(len(temp_list)):\n",
    "                    \n",
    "                    if count_dict[len(path)] > self.threshold:\n",
    "                        break\n",
    "                    \n",
    "                    r = random.choice(temp_list)\n",
    "                    \n",
    "                    for i_1 in range(len(one_hop[node][r])):\n",
    "                        \n",
    "                        if count_dict[len(path)] > self.threshold:\n",
    "                            break\n",
    "                        \n",
    "                        t = random.choice(list(one_hop[node][r]))\n",
    "                        \n",
    "                        if t not in on_path_en:\n",
    "                                \n",
    "                            count_dict[len(path)] += 1\n",
    "\n",
    "                            helper(t, path + [r], on_path_en.union({t}), res, direct_nb, \n",
    "                                   lower_bd, upper_bd, one_hop, length_dict, count_dict)\n",
    "        \n",
    "        length_dict = defaultdict(int)\n",
    "        count_dict = defaultdict(int)\n",
    "        \n",
    "        helper(s, [], {s}, res, direct_nb, lower_bd, upper_bd, one_hop, length_dict, count_dict)\n",
    "        \n",
    "        return(res, length_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7971ac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the classes\n",
    "Class_1 = LoadKG()\n",
    "Class_2 = ObtainPathsByDynamicProgramming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eab2c571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load ids and relation/entity dicts\n",
    "with open('../weight_bin/' + ids_name + '.pickle', 'rb') as handle:\n",
    "    Dict = pickle.load(handle)\n",
    "\n",
    "one_hop = Dict['one_hop']\n",
    "data = Dict['data']\n",
    "s_t_r = Dict['s_t_r']\n",
    "entity2id = Dict['entity2id']\n",
    "id2entity = Dict['id2entity']\n",
    "relation2id = Dict['relation2id']\n",
    "id2relation = Dict['id2relation']\n",
    "\n",
    "#we want to keep the initial entity/relation dicts\n",
    "entity2id_ini = deepcopy(entity2id)\n",
    "id2entity_ini = deepcopy(id2entity)\n",
    "relation2id_ini = deepcopy(relation2id)\n",
    "id2relation_ini = deepcopy(id2relation)\n",
    "\n",
    "num_r = len(id2relation)\n",
    "num_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db717a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-02 15:59:18.343827: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#load the model\n",
    "model = keras.models.load_model('../weight_bin/' + model_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5411f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train_path = '../data/' + data_name + '_ind/train.txt'\n",
    "ind_valid_path = '../data/' + data_name + '_ind/valid.txt'\n",
    "ind_test_path = '../data/' + data_name + '_ind/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff949dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the test dataset\n",
    "one_hop_ind = dict() \n",
    "data_ind = set()\n",
    "s_t_r_ind = defaultdict(set)\n",
    "\n",
    "len_0 = len(relation2id)\n",
    "size_0 = len(entity2id)\n",
    "\n",
    "#fill in the sets and dicts\n",
    "Class_1.load_train_data(ind_train_path, \n",
    "                        one_hop_ind, data_ind, s_t_r_ind,\n",
    "                        entity2id, id2entity, relation2id, id2relation)\n",
    "\n",
    "len_1 = len(relation2id)\n",
    "size_1 = len(entity2id)\n",
    "\n",
    "if len_0 != len_1:\n",
    "    raise ValueError('unseen relation!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88e8b10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2092 4886 7073\n"
     ]
    }
   ],
   "source": [
    "print(size_0, size_1, len(data_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dd84e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the test dataset\n",
    "one_hop_test = dict() \n",
    "data_test = set()\n",
    "s_t_r_test = defaultdict(set)\n",
    "\n",
    "len_0 = len(relation2id)\n",
    "size_0 = len(entity2id)\n",
    "\n",
    "#fill in the sets and dicts\n",
    "Class_1.load_train_data(ind_test_path, \n",
    "                        one_hop_test, data_test, s_t_r_test,\n",
    "                        entity2id, id2entity, relation2id, id2relation)\n",
    "\n",
    "\n",
    "len_1 = len(relation2id)\n",
    "size_1 = len(entity2id)\n",
    "\n",
    "if len_0 != len_1:\n",
    "    raise ValueError('unseen relation!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aebbf823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4886 4886 731\n"
     ]
    }
   ],
   "source": [
    "print(size_0, size_1, len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b231a5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the validation for existing triple removal when ranking\n",
    "one_hop_valid = dict() \n",
    "data_valid = set()\n",
    "s_t_r_valid = defaultdict(set)\n",
    "\n",
    "len_0 = len(relation2id)\n",
    "size_0 = len(entity2id)\n",
    "\n",
    "#fill in the sets and dicts\n",
    "Class_1.load_train_data(ind_valid_path, \n",
    "                        one_hop_valid, data_valid, s_t_r_valid,\n",
    "                        entity2id, id2entity, relation2id, id2relation)\n",
    "\n",
    "len_1 = len(relation2id)\n",
    "size_1 = len(entity2id)\n",
    "\n",
    "if len_0 != len_1:\n",
    "    raise ValueError('unseen relation!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05a7dc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4886 4886 716\n"
     ]
    }
   ],
   "source": [
    "print(size_0, size_1, len(data_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e6be384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nell_v4'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57db92e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_path = '../data/' + data_name + '/valid.txt'\n",
    "test_path = '../data/' + data_name + '/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "371c60a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the test dataset\n",
    "one_hop_train_test = dict() \n",
    "data_train_test = set()\n",
    "s_t_r_train_test = defaultdict(set)\n",
    "\n",
    "len_0 = len(relation2id)\n",
    "size_0 = len(entity2id)\n",
    "\n",
    "#fill in the sets and dicts\n",
    "Class_1.load_train_data(test_path, \n",
    "                        one_hop_train_test, data_train_test, s_t_r_train_test,\n",
    "                        entity2id, id2entity, relation2id, id2relation)\n",
    "\n",
    "\n",
    "len_1 = len(relation2id)\n",
    "size_1 = len(entity2id)\n",
    "\n",
    "if len_0 != len_1:\n",
    "    raise ValueError('unseen relation!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c6f0d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the validation for existing triple removal when ranking\n",
    "one_hop_train_valid = dict() \n",
    "data_train_valid = set()\n",
    "s_t_r_train_valid = defaultdict(set)\n",
    "\n",
    "len_0 = len(relation2id)\n",
    "size_0 = len(entity2id)\n",
    "\n",
    "#fill in the sets and dicts\n",
    "Class_1.load_train_data(valid_path, \n",
    "                        one_hop_train_valid, data_train_valid, s_t_r_train_valid,\n",
    "                        entity2id, id2entity, relation2id, id2relation)\n",
    "\n",
    "len_1 = len(relation2id)\n",
    "size_1 = len(entity2id)\n",
    "\n",
    "if len_0 != len_1:\n",
    "    raise ValueError('unseen relation!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f770b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to build all the big batches\n",
    "def build_big_batches(holder_len, lower_bd, upper_bd, Class_2, one_hop, s_t_r,\n",
    "                      x_p_list, x_r_list, y_list,\n",
    "                      relation2id, entity2id, id2relation, id2entity):\n",
    "    \n",
    "    if holder_len % 10 != 0:\n",
    "        raise ValueError('We would like to take 10X as a big-batch size')\n",
    "    \n",
    "    #the set of all relation IDs\n",
    "    relation_id_set = set()\n",
    "    for i in range(len(id2relation)):\n",
    "        \n",
    "        if i not in id2relation:\n",
    "            raise ValueError('error when generaing id2relation')\n",
    "        \n",
    "        relation_id_set.add(i)\n",
    "    \n",
    "    num_r = len(id2relation)\n",
    "    \n",
    "    #count how many appending has performed\n",
    "    count = 0\n",
    "\n",
    "    #in case not all entities in entity2id are in one_hop, \n",
    "    #so we need to find out who are indeed in\n",
    "    existing_ids = set()\n",
    "    \n",
    "    for s_1 in one_hop:\n",
    "        existing_ids.add(s_1)\n",
    "        \n",
    "    existing_ids = list(existing_ids)\n",
    "    \n",
    "    carry_on = True\n",
    "    \n",
    "    while carry_on:\n",
    "\n",
    "        #obtain paths by dynamic programming\n",
    "        source_id = random.choice(existing_ids)\n",
    "\n",
    "        result, length_dict = Class_2.obtain_paths('direct_neighbour', source_id, \n",
    "                                                   'not_specified', lower_bd, upper_bd, one_hop)\n",
    "        \n",
    "        #We want to increase the diversity of paths and targets.\n",
    "        #So we abandon one sub-graph from a source_id, if we sampled more than K1 path pairs\n",
    "        #Note that we mean \"sampled\", not \"appended\"! \n",
    "        #We do not care whether the pair is actually appended.\n",
    "        threshold_0 = 1000\n",
    "        count_0 = 0\n",
    "        \n",
    "        for target_id in result:\n",
    "\n",
    "            if (not carry_on) or (count_0 > threshold_0):\n",
    "                break\n",
    "            \n",
    "            #we want to make sure s, t are indeed directly connected, \n",
    "            #otherwise there is no relation for positive sample\n",
    "            #also, we want to make sure s and t and not connected by all relations, \n",
    "            #although this situation is rare. \n",
    "            #But in that case, there is no relation for negative samples\n",
    "            #Also, we want at least two different paths here between s and t\n",
    "            if ((source_id, target_id) in s_t_r) and (\n",
    "                len(s_t_r[(source_id, target_id)]) < len(id2relation)) and (\n",
    "                len(result[target_id]) >= 2):\n",
    "                \n",
    "                dir_r = list(s_t_r[(source_id, target_id)])\n",
    "                \n",
    "                non_dir_r = list(relation_id_set.difference(dir_r))\n",
    "                \n",
    "                if len(dir_r) <= 0:\n",
    "                    \n",
    "                    raise ValueError('errors when creating s_t_r !!')\n",
    "                    \n",
    "                temp_path_list = list(result[target_id])\n",
    "                    \n",
    "                #futhermore, we will abandon one targed_id if we sampled more than K2 times\n",
    "                threshold_1 = 50\n",
    "                count_1 = 0\n",
    "                \n",
    "                while count_1 <= threshold_1 and count_0 <= threshold_0:\n",
    "                \n",
    "                    temp_pair = random.sample(temp_path_list,2)\n",
    "                    \n",
    "                    path_1, path_2 = temp_pair[0], temp_pair[1]\n",
    "\n",
    "                    #decide which path is shorter and which is longer\n",
    "                    if len(path_1) <= len(path_2):\n",
    "\n",
    "                        path_s, path_l = path_1, path_2\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        path_s, path_l = path_2, path_1                            \n",
    "\n",
    "                    if (len(path_s) < lower_bd) or (len(path_l) > upper_bd):\n",
    "\n",
    "                        raise ValueError('something wrong with the path finding')\n",
    "\n",
    "                    #proceed when the entire length not yet reached,\n",
    "                    #and whether this path pair is new, and whether the two paths are different\n",
    "                    #But it is optional to require the path to be new. \n",
    "                    #We may remove this requirment, especially for short paths\n",
    "                    '''remember to cancel the comment below when using path_comb'''\n",
    "                    if (carry_on) and (path_s != path_l):\n",
    "\n",
    "                        #we always add one positive and one negative situation together,\n",
    "                        #hence, the length of list should always be even.\n",
    "                        #also we want to make sure the length of lists coincide\n",
    "                        if (len(x_p_list['s']) != len(y_list)) or (\n",
    "                            len(x_p_list['s']) != len(x_p_list['l'])) or (\n",
    "                            len(y_list) != len(x_r_list)) or (\n",
    "                            len(y_list) % 2 != 0):\n",
    "\n",
    "                            raise ValueError('error when building big batches: length error')\n",
    "\n",
    "                        #####positive#####################\n",
    "                        #we randomly choose one direction relation as the target relation\n",
    "                        relation_id = random.choice(dir_r)\n",
    "\n",
    "                        #append the paths: note that we add the space holder id at the end\n",
    "                        #of the shorter path\n",
    "                        x_p_list['s'].append(list(path_s) + [num_r]*abs(len(path_s)-upper_bd))\n",
    "                        x_p_list['l'].append(list(path_l) + [num_r]*abs(len(path_l)-upper_bd))\n",
    "\n",
    "                        #append relation\n",
    "                        x_r_list.append([relation_id])\n",
    "                        y_list.append(1.)\n",
    "\n",
    "                        #####negative#####################\n",
    "                        relation_id = random.choice(non_dir_r)\n",
    "\n",
    "                        #append the paths: note that we add the space holder id at the end\n",
    "                        #of the shorter path\n",
    "                        x_p_list['s'].append(list(path_s) + [num_r]*abs(len(path_s)-upper_bd))\n",
    "                        x_p_list['l'].append(list(path_l) + [num_r]*abs(len(path_l)-upper_bd))\n",
    "\n",
    "                        #append relation\n",
    "                        x_r_list.append([relation_id])\n",
    "                        y_list.append(0.)\n",
    "\n",
    "                        ######add to path combinations#####\n",
    "                        #here is the tricky part: we have to add both (path_s, path_l)\n",
    "                        #and (path_l, path_s). This is because when the length are the same\n",
    "                        #adding only one situation won't guarantee that \n",
    "                        #the same path with different order is also considered.\n",
    "                        #in other words: path combination don't have order, but our dict does.\n",
    "                        #so we have to add both situations.\n",
    "                        '''remember to cancel the comment here when using path_comb'''\n",
    "                        #path_comb[(len(path_s), len(path_l))].add((path_s, path_l))\n",
    "                        #path_comb[(len(path_s), len(path_l))].add((path_l, path_s))\n",
    "\n",
    "                        count += 2\n",
    "\n",
    "                        if count % 1000 == 0:\n",
    "                            print('generating big-batches', count, holder_len)\n",
    "\n",
    "                    if len(y_list) >= holder_len:\n",
    "\n",
    "                        carry_on = False\n",
    "                        \n",
    "                    count_1 += 1\n",
    "                    count_0 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c4c9127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating big-batches 1000 2000\n",
      "generating big-batches 2000 2000\n",
      "500/500 [==============================] - 9s 13ms/step - loss: 1.1861 - binary_accuracy: 0.8415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdf90e54430>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define the lists\n",
    "x_p_list, x_r_list, y_list = {'s': [], 'l': []}, list(), list()\n",
    "\n",
    "#######################################\n",
    "###build the big-batches###############      \n",
    "\n",
    "#fill in the training array list\n",
    "build_big_batches(20000, 2, 10, Class_2, one_hop_ind, s_t_r_ind,\n",
    "                      x_p_list, x_r_list, y_list,\n",
    "                      relation2id, entity2id, id2relation, id2entity)\n",
    "\n",
    "x_test_s = np.asarray(x_p_list['s'], dtype='int')\n",
    "x_test_l = np.asarray(x_p_list['l'], dtype='int')\n",
    "x_test_r = np.asarray(x_r_list, dtype='int')\n",
    "y_test = np.asarray(y_list, dtype='int')\n",
    "\n",
    "model.evaluate([x_test_s, x_test_l, x_test_r], y_test, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9dc571b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating big-batches 1000 20000\n",
      "generating big-batches 2000 20000\n",
      "generating big-batches 3000 20000\n",
      "generating big-batches 4000 20000\n",
      "generating big-batches 5000 20000\n",
      "generating big-batches 6000 20000\n",
      "generating big-batches 7000 20000\n",
      "generating big-batches 8000 20000\n",
      "generating big-batches 9000 20000\n",
      "generating big-batches 10000 20000\n",
      "generating big-batches 11000 20000\n",
      "generating big-batches 12000 20000\n",
      "generating big-batches 13000 20000\n",
      "generating big-batches 14000 20000\n",
      "generating big-batches 15000 20000\n",
      "generating big-batches 16000 20000\n",
      "generating big-batches 17000 20000\n",
      "generating big-batches 18000 20000\n",
      "generating big-batches 19000 20000\n",
      "generating big-batches 20000 20000\n",
      "5000/5000 [==============================] - 18s 4ms/step - loss: 0.7367 - binary_accuracy: 0.8666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7367185354232788, 0.8666499853134155]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define the lists\n",
    "x_p_list, x_r_list, y_list = {'s': [], 'l': []}, list(), list()\n",
    "\n",
    "#######################################\n",
    "###build the big-batches###############      \n",
    "\n",
    "#fill in the training array list\n",
    "build_big_batches(20000, 2, 10, Class_2, one_hop_train_test, s_t_r_train_test,\n",
    "                      x_p_list, x_r_list, y_list,\n",
    "                      relation2id, entity2id, id2relation, id2entity)\n",
    "\n",
    "x_test_s = np.asarray(x_p_list['s'], dtype='int')\n",
    "x_test_l = np.asarray(x_p_list['l'], dtype='int')\n",
    "x_test_r = np.asarray(x_r_list, dtype='int')\n",
    "y_test = np.asarray(y_list, dtype='int')\n",
    "\n",
    "model.evaluate([x_test_s, x_test_l, x_test_r], y_test, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af45d0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60042305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13e83a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
