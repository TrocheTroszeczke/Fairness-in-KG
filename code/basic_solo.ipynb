{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f98128",
   "metadata": {},
   "source": [
    "### Train the inductive link prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7a7dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'WN18RR_v4'\n",
    "model_id = 'SiaLP_3_new'\n",
    "lower_bound = 1\n",
    "upper_bound_path = 10\n",
    "upper_bound_subg = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54797cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#difine the names for saving\n",
    "model_name = 'Model_' + model_id + '_' + data_name\n",
    "one_hop_model_name = 'One_hop_model_' + model_id + '_' + data_name\n",
    "ids_name = 'IDs_' + model_id + '_' + data_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba371e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import opensmile\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from sklearn.utils import shuffle\n",
    "from sys import getsizeof\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c098e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadKG:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.x = 'Hello'\n",
    "        \n",
    "    def load_train_data(self, data_path, one_hop, data, s_t_r, entity2id, id2entity,\n",
    "                     relation2id, id2relation):\n",
    "        \n",
    "        data_ = set()\n",
    "    \n",
    "        ####load the train, valid and test set##########\n",
    "        with open (data_path, 'r') as f:\n",
    "            \n",
    "            data_ini = f.readlines()\n",
    "                        \n",
    "            for i in range(len(data_ini)):\n",
    "            \n",
    "                x = data_ini[i].split()\n",
    "                \n",
    "                x_ = tuple(x)\n",
    "                \n",
    "                data_.add(x_)\n",
    "        \n",
    "        ####relation dict#################\n",
    "        index = len(relation2id)\n",
    "     \n",
    "        for key in data_:\n",
    "            \n",
    "            if key[1] not in relation2id:\n",
    "                \n",
    "                relation = key[1]\n",
    "                \n",
    "                relation2id[relation] = index\n",
    "                \n",
    "                id2relation[index] = relation\n",
    "                \n",
    "                index += 1\n",
    "                \n",
    "                #the inverse relation\n",
    "                iv_r = '_inverse_' + relation\n",
    "                \n",
    "                relation2id[iv_r] = index\n",
    "                \n",
    "                id2relation[index] = iv_r\n",
    "                \n",
    "                index += 1\n",
    "        \n",
    "        #get the id of the inverse relation, by above definition, initial relation has \n",
    "        #always even id, while inverse relation has always odd id.\n",
    "        def inverse_r(r):\n",
    "            \n",
    "            if r % 2 == 0: #initial relation\n",
    "                \n",
    "                iv_r = r + 1\n",
    "            \n",
    "            else: #inverse relation\n",
    "                \n",
    "                iv_r = r - 1\n",
    "            \n",
    "            return(iv_r)\n",
    "        \n",
    "        ####entity dict###################\n",
    "        index = len(entity2id)\n",
    "        \n",
    "        for key in data_:\n",
    "            \n",
    "            source, target = key[0], key[2]\n",
    "            \n",
    "            if source not in entity2id:\n",
    "                                \n",
    "                entity2id[source] = index\n",
    "                \n",
    "                id2entity[index] = source\n",
    "                \n",
    "                index += 1\n",
    "            \n",
    "            if target not in entity2id:\n",
    "                \n",
    "                entity2id[target] = index\n",
    "                \n",
    "                id2entity[index] = target\n",
    "                \n",
    "                index += 1\n",
    "                \n",
    "        #create the set of triples using id instead of string        \n",
    "        for ele in data_:\n",
    "            \n",
    "            s = entity2id[ele[0]]\n",
    "            \n",
    "            r = relation2id[ele[1]]\n",
    "            \n",
    "            t = entity2id[ele[2]]\n",
    "            \n",
    "            if (s,r,t) not in data:\n",
    "                \n",
    "                data.add((s,r,t))\n",
    "            \n",
    "            s_t_r[(s,t)].add(r)\n",
    "            \n",
    "            if s not in one_hop:\n",
    "                \n",
    "                one_hop[s] = set()\n",
    "            \n",
    "            one_hop[s].add((r,t))\n",
    "            \n",
    "            if t not in one_hop:\n",
    "                \n",
    "                one_hop[t] = set()\n",
    "            \n",
    "            r_inv = inverse_r(r)\n",
    "            \n",
    "            s_t_r[(t,s)].add(r_inv)\n",
    "            \n",
    "            one_hop[t].add((r_inv,s))\n",
    "            \n",
    "        #change each set in one_hop to list\n",
    "        for e in one_hop:\n",
    "            \n",
    "            one_hop[e] = list(one_hop[e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd0f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObtainPathsByDynamicProgramming:\n",
    "\n",
    "    def __init__(self, amount_bd=50, size_bd=50, threshold=20000):\n",
    "        \n",
    "        self.amount_bd = amount_bd #how many Tuples we choose in one_hop[node] for next recursion\n",
    "                        \n",
    "        self.size_bd = size_bd #size bound limit the number of paths to a target entity t\n",
    "        \n",
    "        #number of times paths with specific length been performed for recursion\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    '''\n",
    "    Given an entity s, the function will find the paths from s to other entities, using recursion.\n",
    "    \n",
    "    One may refer to LeetCode Problem 797 for details:\n",
    "        https://leetcode.com/problems/all-paths-from-source-to-target/\n",
    "    '''\n",
    "    def obtain_paths(self, mode, s, t_input, lower_bd, upper_bd, one_hop):\n",
    "\n",
    "        if type(lower_bd) != type(1) or lower_bd < 1:\n",
    "            \n",
    "            raise TypeError(\"!!! invalid lower bound setting, must >= 1 !!!\")\n",
    "            \n",
    "        if type(upper_bd) != type(1) or upper_bd < 1:\n",
    "            \n",
    "            raise TypeError(\"!!! invalid upper bound setting, must >= 1 !!!\")\n",
    "            \n",
    "        if lower_bd > upper_bd:\n",
    "            \n",
    "            raise TypeError(\"!!! lower bound must not exced upper bound !!!\")\n",
    "            \n",
    "        if s not in one_hop:\n",
    "            \n",
    "            raise ValueError('!!! entity not in one_hop. Please work on existing entities')\n",
    "\n",
    "        #here is the result dict. Its key is each entity t sharing paths from s\n",
    "        #The value of each t is a set containing the paths from s to t\n",
    "        #These paths can be either the direct connection r, or a multi-hop path\n",
    "        res = defaultdict(set)\n",
    "        \n",
    "        #qualified_t contains the types of t we want to consider,\n",
    "        #that is, what t will be added to the result set.\n",
    "        qualified_t = set()\n",
    "\n",
    "        #under this mode, we will only consider the direct neighbour of s\n",
    "        if mode == 'direct_neighbour':\n",
    "        \n",
    "            for Tuple in one_hop[s]:\n",
    "            \n",
    "                t = Tuple[1]\n",
    "                \n",
    "                qualified_t.add(t)\n",
    "        \n",
    "        #under this mode, we will only consider one specified entity t\n",
    "        elif mode == 'target_specified':\n",
    "            \n",
    "            qualified_t.add(t_input)\n",
    "        \n",
    "        #under this mode, we will consider any entity\n",
    "        elif mode == 'any_target':\n",
    "            \n",
    "            for s_any in one_hop:\n",
    "                \n",
    "                qualified_t.add(s_any)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            raise ValueError('not a valid mode')\n",
    "        \n",
    "        '''\n",
    "        We use recursion to find the paths\n",
    "        On current node with the path [r1, ..., rk] and on-path entities {s, e1, ..., ek-1, node}\n",
    "        from s to this node, we will further find the direct neighbor t' of this node. \n",
    "        If t' is not an on-path entity (not among s, e1,...ek-1, node), we recursively proceed to t' \n",
    "        '''\n",
    "        def helper(node, path, on_path_en, res, qualified_t, lower_bd, upper_bd, one_hop, count_dict):\n",
    "\n",
    "            #when the current path is within lower_bd and upper_bd, \n",
    "            #and the node is among the qualified t, and it has not been fill of paths w.r.t size_limit,\n",
    "            #we will add this path to the node\n",
    "            if (len(path) >= lower_bd) and (len(path) <= upper_bd) and (\n",
    "                node in qualified_t) and (len(res[node]) < self.size_bd):\n",
    "                \n",
    "                res[node].add(tuple(path))\n",
    "                    \n",
    "            #won't start new recursions if the current path length already reaches upper limit\n",
    "            #or the number of recursions performed on this length has reached the limit\n",
    "            if (len(path) < upper_bd) and (count_dict[len(path)] <= self.threshold):\n",
    "                                \n",
    "                #temp list is the id list for us to go-over one_hop[node]\n",
    "                temp_list = [i for i in range(len(one_hop[node]))]\n",
    "                random.shuffle(temp_list) #so we random-shuffle the list\n",
    "                \n",
    "                #only take 20 recursions if there are too many (r,t)\n",
    "                for i in temp_list[:self.amount_bd]:\n",
    "                    \n",
    "                    #obtain tuple of (r,t)\n",
    "                    Tuple = one_hop[node][i]\n",
    "                    r, t = Tuple[0], Tuple[1]\n",
    "                    \n",
    "                    #add to count_dict even if eventually this step not proceed\n",
    "                    count_dict[len(path)] += 1\n",
    "                    \n",
    "                    #if t not on the path and we not exceed the computation threshold, \n",
    "                    #then finally proceed to next recursion\n",
    "                    if (t not in on_path_en) and (count_dict[len(path)] <= self.threshold):\n",
    "\n",
    "                        helper(t, path + [r], on_path_en.union({t}), res, qualified_t, \n",
    "                               lower_bd, upper_bd, one_hop, count_dict)\n",
    "\n",
    "        length_dict = defaultdict(int)\n",
    "        count_dict = defaultdict(int)\n",
    "        \n",
    "        helper(s, [], {s}, res, qualified_t, lower_bd, upper_bd, one_hop, count_dict)\n",
    "        \n",
    "        return(res, count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaf24ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/' + data_name + '/train.txt'\n",
    "valid_path = '../data/' + data_name + '/valid.txt'\n",
    "test_path = '../data/' + data_name + '/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5718867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the classes\n",
    "Class_1 = LoadKG()\n",
    "Class_2 = ObtainPathsByDynamicProgramming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c472f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the dictionaries and sets for load KG\n",
    "one_hop = dict() \n",
    "data = set()\n",
    "s_t_r = defaultdict(set)\n",
    "\n",
    "#define the dictionaries, which is shared by initail and inductive train/valid/test\n",
    "entity2id = dict()\n",
    "id2entity = dict()\n",
    "relation2id = dict()\n",
    "id2relation = dict()\n",
    "\n",
    "#fill in the sets and dicts\n",
    "Class_1.load_train_data(train_path, one_hop, data, s_t_r,\n",
    "                        entity2id, id2entity, relation2id, id2relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c4c0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the dictionaries and sets for load KG\n",
    "one_hop_valid = dict() \n",
    "data_valid = set()\n",
    "s_t_r_valid = defaultdict(set)\n",
    "\n",
    "#fill in the sets and dicts\n",
    "Class_1.load_train_data(valid_path, one_hop_valid, data_valid, s_t_r_valid,\n",
    "                        entity2id, id2entity, relation2id, id2relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178bd0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the dictionaries and sets for load KG\n",
    "one_hop_test = dict() \n",
    "data_test = set()\n",
    "s_t_r_test = defaultdict(set)\n",
    "\n",
    "#fill in the sets and dicts\n",
    "Class_1.load_train_data(test_path, one_hop_test, data_test, s_t_r_test,\n",
    "                        entity2id, id2entity, relation2id, id2relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee8babf",
   "metadata": {},
   "source": [
    "#### Build the path-based siamese neural network structure\n",
    "\n",
    "We use biLSTM to train on the input path embedding sequence to predict the output embedding or the relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68239c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer, using integer to represent each relation type\n",
    "#note that inputs_path is the path inputs, while inputs_out_re is the output relation inputs\n",
    "fst_path = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "scd_path = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "thd_path = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "#the relation input layer (for output embedding)\n",
    "id_rela = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "# Embed each integer in a 300-dimensional vector as input,\n",
    "# note that we add another \"space holder\" embedding, \n",
    "# which hold the spaces if the initial length of paths are not the same\n",
    "in_embd_var = layers.Embedding(len(relation2id)+1, 300)\n",
    "\n",
    "# Obtain the embedding\n",
    "fst_p_embd = in_embd_var(fst_path)\n",
    "scd_p_embd = in_embd_var(scd_path)\n",
    "thd_p_embd = in_embd_var(thd_path)\n",
    "\n",
    "# Embed each integer in a 300-dimensional vector as output\n",
    "rela_embd = layers.Embedding(len(relation2id)+1, 300)(id_rela)\n",
    "\n",
    "#add 2 layer bi-directional LSTM\n",
    "lstm_layer_1 = layers.Bidirectional(layers.LSTM(150, return_sequences=True))\n",
    "lstm_layer_2 = layers.Bidirectional(layers.LSTM(150, return_sequences=True))\n",
    "\n",
    "#first LSTM layer\n",
    "fst_lstm_mid = lstm_layer_1(fst_p_embd)\n",
    "scd_lstm_mid = lstm_layer_1(scd_p_embd)\n",
    "thd_lstm_mid = lstm_layer_1(thd_p_embd)\n",
    "\n",
    "#second LSTM layer\n",
    "fst_lstm_out = lstm_layer_2(fst_lstm_mid)\n",
    "scd_lstm_out = lstm_layer_2(scd_lstm_mid)\n",
    "thd_lstm_out = lstm_layer_2(thd_lstm_mid)\n",
    "\n",
    "#reduce max\n",
    "fst_reduce_max = tf.reduce_max(fst_lstm_out, axis=1)\n",
    "scd_reduce_max = tf.reduce_max(scd_lstm_out, axis=1)\n",
    "thd_reduce_max = tf.reduce_max(thd_lstm_out, axis=1)\n",
    "\n",
    "#concatenate the output vector from both siamese tunnel: (Batch, 900)\n",
    "path_concat = layers.concatenate([fst_reduce_max, scd_reduce_max, thd_reduce_max], axis=-1)\n",
    "\n",
    "#add dropout on top of the concatenation from all channels\n",
    "dropout = layers.Dropout(0.25)(path_concat)\n",
    "\n",
    "#multiply into output embd size by dense layer: (Batch, 300)\n",
    "path_out_vect = layers.Dense(300, activation='tanh')(dropout)\n",
    "\n",
    "#remove the time dimension from the output embd since there is only one step\n",
    "rela_out_embd = tf.reduce_sum(rela_embd, axis=1)\n",
    "\n",
    "# Normalize the vectors to have unit length\n",
    "path_out_vect_norm = tf.math.l2_normalize(path_out_vect, axis=-1)\n",
    "rela_out_embd_norm = tf.math.l2_normalize(rela_out_embd, axis=-1)\n",
    "\n",
    "# Calculate the dot product\n",
    "dot_product = layers.Dot(axes=-1)([path_out_vect_norm, rela_out_embd_norm])\n",
    "\n",
    "#put together the model\n",
    "model = keras.Model([fst_path, scd_path, thd_path, id_rela], dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd4009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config the Adam optimizer \n",
    "opt = keras.optimizers.Adam(learning_rate=0.0005, decay=1e-6)\n",
    "\n",
    "#compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40033a4",
   "metadata": {},
   "source": [
    "#### Build the subgraph-based siamese neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c407303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#each input is an vector with number of relations to be dim:\n",
    "#each dim represent the existence (1) or not (0) of an out-going relation from the entity\n",
    "source_path_1 = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "source_path_2 = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "source_path_3 = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "target_path_1 = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "target_path_2 = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "target_path_3 = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "#the relation input layer (for output embedding)\n",
    "id_rela_ = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "# Embed each integer in a 300-dimensional vector as input,\n",
    "# note that we add another \"space holder\" embedding, \n",
    "# which hold the spaces if the initial length of paths are not the same\n",
    "in_embd_var_ = layers.Embedding(len(relation2id)+1, 300)\n",
    "\n",
    "# Obtain the source embeddings\n",
    "source_embd_1 = in_embd_var_(source_path_1)\n",
    "source_embd_2 = in_embd_var_(source_path_2)\n",
    "source_embd_3 = in_embd_var_(source_path_3)\n",
    "\n",
    "#Obtain the target embeddings\n",
    "target_embd_1 = in_embd_var_(target_path_1)\n",
    "target_embd_2 = in_embd_var_(target_path_2)\n",
    "target_embd_3 = in_embd_var_(target_path_3)\n",
    "\n",
    "# Embed each integer in a 300-dimensional vector as output\n",
    "rela_embd_ = layers.Embedding(len(relation2id)+1, 300)(id_rela_)\n",
    "\n",
    "#add 2 layer bi-directional LSTM network\n",
    "lstm_1 = layers.Bidirectional(layers.LSTM(150, return_sequences=True))\n",
    "lstm_2 = layers.Bidirectional(layers.LSTM(150, return_sequences=True))\n",
    "\n",
    "###source lstm implimentation########\n",
    "#first LSTM layer\n",
    "source_mid_1 = lstm_1(source_embd_1)\n",
    "source_mid_2 = lstm_1(source_embd_2)\n",
    "source_mid_3 = lstm_1(source_embd_3)\n",
    "\n",
    "#second LSTM layer\n",
    "source_out_1 = lstm_2(source_mid_1)\n",
    "source_out_2 = lstm_2(source_mid_2)\n",
    "source_out_3 = lstm_2(source_mid_3)\n",
    "\n",
    "#reduce max\n",
    "source_max_1 = tf.reduce_max(source_out_1, axis=1)\n",
    "source_max_2 = tf.reduce_max(source_out_2, axis=1)\n",
    "source_max_3 = tf.reduce_max(source_out_3, axis=1)\n",
    "\n",
    "#concatenate the output vector from both siamese tunnel: (Batch, 900)\n",
    "source_concat = layers.concatenate([source_max_1, source_max_2, source_max_3], axis=-1)\n",
    "\n",
    "#add dropout on top of the concatenation from all channels\n",
    "source_dropout = layers.Dropout(0.25)(source_concat)\n",
    "\n",
    "###target lstm implimentation########\n",
    "#first LSTM layer\n",
    "target_mid_1 = lstm_1(target_embd_1)\n",
    "target_mid_2 = lstm_1(target_embd_2)\n",
    "target_mid_3 = lstm_1(target_embd_3)\n",
    "\n",
    "#second LSTM layer\n",
    "target_out_1 = lstm_2(target_mid_1)\n",
    "target_out_2 = lstm_2(target_mid_2)\n",
    "target_out_3 = lstm_2(target_mid_3)\n",
    "\n",
    "#reduce max\n",
    "target_max_1 = tf.reduce_max(target_out_1, axis=1)\n",
    "target_max_2 = tf.reduce_max(target_out_2, axis=1)\n",
    "target_max_3 = tf.reduce_max(target_out_3, axis=1)\n",
    "\n",
    "#concatenate the output vector from both siamese tunnel: (Batch, 900)\n",
    "target_concat = layers.concatenate([target_max_1, target_max_2, target_max_3], axis=-1)\n",
    "\n",
    "#add dropout on top of the concatenation from all channels\n",
    "target_dropout = layers.Dropout(0.25)(target_concat)\n",
    "\n",
    "#further concatenate source and target output embeddings: (Batch, 1800)\n",
    "final_concat = layers.concatenate([source_dropout, target_dropout], axis=-1)\n",
    "\n",
    "#multiply into output embd size by dense layer: (Batch, 300)\n",
    "out_vect = layers.Dense(300, activation='tanh')(final_concat)\n",
    "\n",
    "#remove the time dimension from the output embd since there is only one step\n",
    "rela_out_embd_ = tf.reduce_sum(rela_embd_, axis=1)\n",
    "\n",
    "# Normalize the vectors to have unit length\n",
    "out_vect_norm = tf.math.l2_normalize(out_vect, axis=-1)\n",
    "rela_out_embd_norm_ = tf.math.l2_normalize(rela_out_embd_, axis=-1)\n",
    "\n",
    "# Calculate the dot product\n",
    "dot_product_ = layers.Dot(axes=-1)([out_vect_norm, rela_out_embd_norm_])\n",
    "\n",
    "#put together the model\n",
    "model_2 = keras.Model([source_path_1, source_path_2, source_path_3,\n",
    "                       target_path_1, target_path_2, target_path_3, id_rela_], dot_product_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a1f5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config the Adam optimizer \n",
    "opt_ = keras.optimizers.Adam(learning_rate=0.0005, decay=1e-6)\n",
    "\n",
    "#compile the model\n",
    "model_2.compile(loss='binary_crossentropy', optimizer=opt_, metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fd204f",
   "metadata": {},
   "source": [
    "### Build the big-batch for path-based model\n",
    "We will build the big-batch for the path-based model training. That is, we will build three list to store three paths, respectively.\n",
    "\n",
    "In order to reduce computational complexity, we will run the path-finding algorithm for each entity e in the dataset before the training. That is, for each entity e, we will have two dictionaries. Dict 1 stores the paths between e and any other entities in the dataset. Will Dict 2 stores the paths between e and its direct neighbors. The two dicts will be used and invariant throughout the training.\n",
    "\n",
    "* At each step, three different paths between two entities s and t are selected. Each path is append to one of the list. \n",
    "* If this step is for positive samples, the existing relation r will be selected between s and t. If there are more than one relation from s to t, we randomly choose one. Also, the label list will be appended 1.\n",
    "* If this step is for negative samples, one relation that does not exist between s and t will be selected randomly and append to the relation list. Also, the label list will be appended 0.\n",
    "* In practice, the positive step is always fallowed by a negative step. The same paths in the positive step will be used in the next negative step, while the relation is a negative one chosen in the above way.\n",
    "* We do this until the length limit is reached.\n",
    "\n",
    "**For relation prediciton, we will only need to train using (s,r,t) triple. (t,r-1,s) is not necessary and hence not included in training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb3ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to build the big batche for path-based training\n",
    "def build_big_batches_path(lower_bd, upper_bd, data, one_hop, s_t_r,\n",
    "                      x_p_list, x_r_list, y_list,\n",
    "                      relation2id, entity2id, id2relation, id2entity):\n",
    "    \n",
    "    #the set of all relation IDs\n",
    "    relation_id_set = set()\n",
    "    \n",
    "    #the set of all initial relations\n",
    "    ini_r_id_set = set()\n",
    "    \n",
    "    for i in range(len(id2relation)):\n",
    "        \n",
    "        if i not in id2relation:\n",
    "            raise ValueError('error when generaing id2relation')\n",
    "        \n",
    "        relation_id_set.add(i)\n",
    "        \n",
    "        if i % 2 == 0: #initial relation id is always an even number\n",
    "            ini_r_id_set.add(i)\n",
    "    \n",
    "    num_r = len(id2relation)\n",
    "    num_ini_r = len(ini_r_id_set)\n",
    "    \n",
    "    if num_ini_r != int(num_r/2):\n",
    "        raise ValueError('error when generating id2relation')\n",
    "    \n",
    "    #in case not all entities in entity2id are in one_hop, \n",
    "    #so we need to find out who are indeed in\n",
    "    existing_ids = set()\n",
    "    \n",
    "    for s_1 in one_hop:\n",
    "        existing_ids.add(s_1)\n",
    "        \n",
    "    existing_ids = list(existing_ids)\n",
    "    random.shuffle(existing_ids)\n",
    "    \n",
    "    count = 0\n",
    "    for s in existing_ids:\n",
    "        \n",
    "        #impliment the path finding algorithm to find paths between s and t\n",
    "        result, length_dict = Class_2.obtain_paths('direct_neighbour', s, 'nb', lower_bd, upper_bd, one_hop)\n",
    "        \n",
    "        for iteration in range(10):\n",
    "\n",
    "            #proceed only if at least three paths are between s and t\n",
    "            for t in result:\n",
    "\n",
    "                if len(s_t_r[(s,t)]) == 0:\n",
    "\n",
    "                    raise ValueError(s,t,id2entity[s], id2entity[t])\n",
    "\n",
    "                #we are only interested in forward link in relation prediciton\n",
    "                ini_r_list = list()\n",
    "\n",
    "                #obtain initial relations between s and t\n",
    "                for r in s_t_r[(s,t)]:\n",
    "                    if r % 2 == 0:#initial relation id is always an even number\n",
    "                        ini_r_list.append(r)\n",
    "\n",
    "                #if there exist more than three paths between s and t, \n",
    "                #and inital connection between s and t exists,\n",
    "                #and not every r in the relation dictionary exists between s and t (although this is rare)\n",
    "                #we then proceed\n",
    "                if len(result[t]) >= 3 and len(ini_r_list) > 0 and len(ini_r_list) < int(num_ini_r):\n",
    "\n",
    "                    #obtain the list form of all the paths from s to t\n",
    "                    temp_path_list = list(result[t])\n",
    "\n",
    "                    temp_pair = random.sample(temp_path_list, 3)\n",
    "\n",
    "                    path_1, path_2, path_3 = temp_pair[0], temp_pair[1], temp_pair[2]\n",
    "\n",
    "                    #####positive#####################\n",
    "                    #append the paths: note that we add the space holder id at the end of the shorter path\n",
    "                    x_p_list['1'].append(list(path_1) + [num_r]*abs(len(path_1)-upper_bd))\n",
    "                    x_p_list['2'].append(list(path_2) + [num_r]*abs(len(path_2)-upper_bd))\n",
    "                    x_p_list['3'].append(list(path_3) + [num_r]*abs(len(path_3)-upper_bd))\n",
    "\n",
    "                    #append relation\n",
    "                    r = random.choice(ini_r_list)\n",
    "                    x_r_list.append([r])\n",
    "                    y_list.append(1.)\n",
    "\n",
    "                    #####negative#####################\n",
    "                    #append the paths: note that we add the space holder id at the end\n",
    "                    #of the shorter path\n",
    "                    x_p_list['1'].append(list(path_1) + [num_r]*abs(len(path_1)-upper_bd))\n",
    "                    x_p_list['2'].append(list(path_2) + [num_r]*abs(len(path_2)-upper_bd))\n",
    "                    x_p_list['3'].append(list(path_3) + [num_r]*abs(len(path_3)-upper_bd))\n",
    "\n",
    "                    #append relation\n",
    "                    neg_r_list = list(ini_r_id_set.difference(set(ini_r_list)))\n",
    "                    r_ran = random.choice(neg_r_list)\n",
    "                    x_r_list.append([r_ran])\n",
    "                    y_list.append(0.)\n",
    "        \n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print('generating big-batches for path-based model', count, len(existing_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f215dcc4",
   "metadata": {},
   "source": [
    "### Build the big-batch for the subgraph-based network training\n",
    "\n",
    "Again, to reduce computational complexity, we store the subgraph of each entity e at the biginning.\n",
    "\n",
    "* At each step, we will select one triple (s,r,t) from the dataset. Then, reaching out paths of s and t is generated respectively according to their out-going relations.\n",
    "* We will select three paths for each of source and target entity. Add them to the corresponding list.\n",
    "* If this is a positive sample step, the id of relation r is appended to the relation list.\n",
    "* If this is a negative sample step, the id of a random relation is appended to the relation lsit.\n",
    "* Similarly, one negative sample step always follows one positive step. The one-hop vectors from the previous positve step is used again for the negative step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c3264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again, it is too slow to run the path-finding algorithm again and again on the complete FB15K-237\n",
    "#Instead, we will find the subgraph for each entity once.\n",
    "#then in the subgraph based training, the subgraphs are stored and used for multiple times\n",
    "def store_subgraph_dicts(lower_bd, upper_bd, data, one_hop, s_t_r,\n",
    "                         relation2id, entity2id, id2relation, id2entity):\n",
    "    \n",
    "    #the set of all relation IDs\n",
    "    relation_id_set = set()\n",
    "    \n",
    "    for i in range(len(id2relation)):\n",
    "        \n",
    "        if i not in id2relation:\n",
    "            raise ValueError('error when generaing id2relation')\n",
    "        \n",
    "        relation_id_set.add(i)\n",
    "    \n",
    "    num_r = len(id2relation)\n",
    "    \n",
    "    #in case not all entities in entity2id are in one_hop, \n",
    "    #so we need to find out who are indeed in\n",
    "    existing_ids = set()\n",
    "    \n",
    "    for s_1 in one_hop:\n",
    "        existing_ids.add(s_1)\n",
    "    \n",
    "    #the ids to start path finding\n",
    "    existing_ids = list(existing_ids)\n",
    "    random.shuffle(existing_ids)\n",
    "    \n",
    "    #Dict stores the subgraph for each entity\n",
    "    Dict_1 = dict()\n",
    "    \n",
    "    count = 0\n",
    "    for s in existing_ids:\n",
    "        \n",
    "        path_set = set()\n",
    "            \n",
    "        result, length_dict = Class_2.obtain_paths('any_target', s, 'any', lower_bd, upper_bd, one_hop)\n",
    "\n",
    "        for t_ in result:\n",
    "            for path in result[t_]:\n",
    "                path_set.add(path)\n",
    "\n",
    "        del(result, length_dict)\n",
    "        \n",
    "        path_list = list(path_set)\n",
    "        \n",
    "        path_select = random.sample(path_list, min(len(path_list), 100))\n",
    "            \n",
    "        Dict_1[s] = deepcopy(path_select)\n",
    "        \n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print('generating and storing paths for the path-based model', count, len(existing_ids))\n",
    "        \n",
    "    return(Dict_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1551116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to build the big-batch for one-hope neighbor training\n",
    "def build_big_batches_subgraph(lower_bd, upper_bd, data, one_hop, s_t_r,\n",
    "                      x_s_list, x_t_list, x_r_list, y_list, Dict,\n",
    "                      relation2id, entity2id, id2relation, id2entity):\n",
    "    \n",
    "    #the set of all relation IDs\n",
    "    relation_id_set = set()\n",
    "    \n",
    "    #the set of all initial relations\n",
    "    ini_r_id_set = set()\n",
    "    \n",
    "    for i in range(len(id2relation)):\n",
    "        \n",
    "        if i not in id2relation:\n",
    "            raise ValueError('error when generaing id2relation')\n",
    "        \n",
    "        relation_id_set.add(i)\n",
    "        \n",
    "        if i % 2 == 0: #initial relation id is always an even number\n",
    "            ini_r_id_set.add(i)\n",
    "    \n",
    "    num_r = len(id2relation)\n",
    "    num_ini_r = len(ini_r_id_set)\n",
    "    \n",
    "    if num_ini_r != int(num_r/2):\n",
    "        raise ValueError('error when generating id2relation')\n",
    "        \n",
    "    #if an entity has at least three out-stretching paths, it is a qualified one\n",
    "    qualified = set()\n",
    "    for e in Dict:\n",
    "        if len(Dict[e]) >= 3:\n",
    "            qualified.add(e)\n",
    "    qualified = list(qualified)\n",
    "    \n",
    "    data = list(data)\n",
    "    \n",
    "    for iteration in range(10):\n",
    "\n",
    "        data = shuffle(data)\n",
    "\n",
    "        for i_0 in range(len(data)):\n",
    "\n",
    "            triple = data[i_0]\n",
    "\n",
    "            s, r, t = triple[0], triple[1], triple[2] #obtain entities and relation IDs\n",
    "\n",
    "            if s in qualified and t in qualified:\n",
    "\n",
    "                #obtain the path list for true entities\n",
    "                path_s, path_t = list(Dict[s]), list(Dict[t])\n",
    "\n",
    "                #####positive step###########\n",
    "                #randomly obtain three paths for true entities\n",
    "                temp_s = random.sample(path_s, 3)\n",
    "                temp_t = random.sample(path_t, 3)\n",
    "                s_p_1, s_p_2, s_p_3 = temp_s[0], temp_s[1], temp_s[2]\n",
    "                t_p_1, t_p_2, t_p_3 = temp_t[0], temp_t[1], temp_t[2]\n",
    "\n",
    "                #append the paths: note that we add the space holder id at the end of the shorter path\n",
    "                x_s_list['1'].append(list(s_p_1) + [num_r]*abs(len(s_p_1)-upper_bd))\n",
    "                x_s_list['2'].append(list(s_p_2) + [num_r]*abs(len(s_p_2)-upper_bd))\n",
    "                x_s_list['3'].append(list(s_p_3) + [num_r]*abs(len(s_p_3)-upper_bd))\n",
    "\n",
    "                x_t_list['1'].append(list(t_p_1) + [num_r]*abs(len(t_p_1)-upper_bd))\n",
    "                x_t_list['2'].append(list(t_p_2) + [num_r]*abs(len(t_p_2)-upper_bd))\n",
    "                x_t_list['3'].append(list(t_p_3) + [num_r]*abs(len(t_p_3)-upper_bd))\n",
    "\n",
    "                #append relation\n",
    "                x_r_list.append([r])\n",
    "                y_list.append(1.)\n",
    "\n",
    "                #####negative step for relation###########\n",
    "                #append the paths: note that we add the space holder id at the end of the shorter path\n",
    "                x_s_list['1'].append(list(s_p_1) + [num_r]*abs(len(s_p_1)-upper_bd))\n",
    "                x_s_list['2'].append(list(s_p_2) + [num_r]*abs(len(s_p_2)-upper_bd))\n",
    "                x_s_list['3'].append(list(s_p_3) + [num_r]*abs(len(s_p_3)-upper_bd))\n",
    "\n",
    "                x_t_list['1'].append(list(t_p_1) + [num_r]*abs(len(t_p_1)-upper_bd))\n",
    "                x_t_list['2'].append(list(t_p_2) + [num_r]*abs(len(t_p_2)-upper_bd))\n",
    "                x_t_list['3'].append(list(t_p_3) + [num_r]*abs(len(t_p_3)-upper_bd))\n",
    "\n",
    "                #append relation\n",
    "                neg_r_list = list(ini_r_id_set.difference({r}))\n",
    "                r_ran = random.choice(neg_r_list)\n",
    "                x_r_list.append([r_ran])\n",
    "                y_list.append(0.)\n",
    "                \n",
    "                ##############################################\n",
    "                ##############################################\n",
    "                #randomly choose two negative sampled entities\n",
    "                s_ran = random.choice(qualified)\n",
    "                t_ran = random.choice(qualified)\n",
    "\n",
    "                #obtain the path list for random entities\n",
    "                path_s_ran, path_t_ran = list(Dict[s_ran]), list(Dict[t_ran])\n",
    "                \n",
    "                #####positive step#################\n",
    "                #Again: randomly obtain three paths\n",
    "                temp_s = random.sample(path_s, 3)\n",
    "                temp_t = random.sample(path_t, 3)\n",
    "                s_p_1, s_p_2, s_p_3 = temp_s[0], temp_s[1], temp_s[2]\n",
    "                t_p_1, t_p_2, t_p_3 = temp_t[0], temp_t[1], temp_t[2]\n",
    "\n",
    "                #append the paths: note that we add the space holder id at the end of the shorter path\n",
    "                x_s_list['1'].append(list(s_p_1) + [num_r]*abs(len(s_p_1)-upper_bd))\n",
    "                x_s_list['2'].append(list(s_p_2) + [num_r]*abs(len(s_p_2)-upper_bd))\n",
    "                x_s_list['3'].append(list(s_p_3) + [num_r]*abs(len(s_p_3)-upper_bd))\n",
    "\n",
    "                x_t_list['1'].append(list(t_p_1) + [num_r]*abs(len(t_p_1)-upper_bd))\n",
    "                x_t_list['2'].append(list(t_p_2) + [num_r]*abs(len(t_p_2)-upper_bd))\n",
    "                x_t_list['3'].append(list(t_p_3) + [num_r]*abs(len(t_p_3)-upper_bd))\n",
    "\n",
    "                #append relation\n",
    "                x_r_list.append([r])\n",
    "                y_list.append(1.)\n",
    "\n",
    "                #####negative for source entity###########\n",
    "                #randomly obtain three paths\n",
    "                temp_s = random.sample(path_s_ran, 3)\n",
    "                s_p_1, s_p_2, s_p_3 = temp_s[0], temp_s[1], temp_s[2]\n",
    "\n",
    "                #append the paths: note that we add the space holder id at the end of the shorter path\n",
    "                x_s_list['1'].append(list(s_p_1) + [num_r]*abs(len(s_p_1)-upper_bd))\n",
    "                x_s_list['2'].append(list(s_p_2) + [num_r]*abs(len(s_p_2)-upper_bd))\n",
    "                x_s_list['3'].append(list(s_p_3) + [num_r]*abs(len(s_p_3)-upper_bd))\n",
    "\n",
    "                x_t_list['1'].append(list(t_p_1) + [num_r]*abs(len(t_p_1)-upper_bd))\n",
    "                x_t_list['2'].append(list(t_p_2) + [num_r]*abs(len(t_p_2)-upper_bd))\n",
    "                x_t_list['3'].append(list(t_p_3) + [num_r]*abs(len(t_p_3)-upper_bd))\n",
    "\n",
    "                #append relation\n",
    "                x_r_list.append([r])\n",
    "                y_list.append(0.)\n",
    "\n",
    "                #####positive step###########\n",
    "                #Again: randomly obtain three paths\n",
    "                temp_s = random.sample(path_s, 3)\n",
    "                temp_t = random.sample(path_t, 3)\n",
    "                s_p_1, s_p_2, s_p_3 = temp_s[0], temp_s[1], temp_s[2]\n",
    "                t_p_1, t_p_2, t_p_3 = temp_t[0], temp_t[1], temp_t[2]\n",
    "\n",
    "                #append the paths: note that we add the space holder id at the end of the shorter path\n",
    "                x_s_list['1'].append(list(s_p_1) + [num_r]*abs(len(s_p_1)-upper_bd))\n",
    "                x_s_list['2'].append(list(s_p_2) + [num_r]*abs(len(s_p_2)-upper_bd))\n",
    "                x_s_list['3'].append(list(s_p_3) + [num_r]*abs(len(s_p_3)-upper_bd))\n",
    "\n",
    "                x_t_list['1'].append(list(t_p_1) + [num_r]*abs(len(t_p_1)-upper_bd))\n",
    "                x_t_list['2'].append(list(t_p_2) + [num_r]*abs(len(t_p_2)-upper_bd))\n",
    "                x_t_list['3'].append(list(t_p_3) + [num_r]*abs(len(t_p_3)-upper_bd))\n",
    "\n",
    "                #append relation\n",
    "                x_r_list.append([r])\n",
    "                y_list.append(1.)\n",
    "\n",
    "                #####negative for target entity###########\n",
    "                #randomly obtain three paths\n",
    "                temp_t = random.sample(path_t_ran, 3)\n",
    "                t_p_1, t_p_2, t_p_3 = temp_t[0], temp_t[1], temp_t[2]\n",
    "\n",
    "                #append the paths: note that we add the space holder id at the end of the shorter path\n",
    "                x_s_list['1'].append(list(s_p_1) + [num_r]*abs(len(s_p_1)-upper_bd))\n",
    "                x_s_list['2'].append(list(s_p_2) + [num_r]*abs(len(s_p_2)-upper_bd))\n",
    "                x_s_list['3'].append(list(s_p_3) + [num_r]*abs(len(s_p_3)-upper_bd))\n",
    "\n",
    "                x_t_list['1'].append(list(t_p_1) + [num_r]*abs(len(t_p_1)-upper_bd))\n",
    "                x_t_list['2'].append(list(t_p_2) + [num_r]*abs(len(t_p_2)-upper_bd))\n",
    "                x_t_list['3'].append(list(t_p_3) + [num_r]*abs(len(t_p_3)-upper_bd))\n",
    "\n",
    "                #append relation\n",
    "                x_r_list.append([r])\n",
    "                y_list.append(0.)\n",
    "\n",
    "            if i_0 % 200 == 0:\n",
    "                print('generating big-batches for subgraph-based model', i_0, len(data), iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851e0ac2",
   "metadata": {},
   "source": [
    "### Start Training: load the KG and call classes\n",
    "\n",
    "Here, we use the validation set to see the training efficiency. That is, we use the validation to check whether the true relation between entities can be predicted by paths.\n",
    "\n",
    "The trick is: in validation, we have to use the same relation ID and entity ID as in the training. But we don't want to use the links in training anymore. That is, in validation, we want to use (and update if necessary) entity2id, id2entity, relation2id and id2relation. But we want to use new one_hop, data, data_ and s_t_r for validation set. Then, path-finding will also be based on new one_hop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28be7a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8103ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hop_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c02a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f57c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, we save the relation and ids\n",
    "Dict = dict()\n",
    "\n",
    "#save training data\n",
    "Dict['one_hop'] = one_hop\n",
    "Dict['data'] = data\n",
    "Dict['s_t_r'] = s_t_r\n",
    "\n",
    "#save valid data\n",
    "Dict['one_hop_valid'] = one_hop_valid\n",
    "Dict['data_valid'] = data_valid\n",
    "Dict['s_t_r_valid'] = s_t_r_valid\n",
    "\n",
    "#save test data\n",
    "Dict['one_hop_test'] = one_hop_test\n",
    "Dict['data_test'] = data_test\n",
    "Dict['s_t_r_test'] = s_t_r_test\n",
    "\n",
    "#save shared dictionaries\n",
    "Dict['entity2id'] = entity2id\n",
    "Dict['id2entity'] = id2entity\n",
    "Dict['relation2id'] = relation2id\n",
    "Dict['id2relation'] = id2relation\n",
    "\n",
    "with open('../weight_bin/' + ids_name + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(Dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a5deac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###train the path-based model\n",
    "lower_bd = lower_bound\n",
    "upper_bd = upper_bound_path\n",
    "num_epoch = 10\n",
    "batch_size = 32\n",
    "        \n",
    "#define the training lists\n",
    "train_p_list, train_r_list, train_y_list = {'1': [], '2': [], '3': []}, list(), list()\n",
    "\n",
    "#define the validation lists\n",
    "valid_p_list, valid_r_list, valid_y_list = {'1': [], '2': [], '3': []}, list(), list()\n",
    "\n",
    "#######################################\n",
    "###build the big-batches###############      \n",
    "\n",
    "#fill in the training array list\n",
    "build_big_batches_path(lower_bd, upper_bd, data, one_hop, s_t_r,\n",
    "                      train_p_list, train_r_list, train_y_list,\n",
    "                      relation2id, entity2id, id2relation, id2entity)\n",
    "\n",
    "#fill in the validation array list\n",
    "build_big_batches_path(lower_bd, upper_bd, data_valid, one_hop_valid, s_t_r_valid,\n",
    "                      valid_p_list, valid_r_list, valid_y_list,\n",
    "                      relation2id, entity2id, id2relation, id2entity)    \n",
    "\n",
    "#######################################\n",
    "###do the training#####################\n",
    "#sometimes the validation dataset is so small so sparse, \n",
    "#which cannot find three paths between any pair of s and t.\n",
    "#in such a case, we will divide the training big-batch into train and valid\n",
    "if len(valid_y_list) >= 100:\n",
    "    #generate the input arrays\n",
    "    x_train_1 = np.asarray(train_p_list['1'], dtype='int')\n",
    "    x_train_2 = np.asarray(train_p_list['2'], dtype='int')\n",
    "    x_train_3 = np.asarray(train_p_list['3'], dtype='int')\n",
    "    x_train_r = np.asarray(train_r_list, dtype='int')\n",
    "    y_train = np.asarray(train_y_list, dtype='int')\n",
    "\n",
    "    #generate the validation arrays\n",
    "    x_valid_1 = np.asarray(valid_p_list['1'], dtype='int')\n",
    "    x_valid_2 = np.asarray(valid_p_list['2'], dtype='int')\n",
    "    x_valid_3 = np.asarray(valid_p_list['3'], dtype='int')\n",
    "    x_valid_r = np.asarray(valid_r_list, dtype='int')\n",
    "    y_valid = np.asarray(valid_y_list, dtype='int')\n",
    "\n",
    "else:\n",
    "    split = int(len(train_y_list)*0.8)\n",
    "    #generate the input arrays\n",
    "    x_train_1 = np.asarray(train_p_list['1'][:split], dtype='int')\n",
    "    x_train_2 = np.asarray(train_p_list['2'][:split], dtype='int')\n",
    "    x_train_3 = np.asarray(train_p_list['3'][:split], dtype='int')\n",
    "    x_train_r = np.asarray(train_r_list[:split], dtype='int')\n",
    "    y_train = np.asarray(train_y_list[:split], dtype='int')\n",
    "\n",
    "    #generate the validation arrays\n",
    "    x_valid_1 = np.asarray(train_p_list['1'][split:], dtype='int')\n",
    "    x_valid_2 = np.asarray(train_p_list['2'][split:], dtype='int')\n",
    "    x_valid_3 = np.asarray(train_p_list['3'][split:], dtype='int')\n",
    "    x_valid_r = np.asarray(train_r_list[split:], dtype='int')\n",
    "    y_valid = np.asarray(train_y_list[split:], dtype='int')\n",
    "\n",
    "#do the training\n",
    "model.fit([x_train_1, x_train_2, x_train_3, x_train_r], y_train, \n",
    "          validation_data=([x_valid_1, x_valid_2, x_valid_3, x_valid_r], y_valid),\n",
    "          batch_size=batch_size, epochs=num_epoch)   \n",
    "\n",
    "# Save model and weights\n",
    "add_h5 = model_name + '.h5'\n",
    "save_dir = os.path.join(os.getcwd(), '../weight_bin')\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, add_h5)\n",
    "model.save(model_path)\n",
    "print('Save model')\n",
    "del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89853c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###train the subgraph-based model\n",
    "lower_bd = lower_bound\n",
    "upper_bd = upper_bound_subg\n",
    "num_epoch = 10\n",
    "batch_size = 32\n",
    "\n",
    "Dict_train = store_subgraph_dicts(lower_bd, upper_bd, data, one_hop, s_t_r,\n",
    "                         relation2id, entity2id, id2relation, id2entity)\n",
    "\n",
    "Dict_valid = store_subgraph_dicts(lower_bd, upper_bd, data_valid, one_hop_valid, s_t_r_valid,\n",
    "                         relation2id, entity2id, id2relation, id2entity)\n",
    "        \n",
    "#define the training lists\n",
    "train_s_list, train_t_list, train_r_list, train_y_list = {'1': [], '2': [], '3': []}, {'1': [], '2': [], '3': []}, list(), list()\n",
    "\n",
    "#define the validation lists\n",
    "valid_s_list, valid_t_list, valid_r_list, valid_y_list = {'1': [], '2': [], '3': []}, {'1': [], '2': [], '3': []}, list(), list()\n",
    "\n",
    "#######################################\n",
    "###build the big-batches###############      \n",
    "\n",
    "#fill in the training array list\n",
    "build_big_batches_subgraph(lower_bd, upper_bd, data, one_hop, s_t_r,\n",
    "                      train_s_list, train_t_list, train_r_list, train_y_list, Dict_train,\n",
    "                      relation2id, entity2id, id2relation, id2entity)\n",
    "\n",
    "#fill in the validation array list\n",
    "build_big_batches_subgraph(lower_bd, upper_bd, data_valid, one_hop_valid, s_t_r_valid,\n",
    "                      valid_s_list, valid_t_list, valid_r_list, valid_y_list, Dict_valid,\n",
    "                      relation2id, entity2id, id2relation, id2entity)    \n",
    "\n",
    "#######################################\n",
    "###do the training#####################\n",
    "#sometimes the validation dataset is so small so sparse, \n",
    "#which cannot find three paths between any pair of s and t.\n",
    "#in such a case, we will divide the training big-batch into train and valid\n",
    "if len(valid_y_list) >= 100:\n",
    "    #generate the input arrays\n",
    "    x_train_s_1 = np.asarray(train_s_list['1'], dtype='int')\n",
    "    x_train_s_2 = np.asarray(train_s_list['2'], dtype='int')\n",
    "    x_train_s_3 = np.asarray(train_s_list['3'], dtype='int')\n",
    "\n",
    "    x_train_t_1 = np.asarray(train_t_list['1'], dtype='int')\n",
    "    x_train_t_2 = np.asarray(train_t_list['2'], dtype='int')\n",
    "    x_train_t_3 = np.asarray(train_t_list['3'], dtype='int')\n",
    "\n",
    "    x_train_r = np.asarray(train_r_list, dtype='int')\n",
    "    y_train = np.asarray(train_y_list, dtype='int')\n",
    "\n",
    "    #generate the validation arrays\n",
    "    x_valid_s_1 = np.asarray(valid_s_list['1'], dtype='int')\n",
    "    x_valid_s_2 = np.asarray(valid_s_list['2'], dtype='int')\n",
    "    x_valid_s_3 = np.asarray(valid_s_list['3'], dtype='int')\n",
    "\n",
    "    x_valid_t_1 = np.asarray(valid_t_list['1'], dtype='int')\n",
    "    x_valid_t_2 = np.asarray(valid_t_list['2'], dtype='int')\n",
    "    x_valid_t_3 = np.asarray(valid_t_list['3'], dtype='int')\n",
    "\n",
    "    x_valid_r = np.asarray(valid_r_list, dtype='int')\n",
    "    y_valid = np.asarray(valid_y_list, dtype='int')\n",
    "\n",
    "else:\n",
    "    split = int(len(train_y_list)*0.8)\n",
    "    #generate the input arrays\n",
    "    x_train_s_1 = np.asarray(train_s_list['1'][:split], dtype='int')\n",
    "    x_train_s_2 = np.asarray(train_s_list['2'][:split], dtype='int')\n",
    "    x_train_s_3 = np.asarray(train_s_list['3'][:split], dtype='int')\n",
    "\n",
    "    x_train_t_1 = np.asarray(train_t_list['1'][:split], dtype='int')\n",
    "    x_train_t_2 = np.asarray(train_t_list['2'][:split], dtype='int')\n",
    "    x_train_t_3 = np.asarray(train_t_list['3'][:split], dtype='int')\n",
    "\n",
    "    x_train_r = np.asarray(train_r_list[:split], dtype='int')\n",
    "    y_train = np.asarray(train_y_list[:split], dtype='int')\n",
    "\n",
    "    #generate the validation arrays\n",
    "    x_valid_s_1 = np.asarray(train_s_list['1'][split:], dtype='int')\n",
    "    x_valid_s_2 = np.asarray(train_s_list['2'][split:], dtype='int')\n",
    "    x_valid_s_3 = np.asarray(train_s_list['3'][split:], dtype='int')\n",
    "\n",
    "    x_valid_t_1 = np.asarray(train_t_list['1'][split:], dtype='int')\n",
    "    x_valid_t_2 = np.asarray(train_t_list['2'][split:], dtype='int')\n",
    "    x_valid_t_3 = np.asarray(train_t_list['3'][split:], dtype='int')\n",
    "\n",
    "    x_valid_r = np.asarray(train_r_list[split:], dtype='int')\n",
    "    y_valid = np.asarray(train_y_list[split:], dtype='int')\n",
    "\n",
    "#do the training\n",
    "model_2.fit([x_train_s_1, x_train_s_2, x_train_s_3, x_train_t_1, x_train_t_2, x_train_t_3, x_train_r], y_train, \n",
    "          validation_data=([x_valid_s_1, x_valid_s_2, x_valid_s_3, x_valid_t_1, x_valid_t_2, x_valid_t_3, x_valid_r], y_valid),\n",
    "          batch_size=batch_size, epochs=num_epoch)\n",
    "\n",
    "# Save model and weights\n",
    "one_hop_add_h5 = one_hop_model_name + '.h5'\n",
    "one_hop_save_dir = os.path.join(os.getcwd(), '../weight_bin')\n",
    "\n",
    "if not os.path.isdir(one_hop_save_dir):\n",
    "    os.makedirs(one_hop_save_dir)\n",
    "one_hop_model_path = os.path.join(one_hop_save_dir, one_hop_add_h5)\n",
    "model_2.save(one_hop_model_path)\n",
    "print('Save model')\n",
    "del(model_2, Dict_train, Dict_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a74957c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2178e81c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f9152c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cbcb96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4df421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867e4d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f86bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f1d82b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe787997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da50338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d51f71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9543feff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea551344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8526f0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ef907e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e528e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f4eca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9729af14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b107dff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c689768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14b6bf00",
   "metadata": {},
   "source": [
    "### Result on the testset for inductive link prediction\n",
    "\n",
    "We use the testset for inductive link prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ef966",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'WN18RR_v4'\n",
    "model_id = 'SiaLP_3_new'\n",
    "lower_bound = 1\n",
    "upper_bound_path = 10\n",
    "upper_bound_subg = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e2a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#difine the names for saving\n",
    "model_name = 'Model_' + model_id + '_' + data_name\n",
    "one_hop_model_name = 'One_hop_model_' + model_id + '_' + data_name\n",
    "ids_name = 'IDs_' + model_id + '_' + data_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c59ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae165f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hop_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87cc2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f959af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import opensmile\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c81c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadKG:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.x = 'Hello'\n",
    "        \n",
    "    def load_train_data(self, data_path, one_hop, data, s_t_r, entity2id, id2entity,\n",
    "                     relation2id, id2relation):\n",
    "        \n",
    "        data_ = set()\n",
    "    \n",
    "        ####load the train, valid and test set##########\n",
    "        with open (data_path, 'r') as f:\n",
    "            \n",
    "            data_ini = f.readlines()\n",
    "                        \n",
    "            for i in range(len(data_ini)):\n",
    "            \n",
    "                x = data_ini[i].split()\n",
    "                \n",
    "                x_ = tuple(x)\n",
    "                \n",
    "                data_.add(x_)\n",
    "        \n",
    "        ####relation dict#################\n",
    "        index = len(relation2id)\n",
    "     \n",
    "        for key in data_:\n",
    "            \n",
    "            if key[1] not in relation2id:\n",
    "                \n",
    "                relation = key[1]\n",
    "                \n",
    "                relation2id[relation] = index\n",
    "                \n",
    "                id2relation[index] = relation\n",
    "                \n",
    "                index += 1\n",
    "                \n",
    "                #the inverse relation\n",
    "                iv_r = '_inverse_' + relation\n",
    "                \n",
    "                relation2id[iv_r] = index\n",
    "                \n",
    "                id2relation[index] = iv_r\n",
    "                \n",
    "                index += 1\n",
    "        \n",
    "        #get the id of the inverse relation, by above definition, initial relation has \n",
    "        #always even id, while inverse relation has always odd id.\n",
    "        def inverse_r(r):\n",
    "            \n",
    "            if r % 2 == 0: #initial relation\n",
    "                \n",
    "                iv_r = r + 1\n",
    "            \n",
    "            else: #inverse relation\n",
    "                \n",
    "                iv_r = r - 1\n",
    "            \n",
    "            return(iv_r)\n",
    "        \n",
    "        ####entity dict###################\n",
    "        index = len(entity2id)\n",
    "        \n",
    "        for key in data_:\n",
    "            \n",
    "            source, target = key[0], key[2]\n",
    "            \n",
    "            if source not in entity2id:\n",
    "                                \n",
    "                entity2id[source] = index\n",
    "                \n",
    "                id2entity[index] = source\n",
    "                \n",
    "                index += 1\n",
    "            \n",
    "            if target not in entity2id:\n",
    "                \n",
    "                entity2id[target] = index\n",
    "                \n",
    "                id2entity[index] = target\n",
    "                \n",
    "                index += 1\n",
    "                \n",
    "        #create the set of triples using id instead of string        \n",
    "        for ele in data_:\n",
    "            \n",
    "            s = entity2id[ele[0]]\n",
    "            \n",
    "            r = relation2id[ele[1]]\n",
    "            \n",
    "            t = entity2id[ele[2]]\n",
    "            \n",
    "            if (s,r,t) not in data:\n",
    "                \n",
    "                data.add((s,r,t))\n",
    "            \n",
    "            s_t_r[(s,t)].add(r)\n",
    "            \n",
    "            if s not in one_hop:\n",
    "                \n",
    "                one_hop[s] = set()\n",
    "            \n",
    "            one_hop[s].add((r,t))\n",
    "            \n",
    "            if t not in one_hop:\n",
    "                \n",
    "                one_hop[t] = set()\n",
    "            \n",
    "            r_inv = inverse_r(r)\n",
    "            \n",
    "            s_t_r[(t,s)].add(r_inv)\n",
    "            \n",
    "            one_hop[t].add((r_inv,s))\n",
    "            \n",
    "        #change each set in one_hop to list\n",
    "        for e in one_hop:\n",
    "            \n",
    "            one_hop[e] = list(one_hop[e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a714e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObtainPathsByDynamicProgramming:\n",
    "\n",
    "    def __init__(self, amount_bd=50, size_bd=50, threshold=20000):\n",
    "        \n",
    "        self.amount_bd = amount_bd #how many Tuples we choose in one_hop[node] for next recursion\n",
    "                        \n",
    "        self.size_bd = size_bd #size bound limit the number of paths to a target entity t\n",
    "        \n",
    "        #number of times paths with specific length been performed for recursion\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    '''\n",
    "    Given an entity s, the function will find the paths from s to other entities, using recursion.\n",
    "    \n",
    "    One may refer to LeetCode Problem 797 for details:\n",
    "        https://leetcode.com/problems/all-paths-from-source-to-target/\n",
    "    '''\n",
    "    def obtain_paths(self, mode, s, t_input, lower_bd, upper_bd, one_hop):\n",
    "\n",
    "        if type(lower_bd) != type(1) or lower_bd < 1:\n",
    "            \n",
    "            raise TypeError(\"!!! invalid lower bound setting, must >= 1 !!!\")\n",
    "            \n",
    "        if type(upper_bd) != type(1) or upper_bd < 1:\n",
    "            \n",
    "            raise TypeError(\"!!! invalid upper bound setting, must >= 1 !!!\")\n",
    "            \n",
    "        if lower_bd > upper_bd:\n",
    "            \n",
    "            raise TypeError(\"!!! lower bound must not exced upper bound !!!\")\n",
    "            \n",
    "        if s not in one_hop:\n",
    "            \n",
    "            raise ValueError('!!! entity not in one_hop. Please work on existing entities')\n",
    "\n",
    "        #here is the result dict. Its key is each entity t sharing paths from s\n",
    "        #The value of each t is a set containing the paths from s to t\n",
    "        #These paths can be either the direct connection r, or a multi-hop path\n",
    "        res = defaultdict(set)\n",
    "        \n",
    "        #qualified_t contains the types of t we want to consider,\n",
    "        #that is, what t will be added to the result set.\n",
    "        qualified_t = set()\n",
    "\n",
    "        #under this mode, we will only consider the direct neighbour of s\n",
    "        if mode == 'direct_neighbour':\n",
    "        \n",
    "            for Tuple in one_hop[s]:\n",
    "            \n",
    "                t = Tuple[1]\n",
    "                \n",
    "                qualified_t.add(t)\n",
    "        \n",
    "        #under this mode, we will only consider one specified entity t\n",
    "        elif mode == 'target_specified':\n",
    "            \n",
    "            qualified_t.add(t_input)\n",
    "        \n",
    "        #under this mode, we will consider any entity\n",
    "        elif mode == 'any_target':\n",
    "            \n",
    "            for s_any in one_hop:\n",
    "                \n",
    "                qualified_t.add(s_any)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            raise ValueError('not a valid mode')\n",
    "        \n",
    "        '''\n",
    "        We use recursion to find the paths\n",
    "        On current node with the path [r1, ..., rk] and on-path entities {s, e1, ..., ek-1, node}\n",
    "        from s to this node, we will further find the direct neighbor t' of this node. \n",
    "        If t' is not an on-path entity (not among s, e1,...ek-1, node), we recursively proceed to t' \n",
    "        '''\n",
    "        def helper(node, path, on_path_en, res, qualified_t, lower_bd, upper_bd, one_hop, count_dict):\n",
    "\n",
    "            #when the current path is within lower_bd and upper_bd, \n",
    "            #and the node is among the qualified t, and it has not been fill of paths w.r.t size_limit,\n",
    "            #we will add this path to the node\n",
    "            if (len(path) >= lower_bd) and (len(path) <= upper_bd) and (\n",
    "                node in qualified_t) and (len(res[node]) < self.size_bd):\n",
    "                \n",
    "                res[node].add(tuple(path))\n",
    "                    \n",
    "            #won't start new recursions if the current path length already reaches upper limit\n",
    "            #or the number of recursions performed on this length has reached the limit\n",
    "            if (len(path) < upper_bd) and (count_dict[len(path)] <= self.threshold):\n",
    "                                \n",
    "                #temp list is the id list for us to go-over one_hop[node]\n",
    "                temp_list = [i for i in range(len(one_hop[node]))]\n",
    "                random.shuffle(temp_list) #so we random-shuffle the list\n",
    "                \n",
    "                #only take 20 recursions if there are too many (r,t)\n",
    "                for i in temp_list[:self.amount_bd]:\n",
    "                    \n",
    "                    #obtain tuple of (r,t)\n",
    "                    Tuple = one_hop[node][i]\n",
    "                    r, t = Tuple[0], Tuple[1]\n",
    "                    \n",
    "                    #add to count_dict even if eventually this step not proceed\n",
    "                    count_dict[len(path)] += 1\n",
    "                    \n",
    "                    #if t not on the path and we not exceed the computation threshold, \n",
    "                    #then finally proceed to next recursion\n",
    "                    if (t not in on_path_en) and (count_dict[len(path)] <= self.threshold):\n",
    "\n",
    "                        helper(t, path + [r], on_path_en.union({t}), res, qualified_t, \n",
    "                               lower_bd, upper_bd, one_hop, count_dict)\n",
    "\n",
    "        length_dict = defaultdict(int)\n",
    "        count_dict = defaultdict(int)\n",
    "        \n",
    "        helper(s, [], {s}, res, qualified_t, lower_bd, upper_bd, one_hop, count_dict)\n",
    "        \n",
    "        return(res, count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d1f8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the classes\n",
    "Class_1 = LoadKG()\n",
    "Class_2 = ObtainPathsByDynamicProgramming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f13661",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load ids and relation/entity dicts\n",
    "with open('../weight_bin/' + ids_name + '.pickle', 'rb') as handle:\n",
    "    Dict = pickle.load(handle)\n",
    "    \n",
    "#save training data\n",
    "one_hop = Dict['one_hop']\n",
    "data = Dict['data']\n",
    "s_t_r = Dict['s_t_r']\n",
    "\n",
    "#save valid data\n",
    "one_hop_valid = Dict['one_hop_valid']\n",
    "data_valid = Dict['data_valid']\n",
    "s_t_r_valid = Dict['s_t_r_valid']\n",
    "\n",
    "#save test data\n",
    "one_hop_test = Dict['one_hop_test']\n",
    "data_test = Dict['data_test']\n",
    "s_t_r_test = Dict['s_t_r_test']\n",
    "\n",
    "#save shared dictionaries\n",
    "entity2id = Dict['entity2id']\n",
    "id2entity = Dict['id2entity']\n",
    "relation2id = Dict['relation2id']\n",
    "id2relation = Dict['id2relation']\n",
    "\n",
    "#we want to keep the initial entity/relation dicts before adding new entities\n",
    "entity2id_ini = deepcopy(entity2id)\n",
    "id2entity_ini = deepcopy(id2entity)\n",
    "relation2id_ini = deepcopy(relation2id)\n",
    "id2relation_ini = deepcopy(id2relation)\n",
    "\n",
    "num_r = len(id2relation)\n",
    "num_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74048a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027883e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c64cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "model = keras.models.load_model('../weight_bin/' + model_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e34e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the one-hop neighbor model\n",
    "model_2 = keras.models.load_model('../weight_bin/' + one_hop_model_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ea521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train_path = '../data/' + data_name + '_ind/train.txt'\n",
    "ind_valid_path = '../data/' + data_name + '_ind/valid.txt'\n",
    "ind_test_path = '../data/' + data_name + '_ind/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2e6087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the test dataset\n",
    "one_hop_ind = dict() \n",
    "data_ind = set()\n",
    "s_t_r_ind = defaultdict(set)\n",
    "\n",
    "len_0 = len(relation2id)\n",
    "size_0 = len(entity2id)\n",
    "\n",
    "#fill in the sets and dicts\n",
    "Class_1.load_train_data(ind_train_path, \n",
    "                        one_hop_ind, data_ind, s_t_r_ind,\n",
    "                        entity2id, id2entity, relation2id, id2relation)\n",
    "\n",
    "len_1 = len(relation2id)\n",
    "size_1 = len(entity2id)\n",
    "\n",
    "if len_0 != len_1:\n",
    "    raise ValueError('unseen relation!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6dfe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(size_0, size_1, len(data_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cec98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the test dataset\n",
    "one_hop_ind_test = dict() \n",
    "data_ind_test = set()\n",
    "s_t_r_ind_test = defaultdict(set)\n",
    "\n",
    "len_0 = len(relation2id)\n",
    "size_0 = len(entity2id)\n",
    "\n",
    "#fill in the sets and dicts\n",
    "Class_1.load_train_data(ind_test_path, \n",
    "                        one_hop_ind_test, data_ind_test, s_t_r_ind_test,\n",
    "                        entity2id, id2entity, relation2id, id2relation)\n",
    "\n",
    "\n",
    "len_1 = len(relation2id)\n",
    "size_1 = len(entity2id)\n",
    "\n",
    "if len_0 != len_1:\n",
    "    raise ValueError('unseen relation!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d18fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(size_0, size_1, len(data_ind_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757526ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the validation for existing triple removal when ranking\n",
    "one_hop_ind_valid = dict() \n",
    "data_ind_valid = set()\n",
    "s_t_r_ind_valid = defaultdict(set)\n",
    "\n",
    "len_0 = len(relation2id)\n",
    "size_0 = len(entity2id)\n",
    "\n",
    "#fill in the sets and dicts\n",
    "Class_1.load_train_data(ind_valid_path, \n",
    "                        one_hop_ind_valid, data_ind_valid, s_t_r_ind_valid,\n",
    "                        entity2id, id2entity, relation2id, id2relation)\n",
    "\n",
    "len_1 = len(relation2id)\n",
    "size_1 = len(entity2id)\n",
    "\n",
    "if len_0 != len_1:\n",
    "    raise ValueError('unseen relation!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2980a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(size_0, size_1, len(data_ind_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f17ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(entity2id), len(entity2id_ini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1cda6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain all the inital entities and new entities\n",
    "ini_ent_set, new_ent_set, all_ent_set = set(), set(), set()\n",
    "\n",
    "for ID in id2entity:\n",
    "    all_ent_set.add(ID)\n",
    "    if ID in id2entity_ini:\n",
    "        ini_ent_set.add(ID)\n",
    "    else:\n",
    "        new_ent_set.add(ID)\n",
    "        \n",
    "print(len(ini_ent_set), len(new_ent_set), len(all_ent_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915ad29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want to check whether there are overlapping \n",
    "#between the entities of train triples and inductive test and valid triples\n",
    "overlapping = 0\n",
    "\n",
    "for ele in data_ind_test:\n",
    "    \n",
    "    s, r, t = ele[0], ele[1], ele[2]\n",
    "    \n",
    "    if s in id2entity_ini or t in id2entity_ini:\n",
    "        \n",
    "        overlapping += 1\n",
    "        \n",
    "overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eb1e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapping = 0\n",
    "\n",
    "for ele in data_ind_valid:\n",
    "    \n",
    "    s, r, t = ele[0], ele[1], ele[2]\n",
    "    \n",
    "    if s in id2entity_ini or t in id2entity_ini:\n",
    "        \n",
    "        overlapping += 1\n",
    "        \n",
    "overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3c9dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want to check whether there are overlapping \n",
    "#between the entities of train triples and inductive test and valid triples\n",
    "overlapping = 0\n",
    "\n",
    "for ele in data_ind:\n",
    "    \n",
    "    s, r, t = ele[0], ele[1], ele[2]\n",
    "    \n",
    "    if s in id2entity_ini or t in id2entity_ini:\n",
    "        \n",
    "        overlapping += 1\n",
    "        \n",
    "overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ea5533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the function to do path-based relation scoring\n",
    "def path_based_relation_scoring(s, t, lower_bd, upper_bd, one_hop, id2relation, model):\n",
    "    \n",
    "    path_holder = set()\n",
    "    \n",
    "    for iteration in range(3):\n",
    "    \n",
    "        result, length_dict = Class_2.obtain_paths('target_specified', \n",
    "                                                   s, t, lower_bd, upper_bd, one_hop)\n",
    "        if t in result:\n",
    "            \n",
    "            for path in result[t]:\n",
    "                \n",
    "                path_holder.add(path)\n",
    "                \n",
    "        del(result, length_dict)\n",
    "    \n",
    "    path_holder = list(path_holder)\n",
    "    random.shuffle(path_holder)\n",
    "    \n",
    "    score_dict = defaultdict(float)\n",
    "    count_dict = defaultdict(int)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    if len(path_holder) >= 3:\n",
    "    \n",
    "        #iterate over path_1\n",
    "        while count < 10:\n",
    "\n",
    "            temp_pair = random.sample(path_holder, 3)\n",
    "\n",
    "            path_1, path_2, path_3 = temp_pair[0], temp_pair[1], temp_pair[2]\n",
    "\n",
    "            list_1 = list()\n",
    "            list_2 = list()\n",
    "            list_3 = list()\n",
    "            list_r = list()\n",
    "\n",
    "            for i in range(len(id2relation)):\n",
    "\n",
    "                if i not in id2relation:\n",
    "\n",
    "                    raise ValueError ('error when generating id2relation')\n",
    "                \n",
    "                #only care about initial relations\n",
    "                if i % 2 == 0:\n",
    "\n",
    "                    list_1.append(list(path_1) + [num_r]*abs(len(path_1)-upper_bd))\n",
    "                    list_2.append(list(path_2) + [num_r]*abs(len(path_2)-upper_bd))\n",
    "                    list_3.append(list(path_3) + [num_r]*abs(len(path_3)-upper_bd))\n",
    "                    list_r.append([i])\n",
    "            \n",
    "            #change to arrays\n",
    "            input_1 = np.array(list_1)\n",
    "            input_2 = np.array(list_2)\n",
    "            input_3 = np.array(list_3)\n",
    "            input_r = np.array(list_r)\n",
    "\n",
    "            pred = model.predict([input_1, input_2, input_3, input_r], verbose = 0)\n",
    "\n",
    "            for i in range(pred.shape[0]):\n",
    "                #need to times 2 to go back to relation id from pred position\n",
    "                score_dict[2*i] += float(pred[i])\n",
    "                count_dict[2*i] += 1\n",
    "\n",
    "            count += 1\n",
    "            \n",
    "    #average the score\n",
    "    for r in score_dict:\n",
    "        score_dict[r] = deepcopy(score_dict[r]/float(count_dict[r]))\n",
    "    \n",
    "    print(len(score_dict), len(path_holder))\n",
    "\n",
    "    return(score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e1b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the function to do path-based triple scoring: input one triple\n",
    "def path_based_triple_scoring(s, r, t, lower_bd, upper_bd, one_hop, id2relation, model):\n",
    "    \n",
    "    path_holder = set()\n",
    "    \n",
    "    for iteration in range(3):\n",
    "    \n",
    "        result, length_dict = Class_2.obtain_paths('target_specified', \n",
    "                                                   s, t, lower_bd, upper_bd, one_hop)\n",
    "        if t in result:\n",
    "            \n",
    "            for path in result[t]:\n",
    "                \n",
    "                path_holder.add(path)\n",
    "                \n",
    "        del(result, length_dict)\n",
    "    \n",
    "    path_holder = list(path_holder)\n",
    "    random.shuffle(path_holder)\n",
    "    \n",
    "    score = 0.\n",
    "    count = 0\n",
    "    \n",
    "    if len(path_holder) >= 3:\n",
    "        \n",
    "        list_1 = list()\n",
    "        list_2 = list()\n",
    "        list_3 = list()\n",
    "        list_r = list()\n",
    "    \n",
    "        #iterate over path_1\n",
    "        while count < 10:\n",
    "\n",
    "            temp_pair = random.sample(path_holder, 3)\n",
    "            path_1, path_2, path_3 = temp_pair[0], temp_pair[1], temp_pair[2]\n",
    "\n",
    "            list_1.append(list(path_1) + [num_r]*abs(len(path_1)-upper_bd))\n",
    "            list_2.append(list(path_2) + [num_r]*abs(len(path_2)-upper_bd))\n",
    "            list_3.append(list(path_3) + [num_r]*abs(len(path_3)-upper_bd))\n",
    "            list_r.append([r])\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "        #change to arrays\n",
    "        input_1 = np.array(list_1)\n",
    "        input_2 = np.array(list_2)\n",
    "        input_3 = np.array(list_3)\n",
    "        input_r = np.array(list_r)\n",
    "\n",
    "        pred = model.predict([input_1, input_2, input_3, input_r], verbose = 0)\n",
    "\n",
    "        for i in range(pred.shape[0]):\n",
    "            score += float(pred[i])\n",
    "            \n",
    "        #average the score\n",
    "        score = score/float(count)\n",
    "\n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8512335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subgraph based relation scoring\n",
    "def subgraph_relation_scoring(s, t, lower_bd, upper_bd, one_hop, id2relation, model_2):\n",
    "    \n",
    "    path_s, path_t = set(), set() #sets holding all the paths from s or t\n",
    "    \n",
    "    for iteration in range(3):\n",
    "    \n",
    "        #obtain the paths out from s or t by \"any target\" mode. That is, \n",
    "        result_s, length_dict_s = Class_2.obtain_paths('any_target', s, 'any', lower_bd, upper_bd, one_hop)\n",
    "        result_t, length_dict_t = Class_2.obtain_paths('any_target', t, 'any', lower_bd, upper_bd, one_hop)\n",
    "\n",
    "        #add paths to the source/target path_set\n",
    "        for e in result_s:\n",
    "            for path in result_s[e]:\n",
    "                path_s.add(path)\n",
    "        for e in result_t:\n",
    "            for path in result_t[e]:\n",
    "                path_t.add(path)\n",
    "                \n",
    "        del(result_s, length_dict_s, result_t, length_dict_t)\n",
    "    \n",
    "    #final output: the score dict\n",
    "    score_dict = defaultdict(float)\n",
    "    count_dict = defaultdict(int)\n",
    "    \n",
    "    #see if both path_s and path_t have at least three paths\n",
    "    if len(path_s) >= 3 and len(path_t) >= 3:\n",
    "\n",
    "        #change to lists\n",
    "        path_s, path_t = list(path_s), list(path_t)\n",
    "        \n",
    "        count = 0\n",
    "        while count < 10:\n",
    "            \n",
    "            #lists holding the input to the network\n",
    "            list_s_1 = list()\n",
    "            list_s_2 = list()\n",
    "            list_s_3 = list()\n",
    "            list_t_1 = list()\n",
    "            list_t_2 = list()\n",
    "            list_t_3 = list()\n",
    "            list_r = list()\n",
    "\n",
    "            #randomly obtain three paths\n",
    "            temp_s = random.sample(path_s, 3)\n",
    "            temp_t = random.sample(path_t, 3)\n",
    "            s_p_1, s_p_2, s_p_3 = temp_s[0], temp_s[1], temp_s[2]\n",
    "            t_p_1, t_p_2, t_p_3 = temp_t[0], temp_t[1], temp_t[2]\n",
    "            \n",
    "            #add all forward (initial relation)\n",
    "            for i in range(len(id2relation)):\n",
    "\n",
    "                if i not in id2relation:\n",
    "\n",
    "                    raise ValueError ('error when generating id2relation')\n",
    "                    \n",
    "                if i % 2 == 0:\n",
    "\n",
    "                    #append the paths: note that we add the space holder id at the end of the shorter path\n",
    "                    list_s_1.append(list(s_p_1) + [num_r]*abs(len(s_p_1)-upper_bd))\n",
    "                    list_s_2.append(list(s_p_2) + [num_r]*abs(len(s_p_2)-upper_bd))\n",
    "                    list_s_3.append(list(s_p_3) + [num_r]*abs(len(s_p_3)-upper_bd))\n",
    "                    \n",
    "                    list_t_1.append(list(t_p_1) + [num_r]*abs(len(t_p_1)-upper_bd))\n",
    "                    list_t_2.append(list(t_p_2) + [num_r]*abs(len(t_p_2)-upper_bd))\n",
    "                    list_t_3.append(list(t_p_3) + [num_r]*abs(len(t_p_3)-upper_bd))\n",
    "                    \n",
    "                    list_r.append([i])\n",
    "                \n",
    "            #change to arrays\n",
    "            input_s_1 = np.array(list_s_1)\n",
    "            input_s_2 = np.array(list_s_2)\n",
    "            input_s_3 = np.array(list_s_3)\n",
    "            input_t_1 = np.array(list_t_1)\n",
    "            input_t_2 = np.array(list_t_2)\n",
    "            input_t_3 = np.array(list_t_3)\n",
    "            input_r = np.array(list_r)\n",
    "            \n",
    "            pred = model_2.predict([input_s_1, input_s_2, input_s_3,\n",
    "                                    input_t_1, input_t_2, input_t_3, input_r], verbose = 0)\n",
    "\n",
    "            for i in range(pred.shape[0]):\n",
    "                #need to times 2 to go back to relation id from pred position\n",
    "                score_dict[2*i] += float(pred[i])\n",
    "                count_dict[2*i] += 1\n",
    "\n",
    "            count += 1\n",
    "            \n",
    "    #average the score\n",
    "    for r in score_dict:\n",
    "        score_dict[r] = deepcopy(score_dict[r]/float(count_dict[r]))\n",
    "            \n",
    "    print(len(score_dict), len(path_s), len(path_t))\n",
    "        \n",
    "    return(score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26015662",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subgraph based triple scoring\n",
    "def subgraph_triple_scoring(s, r, t, lower_bd, upper_bd, one_hop, id2relation, model_2):\n",
    "    \n",
    "    path_s, path_t = set(), set() #sets holding all the paths from s or t\n",
    "    \n",
    "    for iteration in range(3):\n",
    "    \n",
    "        #obtain the paths out from s or t by \"any target\" mode. That is, \n",
    "        result_s, length_dict_s = Class_2.obtain_paths('any_target', s, 'any', lower_bd, upper_bd, one_hop)\n",
    "        result_t, length_dict_t = Class_2.obtain_paths('any_target', t, 'any', lower_bd, upper_bd, one_hop)\n",
    "\n",
    "        #add paths to the source/target path_set\n",
    "        for e in result_s:\n",
    "            for path in result_s[e]:\n",
    "                path_s.add(path)\n",
    "        for e in result_t:\n",
    "            for path in result_t[e]:\n",
    "                path_t.add(path)\n",
    "                \n",
    "        del(result_s, length_dict_s, result_t, length_dict_t)\n",
    "    \n",
    "    #final output: the score dict\n",
    "    score = 0.\n",
    "    \n",
    "    #see if both path_s and path_t have at least three paths\n",
    "    if len(path_s) >= 3 and len(path_t) >= 3:\n",
    "\n",
    "        #change to lists\n",
    "        path_s, path_t = list(path_s), list(path_t)\n",
    "        \n",
    "        #lists holding the input to the network\n",
    "        list_s_1 = list()\n",
    "        list_s_2 = list()\n",
    "        list_s_3 = list()\n",
    "        list_t_1 = list()\n",
    "        list_t_2 = list()\n",
    "        list_t_3 = list()\n",
    "        list_r = list()\n",
    "        \n",
    "        count = 0\n",
    "        while count < 10:\n",
    "\n",
    "            #randomly obtain three paths\n",
    "            temp_s = random.sample(path_s, 3)\n",
    "            temp_t = random.sample(path_t, 3)\n",
    "            s_p_1, s_p_2, s_p_3 = temp_s[0], temp_s[1], temp_s[2]\n",
    "            t_p_1, t_p_2, t_p_3 = temp_t[0], temp_t[1], temp_t[2]\n",
    "\n",
    "            #append the paths: note that we add the space holder id at the end of the shorter path\n",
    "            list_s_1.append(list(s_p_1) + [num_r]*abs(len(s_p_1)-upper_bd))\n",
    "            list_s_2.append(list(s_p_2) + [num_r]*abs(len(s_p_2)-upper_bd))\n",
    "            list_s_3.append(list(s_p_3) + [num_r]*abs(len(s_p_3)-upper_bd))\n",
    "\n",
    "            list_t_1.append(list(t_p_1) + [num_r]*abs(len(t_p_1)-upper_bd))\n",
    "            list_t_2.append(list(t_p_2) + [num_r]*abs(len(t_p_2)-upper_bd))\n",
    "            list_t_3.append(list(t_p_3) + [num_r]*abs(len(t_p_3)-upper_bd))\n",
    "\n",
    "            list_r.append([r])\n",
    "            count += 1\n",
    "                \n",
    "        #change to arrays\n",
    "        input_s_1 = np.array(list_s_1)\n",
    "        input_s_2 = np.array(list_s_2)\n",
    "        input_s_3 = np.array(list_s_3)\n",
    "        input_t_1 = np.array(list_t_1)\n",
    "        input_t_2 = np.array(list_t_2)\n",
    "        input_t_3 = np.array(list_t_3)\n",
    "        input_r = np.array(list_r)\n",
    "\n",
    "        pred = model_2.predict([input_s_1, input_s_2, input_s_3,\n",
    "                                input_t_1, input_t_2, input_t_3, input_r], verbose = 0)\n",
    "\n",
    "        for i in range(pred.shape[0]):\n",
    "            score += float(pred[i])\n",
    "\n",
    "        #average the score\n",
    "        score = score/float(count)\n",
    "        \n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b602d8d",
   "metadata": {},
   "source": [
    "#### Not fine tuned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb84dd20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "#obtain the Hits@N for relation prediction##############\n",
    "\n",
    "#we select all the triples in the inductive test set\n",
    "selected = list(data_ind_test)\n",
    "\n",
    "###Hit at 1#############################\n",
    "#generate the negative samples by randomly replace relation with all the other relaiton\n",
    "Hits_at_1 = 0\n",
    "Hits_at_3 = 0\n",
    "Hits_at_10 = 0\n",
    "MRR_raw = 0.\n",
    "\n",
    "for i in range(len(selected)):\n",
    "    \n",
    "    s_true, r_true, t_true = selected[i][0], selected[i][1], selected[i][2]\n",
    "    \n",
    "    #run the path-based scoring\n",
    "    score_dict_path = path_based_relation_scoring(s_true, t_true, lower_bound, upper_bound_path, one_hop_ind, id2relation, model)\n",
    "    \n",
    "    #run the one-hop neighbour based scoring\n",
    "    score_dict_subg = subgraph_relation_scoring(s_true, t_true, lower_bound, upper_bound_subg, one_hop_ind, id2relation, model_2)\n",
    "    \n",
    "    #final score dict\n",
    "    score_dict = defaultdict(float)\n",
    "    \n",
    "    for r in score_dict_path:\n",
    "        score_dict[r] += score_dict_path[r]\n",
    "    for r in score_dict_subg:\n",
    "        score_dict[r] += score_dict_subg[r]\n",
    "    \n",
    "    #[... [score, r], ...]\n",
    "    temp_list = list()\n",
    "    \n",
    "    for r in id2relation:\n",
    "        \n",
    "        #again, we only care about initial relation prediciton\n",
    "        if r % 2 == 0:\n",
    "        \n",
    "            if r in score_dict:\n",
    "\n",
    "                temp_list.append([score_dict[r], r])\n",
    "\n",
    "            else:\n",
    "\n",
    "                temp_list.append([0.0, r])\n",
    "        \n",
    "    sorted_list = sorted(temp_list, key = lambda x: x[0], reverse=True)\n",
    "    \n",
    "    p = 0\n",
    "    exist_tri = 0\n",
    "    \n",
    "    while p < len(sorted_list) and sorted_list[p][1] != r_true:\n",
    "        \n",
    "        #moreover, we want to remove existing triples\n",
    "        if ((s_true, sorted_list[p][1], t_true) in data_test) or (\n",
    "            (s_true, sorted_list[p][1], t_true) in data_valid) or (\n",
    "            (s_true, sorted_list[p][1], t_true) in data) or (\n",
    "            (s_true, sorted_list[p][1], t_true) in data_ind) or (\n",
    "            (s_true, sorted_list[p][1], t_true) in data_ind_valid) or (\n",
    "            (s_true, sorted_list[p][1], t_true) in data_ind_test):\n",
    "            \n",
    "            exist_tri += 1\n",
    "            \n",
    "        p += 1\n",
    "    \n",
    "    if p - exist_tri == 0:\n",
    "        \n",
    "        Hits_at_1 += 1\n",
    "        \n",
    "    if p - exist_tri < 3:\n",
    "        \n",
    "        Hits_at_3 += 1\n",
    "        \n",
    "    if p - exist_tri < 10:\n",
    "        \n",
    "        Hits_at_10 += 1\n",
    "        \n",
    "    MRR_raw += 1./float(p - exist_tri + 1.) \n",
    "        \n",
    "    print('checkcorrect', r_true, sorted_list[p][1],\n",
    "          'real score', sorted_list[p][0],\n",
    "          'Hits@1', Hits_at_1/(i+1),\n",
    "          'Hits@3', Hits_at_3/(i+1),\n",
    "          'Hits@10', Hits_at_10/(i+1),\n",
    "          'MRR', MRR_raw/(i+1),\n",
    "          'cur_rank', p - exist_tri,\n",
    "          'abs_cur_rank', p,\n",
    "          'total_num', i, len(selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f0b73c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "##obtain the AUC-PR for the test triples###\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.metrics import auc, plot_precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#we select all the triples in the inductive test set\n",
    "pos_triples = list(data_ind_test)\n",
    "\n",
    "#we build the negative samples by randomly replace head or tail entity in the triple.\n",
    "neg_triples = list()\n",
    "\n",
    "for i in range(len(pos_triples)):\n",
    "    \n",
    "    s_pos, r_pos, t_pos = pos_triples[i][0], pos_triples[i][1], pos_triples[i][2]\n",
    "    \n",
    "    #decide to replace the head or tail entity\n",
    "    number_0 = random.uniform(0, 1)\n",
    "    \n",
    "    if number_0 < 0.5: #replace head entity\n",
    "        \n",
    "        s_neg = random.choice(list(new_ent_set))\n",
    "        \n",
    "        #filter out the existing triples\n",
    "        while ((s_neg, r_pos, t_pos) in data_test) or (\n",
    "               (s_neg, r_pos, t_pos) in data_valid) or (\n",
    "               (s_neg, r_pos, t_pos) in data) or (\n",
    "               (s_neg, r_pos, t_pos) in data_ind) or (\n",
    "               (s_neg, r_pos, t_pos) in data_ind_valid) or (\n",
    "               (s_neg, r_pos, t_pos) in data_ind_test):\n",
    "            \n",
    "            s_neg = random.choice(list(new_ent_set))\n",
    "        \n",
    "        neg_triples.append((s_neg, r_pos, t_pos))\n",
    "    \n",
    "    else: #replace tail entity\n",
    "\n",
    "        t_neg = random.choice(list(new_ent_set))\n",
    "        \n",
    "        #filter out the existing triples\n",
    "        while ((s_pos, r_pos, t_neg) in data_test) or (\n",
    "               (s_pos, r_pos, t_neg) in data_valid) or (\n",
    "               (s_pos, r_pos, t_neg) in data) or (\n",
    "               (s_pos, r_pos, t_neg) in data_ind) or (\n",
    "               (s_pos, r_pos, t_neg) in data_ind_valid) or (\n",
    "               (s_pos, r_pos, t_neg) in data_ind_test):\n",
    "            \n",
    "            t_neg = random.choice(list(new_ent_set))\n",
    "        \n",
    "        neg_triples.append((s_pos, r_pos, t_neg))\n",
    "\n",
    "if len(pos_triples) != len(neg_triples):\n",
    "    raise ValueError('error when generating negative triples')\n",
    "        \n",
    "#combine all triples\n",
    "all_triples = pos_triples + neg_triples\n",
    "\n",
    "#obtain the label array\n",
    "arr1 = np.ones((len(pos_triples),))\n",
    "arr2 = np.zeros((len(neg_triples),))\n",
    "y_test = np.concatenate((arr1, arr2))\n",
    "\n",
    "#shuffle positive and negative triples (optional)\n",
    "all_triples, y_test = shuffle(all_triples, y_test)\n",
    "\n",
    "#obtain the score aray\n",
    "y_score = np.zeros((len(y_test),))\n",
    "\n",
    "#implement the scoring\n",
    "for i in range(len(all_triples)):\n",
    "    \n",
    "    s, r, t = all_triples[i][0], all_triples[i][1], all_triples[i][2]\n",
    "    \n",
    "    #path_score = path_based_triple_scoring(s, r, t, lower_bound, upper_bound_path, one_hop_ind, id2relation, model)\n",
    "    \n",
    "    subg_score = subgraph_triple_scoring(s, r, t, lower_bound, upper_bound_subg, one_hop_ind, id2relation, model_2)\n",
    "    \n",
    "    #ave_score = (path_score + subg_score)/float(2)\n",
    "    \n",
    "    #y_score[i] = ave_score\n",
    "    y_score[i] = subg_score\n",
    "    \n",
    "    if i % 20 == 0 and i > 0:\n",
    "        print('evaluating scores', i, len(all_triples))\n",
    "        \n",
    "        # Data to plot precision - recall curve\n",
    "        precision, recall, thresholds = precision_recall_curve(y_test[:i], y_score[:i])\n",
    "        # Use AUC function to calculate the area under the curve of precision recall curve\n",
    "        auc_precision_recall = auc(recall, precision)\n",
    "        print('AUC-PR is:', auc_precision_recall)\n",
    "        \n",
    "        \n",
    "# Data to plot precision - recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_score)\n",
    "# Use AUC function to calculate the area under the curve of precision recall curve\n",
    "auc_precision_recall = auc(recall, precision)\n",
    "print('AUC-PR is:', auc_precision_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101f932",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "##obtain the AUC-PR for the test triples, using sklearn###\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.metrics import auc, plot_precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#we select all the triples in the inductive test set\n",
    "pos_triples = list(data_ind_test)\n",
    "\n",
    "#we build the negative samples by randomly replace head or tail entity in the triple.\n",
    "neg_triples = list()\n",
    "\n",
    "for i in range(len(pos_triples)):\n",
    "    \n",
    "    s_pos, r_pos, t_pos = pos_triples[i][0], pos_triples[i][1], pos_triples[i][2]\n",
    "    \n",
    "    #decide to replace the head or tail entity\n",
    "    number_0 = random.uniform(0, 1)\n",
    "    \n",
    "    if number_0 < 0.5: #replace head entity\n",
    "        \n",
    "        s_neg = random.choice(list(new_ent_set))\n",
    "        \n",
    "        #filter out the existing triples\n",
    "        while ((s_neg, r_pos, t_pos) in data_test) or (\n",
    "               (s_neg, r_pos, t_pos) in data_valid) or (\n",
    "               (s_neg, r_pos, t_pos) in data) or (\n",
    "               (s_neg, r_pos, t_pos) in data_ind) or (\n",
    "               (s_neg, r_pos, t_pos) in data_ind_valid) or (\n",
    "               (s_neg, r_pos, t_pos) in data_ind_test):\n",
    "            \n",
    "            s_neg = random.choice(list(new_ent_set))\n",
    "        \n",
    "        neg_triples.append((s_neg, r_pos, t_pos))\n",
    "    \n",
    "    else: #replace tail entity\n",
    "\n",
    "        t_neg = random.choice(list(new_ent_set))\n",
    "        \n",
    "        #filter out the existing triples\n",
    "        while ((s_pos, r_pos, t_neg) in data_test) or (\n",
    "               (s_pos, r_pos, t_neg) in data_valid) or (\n",
    "               (s_pos, r_pos, t_neg) in data) or (\n",
    "               (s_pos, r_pos, t_neg) in data_ind) or (\n",
    "               (s_pos, r_pos, t_neg) in data_ind_valid) or (\n",
    "               (s_pos, r_pos, t_neg) in data_ind_test):\n",
    "            \n",
    "            t_neg = random.choice(list(new_ent_set))\n",
    "        \n",
    "        neg_triples.append((s_pos, r_pos, t_neg))\n",
    "\n",
    "if len(pos_triples) != len(neg_triples):\n",
    "    raise ValueError('error when generating negative triples')\n",
    "        \n",
    "#combine all triples\n",
    "all_triples = pos_triples + neg_triples\n",
    "\n",
    "#obtain the label array\n",
    "arr1 = np.ones((len(pos_triples),))\n",
    "arr2 = np.zeros((len(neg_triples),))\n",
    "y_test = np.concatenate((arr1, arr2))\n",
    "\n",
    "#shuffle positive and negative triples (optional)\n",
    "all_triples, y_test = shuffle(all_triples, y_test)\n",
    "\n",
    "#obtain the score aray\n",
    "y_score = np.zeros((len(y_test),))\n",
    "\n",
    "#implement the scoring\n",
    "for i in range(len(all_triples)):\n",
    "    \n",
    "    s, r, t = all_triples[i][0], all_triples[i][1], all_triples[i][2]\n",
    "    \n",
    "    #path_score = path_based_triple_scoring(s, r, t, lower_bound, upper_bound_path, one_hop_ind, id2relation, model)\n",
    "    \n",
    "    subg_score = subgraph_triple_scoring(s, r, t, lower_bound, upper_bound_subg, one_hop_ind, id2relation, model_2)\n",
    "    \n",
    "    #ave_score = (path_score + subg_score)/float(2)\n",
    "    \n",
    "    #y_score[i] = ave_score\n",
    "    y_score[i] = subg_score\n",
    "    \n",
    "    if i % 20 == 0 and i > 0:\n",
    "        print('evaluating scores', i, len(all_triples))\n",
    "        auc = metrics.roc_auc_score(y_test[:i], y_score[:i])\n",
    "        auc_pr = metrics.average_precision_score(y_test[:i], y_score[:i])\n",
    "        print('auc, auc-pr', auc, auc_pr)\n",
    "        \n",
    "print('evaluating scores', i, len(all_triples))\n",
    "auc = metrics.roc_auc_score(y_test, y_score)\n",
    "auc_pr = metrics.average_precision_score(y_test, y_score)\n",
    "print('(final) auc, auc-pr', auc, auc_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6403a2e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "#obtain the Hits@N for entity prediction##############\n",
    "\n",
    "#we select all the triples in the inductive test set\n",
    "selected = list(data_ind_test)\n",
    "\n",
    "###Hit at 1#############################\n",
    "#generate the negative samples by randomly replace relation with all the other relaiton\n",
    "Hits_at_1 = 0\n",
    "Hits_at_3 = 0\n",
    "Hits_at_10 = 0\n",
    "MRR_raw = 0.\n",
    "\n",
    "for i in range(len(selected)):\n",
    "    \n",
    "    triple_list = list()\n",
    "    \n",
    "    #score the true triple\n",
    "    s_pos, r_pos, t_pos = selected[i][0], selected[i][1], selected[i][2]\n",
    "\n",
    "    #path_score = path_based_triple_scoring(s_pos, r_pos, t_pos, lower_bound, upper_bound_path, one_hop_ind, id2relation, model)\n",
    "\n",
    "    subg_score = subgraph_triple_scoring(s_pos, r_pos, t_pos, lower_bound, upper_bound_subg, one_hop_ind, id2relation, model_2)\n",
    "    \n",
    "    #ave_score = (path_score + subg_score)/float(2)\n",
    "    \n",
    "    triple_list.append([(s_pos, r_pos, t_pos), subg_score])\n",
    "    \n",
    "    #generate the 50 random samples\n",
    "    for sub_i in range(50):\n",
    "        \n",
    "        #decide to replace the head or tail entity\n",
    "        number_0 = random.uniform(0, 1)\n",
    "\n",
    "        if number_0 < 0.5: #replace head entity\n",
    "            \n",
    "            s_neg = random.choice(list(new_ent_set))\n",
    "            \n",
    "            while ((s_neg, r_pos, t_pos) in data_test) or (\n",
    "                   (s_neg, r_pos, t_pos) in data_valid) or (\n",
    "                   (s_neg, r_pos, t_pos) in data) or (\n",
    "                   (s_neg, r_pos, t_pos) in data_ind) or (\n",
    "                   (s_neg, r_pos, t_pos) in data_ind_valid) or (\n",
    "                   (s_neg, r_pos, t_pos) in data_ind_test):\n",
    "\n",
    "                s_neg = random.choice(list(new_ent_set))\n",
    "            \n",
    "            #path_score = path_based_triple_scoring(s_neg, r_pos, t_pos, lower_bound, upper_bound_path, one_hop_ind, id2relation, model)\n",
    "\n",
    "            subg_score = subgraph_triple_scoring(s_neg, r_pos, t_pos, lower_bound, upper_bound_subg, one_hop_ind, id2relation, model_2)\n",
    "\n",
    "            #ave_score = (path_score + subg_score)/float(2)\n",
    "\n",
    "            triple_list.append([(s_neg, r_pos, t_pos), subg_score])\n",
    "            \n",
    "        else: #replace tail entity\n",
    "\n",
    "            t_neg = random.choice(list(new_ent_set))\n",
    "            \n",
    "            #filter out the existing triples\n",
    "            while ((s_pos, r_pos, t_neg) in data_test) or (\n",
    "                   (s_pos, r_pos, t_neg) in data_valid) or (\n",
    "                   (s_pos, r_pos, t_neg) in data) or (\n",
    "                   (s_pos, r_pos, t_neg) in data_ind) or (\n",
    "                   (s_pos, r_pos, t_neg) in data_ind_valid) or (\n",
    "                   (s_pos, r_pos, t_neg) in data_ind_test):\n",
    "\n",
    "                t_neg = random.choice(list(new_ent_set))\n",
    "            \n",
    "            #path_score = path_based_triple_scoring(s_pos, r_pos, t_neg, lower_bound, upper_bound_path, one_hop_ind, id2relation, model)\n",
    "\n",
    "            subg_score = subgraph_triple_scoring(s_pos, r_pos, t_neg, lower_bound, upper_bound_subg, one_hop_ind, id2relation, model_2)\n",
    "\n",
    "            #ave_score = (path_score + subg_score)/float(2)\n",
    "\n",
    "            triple_list.append([(s_pos, r_pos, t_neg), subg_score])\n",
    "            \n",
    "    #random shuffle!\n",
    "    random.shuffle(triple_list)\n",
    "    \n",
    "    #sort\n",
    "    sorted_list = sorted(triple_list, key = lambda x: x[-1], reverse=True)\n",
    "    \n",
    "    p = 0\n",
    "    \n",
    "    while p < len(sorted_list) and sorted_list[p][0] != (s_pos, r_pos, t_pos):\n",
    "            \n",
    "        p += 1\n",
    "    \n",
    "    if p == 0:\n",
    "        \n",
    "        Hits_at_1 += 1\n",
    "        \n",
    "    if p < 3:\n",
    "        \n",
    "        Hits_at_3 += 1\n",
    "        \n",
    "    if p < 10:\n",
    "        \n",
    "        Hits_at_10 += 1\n",
    "        \n",
    "    MRR_raw += 1./float(p + 1.) \n",
    "        \n",
    "    print('checkcorrect', (s_pos, r_pos, t_pos), sorted_list[p][0],\n",
    "          'real score', sorted_list[p][-1],\n",
    "          'Hits@1', Hits_at_1/(i+1),\n",
    "          'Hits@3', Hits_at_3/(i+1),\n",
    "          'Hits@10', Hits_at_10/(i+1),\n",
    "          'MRR', MRR_raw/(i+1),\n",
    "          'rank', p,\n",
    "          'total_num', i, len(selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dcec14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50b1482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c23edb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10052e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee5bc67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998880eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
